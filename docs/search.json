[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Modelling for Data Scientists",
    "section": "",
    "text": "Welcome\nThis is the website for Spatial Modeling for Data Scientists 2022/23. This is a course taught by Dr. Francisco Rowe at the University of Liverpool, United Kingdom. The course materials were developed by Dr. Francisco Rowe and Professor Dani Arribas-Bel. You will learn how to analyse and model different types of spatial data as well as gaining an understanding of the various challenges arising from manipulating such data.\nThe website is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. A compilation of this web course is hosted as a GitHub repository that you can access:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Spatial Modelling for Data Scientists",
    "section": "Contact",
    "text": "Contact\n\nFrancisco Rowe - F.Rowe-Gonzalez [at] liverpool.ac.uk\nSenior Lecturer in Quantitative Human Geography\nOffice 507, Roxby Building,\nUniversity of Liverpool - 74 Bedford St S,\nLiverpool, L69 7ZT,\nUnited Kingdom."
  },
  {
    "objectID": "01-overview.html#aims",
    "href": "01-overview.html#aims",
    "title": "1  Overview",
    "section": "1.1 Aims",
    "text": "1.1 Aims\nThis module aims to provides students with a range of techniques for analysing and modelling spatial data:\n\nbuild upon the more general research training delivered via companion modules on Data Collection and Data Analysis, both of which have an aspatial focus;\nhighlight a number of key social issues that have a spatial dimension;\nexplain the specific challenges faced when attempting to analyse spatial data;\nintroduce a range of analytical techniques and approaches suitable for the analysis of spatial data; and,\nenhance practical skills in using R software packages to implement a wide range of spatial analytical tools."
  },
  {
    "objectID": "01-overview.html#learning-outcomes",
    "href": "01-overview.html#learning-outcomes",
    "title": "1  Overview",
    "section": "1.2 Learning Outcomes",
    "text": "1.2 Learning Outcomes\nBy the end of the module, students should be able to:\n\nidentify some key sources of spatial data and resources of spatial analysis and modelling tools;\nexplain the advantages of taking spatial structure into account when analysing spatial data;\napply a range of computer-based techniques for the analysis of spatial data, including mapping, correlation, kernel density estimation, regression, multi-level models, geographically-weighted regression, spatial interaction models and spatial econometrics;\napply appropriate analytical strategies to tackle the key methodological challenges facing spatial analysis – spatial autocorrelation, heterogeneity, and ecological fallacy; and,\nselect appropriate analytical tools for analysing specific spatial data sets to address emerging social issues facing the society."
  },
  {
    "objectID": "01-overview.html#feedback",
    "href": "01-overview.html#feedback",
    "title": "1  Overview",
    "section": "1.3 Feedback",
    "text": "1.3 Feedback\n\nFormal assessment of two computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during lecture, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum maintained by the module lead. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times."
  },
  {
    "objectID": "01-overview.html#computational-environment",
    "href": "01-overview.html#computational-environment",
    "title": "1  Overview",
    "section": "1.4 Computational Environment",
    "text": "1.4 Computational Environment\nTo reproduce the code in the book, you need the following software packages:\n\nR-4.2.2\nRStudio 2022.12.0-353\nQuarto 1.2.280\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\n1.4.1 Dependency list\nThe list of libraries used in this book is provided below:\n\narm\ncar\ncorrplot\nFRK\ngghighlight\nggplot2\nggmap\nGISTools\ngridExtra\ngstat\njtools\nkableExtra\nknitr\nlme4\nlmtest\nlubridate\nMASS\nmerTools\nplyr\nRColorBrewer\nrgdal\nsf\nsjPlot\nsp\nspgwr\nspatialreg\nspacetime\nstargazer\ntidyverse\ntmap\ntufte\nviridis\n\nCopy, paste and run the code below in your console. Ensure all packages are installed on your computer.\n\ndeps <- list(\n    \"arm\",\n    \"car\",\n    \"corrplot\",\n    \"FRK\",\n    \"gghighlight\",\n    \"ggplot2\",\n    \"ggmap\",\n    \"gridExtra\",\n    \"gstat\",\n    \"jtools\",\n    \"kableExtra\",\n    \"knitr\",\n    \"lme4\",\n    \"lmtest\",\n    \"lubridate\",\n    \"MASS\",\n    \"merTools\",\n    \"plyr\",\n    \"RColorBrewer\",\n    \"rgdal\",\n    \"sf\",\n    \"sjPlot\",\n    \"sp\",\n    \"spgwr\",\n    \"spatialreg\",\n    \"spacetime\",\n    \"stargazer\",\n    \"tidyverse\",\n    \"tmap\",\n    \"tufte\",\n    \"viridis\"\n)\n\n\n# we can load them all to make sure they are installed:\nfor(lib in deps){library(lib, character.only = TRUE)}"
  },
  {
    "objectID": "01-overview.html#assessment",
    "href": "01-overview.html#assessment",
    "title": "1  Overview",
    "section": "1.5 Assessment",
    "text": "1.5 Assessment\nThe final module mark is composed of the two computational essays. Together they are designed to cover the materials introduced in the entirety of content covered during the semester. A computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. Each teaching week, you will be required to address a set of questions relating to the module content covered in that week, and to use the material that you will produce for this purpose to build your computational essay.\nAssignment 1 (50%) refer to the set of questions at the end of Chapter 4, Chapter 5 and Chapter 6. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\nAssignment 2 (50%) refer to the set of questions at the end of Chapter 7, Chapter 8, Chapter 9 and Chapter 10. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\n\n1.5.1 Format Requirements\nBoth assignments will have the same requirements:\n\nMaximum word count: 2,000 words, excluding figures and references.\nUp to three maps, plot or figures (a figure may include more than one map and/or plot and will only count as one but needs to be integrated in the figure)\nUp to two tables.\n\nAssignments need to be prepared in “Quarto Document” format (i.e. qmd extension) and then converted into a self-contained HTML file that will then be submitted via Turnitin. The document should only display content that will be assessed. Intermediate steps do not need to be displayed. Messages resulting from loading packages, attaching data frames, or similar messages do not need to be included as output code. Useful resources to customise your R notebook can be found on Quarto’s website.\nTwo Quarto Document templates will be available via the module Canvas site.\nSubmission is electronic only via Turnitin on Canvas.\n\n\n1.5.2 Marking criteria\nThe Standard Environmental Sciences School marking criteria apply, with a stronger emphasis on evidencing the use of regression models, critical analysis of results and presentation standards. In addition to these general criteria, the code and outputs (i.e. tables, maps and plots) contained within the notebook submitted for assessment will be assessed according to the extent of documentation and evidence of expertise in changing and extending the code options illustrated in each chapter. Specifically, the following criteria will be applied:\n\n0-15: no documentation and use of default options.\n16-39: little documentation and use of default options.\n40-49: some documentation, and use of default options.\n50-59: extensive documentation, and edit of some of the options provided in the notebook (e.g. change north arrow location).\n60-69: extensive well organised and easy to read documentation, and evidence of understanding of options provided in the code (e.g. tweaking existing options).\n70-79: all above, plus clear evidence of code design skills (e.g. customising graphics, combining plots (or tables) into a single output, adding clear axis labels and variable names on graphic outputs, etc.).\n80-100: all as above, plus code containing novel contributions that extend/improve the functionality the code was provided with (e.g. comparative model assessments, novel methods to perform the task, etc.)."
  },
  {
    "objectID": "02-spatial_data.html#spatial-data-types",
    "href": "02-spatial_data.html#spatial-data-types",
    "title": "2  Spatial Data",
    "section": "2.1 Spatial Data types",
    "text": "2.1 Spatial Data types\nDifferent classifications of spatial data types exist. Knowing the structure of the data at hand is important as specific analytical methods would be more appropriate for particular data types. We will use a particular classification involving four data types: lattice/areal data, point data, flow data and trajectory data (Fig. 1). This is not a exhaustive list but it is helpful to motivate the analytical and modelling methods that we cover in this book.\n\n\n\nFig. 1. Data Types. Area / Lattice data source: Önnerfors et al. (2019). Point data source: Tao et al. (2018). Flow data source: Rowe and Patias (2020). Trajectory data source: Kwan and Lee (2004).\n\n\nLattice/Areal Data. These data correspond to records of attribute values (such as population counts) for a fixed geographical area. They may comprise regular shapes (such as grids or pixels) or irregular shapes (such as states, counties or travel-to-work areas). Raster data are a common source of regular lattice/areal area, while censuses are probably the most common form of irregular lattice/areal area. Point data within an area can be aggregated to produce lattice/areal data.\nPoint Data. These data refer to records of the geographic location of an discrete event, or the number of occurrences of geographical process at a given location. As displayed in Fig. 1, examples include the geographic location of bus stops in a city, or the number of boarding passengers at each bus stop.\nFlow Data. These data refer to records of measurements for a pair of geographic point locations. or pair of areas. These data capture the linkage or spatial interaction between two locations. Migration flows between a place of origin and a place of destination is an example of this type of data.\nTrajectory Data. These data record geographic locations of moving objects at various points in time. A trajectory is composed of a single string of data recording the geographic location of an object at various points in time and each record in the string contains a time stamp. These data are complex and can be classified into explicit trajectory data and implicit trajectory data. The former refer to well-structured data and record positions of objects continuously and intensively at uniform time intervals, such as GPS data. The latter is less structured and record data in relatively time point intervals, including sensor-based, network-based and signal-based data (Kong et al. 2018).\nIn this course, we cover analytical and modelling approaches for point, lattice/areal and flow data. While we do not explicitly analyse trajectory data, various of the analytical approaches described in this book can be extended to incorporate time, and can be applied to model these types of data. In Chapter 10, we describe approaches to analyse and model spatio-temporal data. These same methods can be applied to trajectory data."
  },
  {
    "objectID": "02-spatial_data.html#hierarchical-structure-of-data",
    "href": "02-spatial_data.html#hierarchical-structure-of-data",
    "title": "2  Spatial Data",
    "section": "2.2 Hierarchical Structure of Data",
    "text": "2.2 Hierarchical Structure of Data\nThe hierarchical organisation is a key feature of spatial data. Smaller geographical units are organised within larger geographical units. You can find the hierarchical representation of UK Statistical Geographies on the Office for National Statistics website. In the bottom part of the output below, we can observe a spatial data frame for Liverpool displaying the hierarchical structure of census data (from the smallest to the largest): Output Areas (OAs), Lower Super Output Areas (LSOAs), Middle Super Output Areas (MSOAs) and Local Authority Districts (LADs). This hierarchical structure entails that units in smaller geographies are nested within units in larger geographies, and that smaller units can be aggregated to produce large units.\n\n\n\n\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479\nProjected CRS: Transverse_Mercator\n      OA_CD   LSOA_CD   MSOA_CD    LAD_CD                       geometry\n1 E00176737 E01033761 E02006932 E08000012 MULTIPOLYGON (((335106.3 38...\n2 E00033515 E01006614 E02001358 E08000012 MULTIPOLYGON (((335810.5 39...\n3 E00033141 E01006546 E02001365 E08000012 MULTIPOLYGON (((336738 3931...\n4 E00176757 E01006646 E02001369 E08000012 MULTIPOLYGON (((335914.5 39...\n5 E00034050 E01006712 E02001375 E08000012 MULTIPOLYGON (((339325 3914...\n6 E00034280 E01006761 E02001366 E08000012 MULTIPOLYGON (((338198.1 39...\n\n\nNext we quickly go through the components of the output above. The first line indicates the type of feature and the number of rows (features) and columns (fields) in the data frame, except for the geometry. The second and third lines identify the type of geometry and dimension. The fourth line bbox stands for bounding box and display the min and max coordinates containing the Liverpool area in the data frame. The fifth line projected CRS indicates the coordinate reference system projection. If you would like to learn more about the various components of spatial data frames, please see the R sf package vignette on Simple Features."
  },
  {
    "objectID": "02-spatial_data.html#key-challenges",
    "href": "02-spatial_data.html#key-challenges",
    "title": "2  Spatial Data",
    "section": "2.3 Key Challenges",
    "text": "2.3 Key Challenges\nMajor challenges exist when working with spatial data. Below we explore some of the key longstanding problems data scientists often face when working with geographical data.\n\n2.3.1 Modifible Area Unit Problem (MAUP)\nThe Modifible Area Unit Problem (MAUP) represents a challenge that has troubled geographers for decades (Openshaw 1981). Two aspects of the MAUP are normally recognised in empirical analysis relating to scale and zonation. Fig. 2 illustrates these issues\n\nScale refers to the idea that a geographical area can be divided into geographies with differing numbers of spatial units.\nZonation refers to the idea that a geographical area can be divided into the same number of units in a variety of ways.\n\n\n\n\nFig. 2. MAUP effect. (a) scale effect; and, (b) zonation effect. Source: Loidl et al. (2016).\n\n\nThe MAUP is a critical issue as it can impact our analysis and thus any conclusions we can infer from our results (e.g. Fotheringham and Wong 1991). There is no agreed systematic approach on how to handle the effects of the MAUP. Some have suggested to perform analyses based on different existing geographical scales, and assess the consistency of the results and identify potential sources of change. The issue with such approach is that results from analysis at different scales are likely to differ because distinct dimensions of a geographic process may be captured at different scales. For example, in migration studies, smaller geographies may be more suitable to capture residential mobility over short distances, while large geographies may be more suitable to capture long-distance migration. And it is well documented that these types of moves are driven by different factors. While residential mobility tends to be driven by housing related reasons, long-distance migration is more closely related to employment-related motives (Niedomysl 2011).\nAn alternative approach is to use the smallest geographical system available and create random aggregations at various geographical scales, to directly quantify the extent of scale and zonation. This approach has shown promising results in applications to study internal migration flows (Stillwell, Daras, and Bell 2018). Another approach involves the production of “meaningful” or functional geographies that can more appropriately capture the process of interest. There is an active area of work defining functional labour markets (Casado-Díaz, Martínez-Bernabéu, and Rowe 2017), urban areas (Arribas-Bel, Garcia-López, and Viladecans-Marsal 2021) and various forms of geodemographic classifications (Singleton and Spielman 2013; Patias, Rowe, and Cavazzi 2019) . However there is the recognition that none of the existing approaches resolve the effects of the MAUP and recently it has been suggested that the most plausible ‘solution’ would be to ignore the MAUP (Wolf et al. 2020).\n\n\n2.3.2 Ecological Fallacy\nEcological fallacy is an error in the interpretation of statistical data based on aggregate information. Specifically it refers to inferences made about the nature of specific individuals based solely on statistics aggregated for a given group. It is about thinking that relationships observed for groups necessarily hold for individuals. A key example is Robinson (1950) who illustrates this problem exploring the difference between ecological correlations and individual correlations. He looked at the relationship between country of birth and literacy. Robinson (1950) used the percent of foreign-born population and percent of literate population for the 48 states in the United States in 1930. The ecological correlation based on these data was 0.53. This suggests a positive association between foreign birth and literacy, and could be interpreted as foreign born individuals being more likely to be literate than native-born individuals. Yet, the correlation based on individual data was negative -0.11 which indicates the opposite. The main point emerging from this example is to carefully interpret analysis based on spatial data and avoid making inferences about individuals from these data.\n\n\n2.3.3 Spatial Dependence\nSpatial dependence refers to the spatial relationship of a variable’s values for a pair of locations at a certain distance apart, so that these values are more similar (or less similar) than expected for randomly associated pairs of observations (Anselin 1988). For example, we could think of observed patterns of ethnic segregation in an area are a result of spillover effects of pre-existing patterns of ethnic segregation in neighbouring areas. Chapter 5 will illustrate approach to explicitly incorporate spatial dependence in regression analysis.\n\n\n2.3.4 Spatial Heterogeneity\nSpatial heterogeneity refers to the uneven distribution of a variable’s values across space. Concentration of deprivation or unemployment across an area are good examples of spatial heterogeneity. We illustrate various ways to visualise, explore and measure the spatial distribution of data in multiple chapters. We also discuss on potential modelling approaches to capture spatial heterogeneity in Chapter 5, Chapter 7 and Chapter 10.\n\n\n2.3.5 Spatial nonstationarity\nSpatial nonstationarity refers to variations in the relationship between an outcome variable and a set of predictor variables across space. In a modelling context, it relates to a situation in which a simple “global” model is inappropriate to explain the relationships between a set of variables. The geographical nature of the model must be modified to reflect local structural relationships within the data. For example, ethinic segregation has been positively associated with employment outcomes in some countries pointing to networks in pre-existing communities facilitating access to the local labour market. Inversely ethinic segregation has been negatively associated with employment outcomes pointing to lack of integration into the broader local community. We illustrate various modelling approaches to capture spatial nonstationarity in Chapter 8 and Chapter 9.\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Vol. 4. Springer Science & Business Media.\n\n\nArribas-Bel, Daniel, M.-À. Garcia-López, and Elisabet Viladecans-Marsal. 2021. “Building(s and) Cities: Delineating Urban Areas with a Machine Learning Algorithm.” Journal of Urban Economics 125 (September): 103217. https://doi.org/10.1016/j.jue.2019.103217.\n\n\nCasado-Díaz, José Manuel, Lucas Martínez-Bernabéu, and Francisco Rowe. 2017. “An Evolutionary Approach to the Delimitation of Labour Market Areas: An Empirical Application for Chile.” Spatial Economic Analysis 12 (4): 379–403. https://doi.org/10.1080/17421772.2017.1273541.\n\n\nFotheringham, A Stewart, and David WS Wong. 1991. “The Modifiable Areal Unit Problem in Multivariate Statistical Analysis.” Environment and Planning A 23 (7): 1025–44.\n\n\nKong, Xiangjie, Menglin Li, Kai Ma, Kaiqi Tian, Mengyuan Wang, Zhaolong Ning, and Feng Xia. 2018. “Big Trajectory Data: A Survey of Applications and Services.” IEEE Access 6: 58295–306. https://doi.org/10.1109/access.2018.2873779.\n\n\nKwan, Mei-Po, and Jiyeong Lee. 2004. “Geovisualization of Human Activity Patterns Using 3D GIS: A Time-Geographic Approach.” Spatially Integrated Social Science 27: 721–44.\n\n\nLoidl, Martin, Gudrun Wallentin, Robin Wendel, and Bernhard Zagel. 2016. “Mapping Bicycle Crash Risk Patterns on the Local Scale.” Safety 2 (3): 17. https://doi.org/10.3390/safety2030017.\n\n\nNiedomysl, Thomas. 2011. “How Migration Motives Change over Migration Distance: Evidence on Variation Across Socio-Economic and Demographic Groups.” Regional Studies 45 (6): 843–55. https://doi.org/10.1080/00343401003614266.\n\n\nÖnnerfors, Åsa, Mariana Kotzeva, Teodóra Brandmüller, et al. 2019. “Eurostat Regional Yearbook 2019 Edition.”\n\n\nOpenshaw, Stan. 1981. “The Modifiable Areal Unit Problem.” Quantitative Geography: A British View, 60–69.\n\n\nPatias, Nikos, Francisco Rowe, and Stefano Cavazzi. 2019. “A Scalable Analytical Framework for Spatio-Temporal Analysis of Neighborhood Change: A Sequence Analysis Approach.” In, 223–41. Springer International Publishing. https://doi.org/10.1007/978-3-030-14745-7_13.\n\n\nRobinson, WS. 1950. “Ecological Correlations and Individual Behavior.” American Sociological Review 15 (195): 351–57.\n\n\nRowe, Francisco, and Nikos Patias. 2020. “Mapping the Spatial Patterns of Internal Migration in Europe.” Regional Studies, Regional Science 7 (1): 390–93. https://doi.org/10.1080/21681376.2020.1811139.\n\n\nSingleton, Alexander D., and Seth E. Spielman. 2013. “The Past, Present, and Future of Geodemographic Research in the United States and United Kingdom.” The Professional Geographer 66 (4): 558–67. https://doi.org/10.1080/00330124.2013.848764.\n\n\nStillwell, John, Konstantinos Daras, and Martin Bell. 2018. “Spatial Aggregation Methods for Investigating the MAUP Effects in Migration Analysis.” Applied Spatial Analysis and Policy 11 (4): 693–711. https://doi.org/10.1007/s12061-018-9274-6.\n\n\nTao, Sui, Jonathan Corcoran, Francisco Rowe, and Mark Hickman. 2018. “To Travel or Not to Travel: ‘Weather’ Is the Question. Modelling the Effect of Local Weather Conditions on Bus Ridership.” Transportation Research Part C: Emerging Technologies 86 (January): 147–67. https://doi.org/10.1016/j.trc.2017.11.005.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2020. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3): 596–608. https://doi.org/10.1177/0309132520924722."
  },
  {
    "objectID": "03-data-wrangling.html#dependencies",
    "href": "03-data-wrangling.html#dependencies",
    "title": "3  Data Wrangling",
    "section": "3.1 Dependencies",
    "text": "3.1 Dependencies\nThis tutorial uses the libraries below. Ensure they are installed on your machine2 before loading them executing the following code chunk:\n\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis)"
  },
  {
    "objectID": "03-data-wrangling.html#introducing-r",
    "href": "03-data-wrangling.html#introducing-r",
    "title": "3  Data Wrangling",
    "section": "3.2 Introducing R",
    "text": "3.2 Introducing R\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques. It has gained widespread use in academia and industry. R offers a wider array of functionality than a traditional statistics package, such as SPSS and is composed of core (base) functionality, and is expandable through libraries hosted on CRAN. CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.\nCommands are sent to R using either the terminal / command line or the R Console which is installed with R on either Windows or OS X. On Linux, there is no equivalent of the console, however, third party solutions exist. On your own machine, R can be installed from here.\nNormally RStudio is used to implement R coding. RStudio is an integrated development environment (IDE) for R and provides a more user-friendly front-end to R than the front-end provided with R.\nTo run R or RStudio, just double click on the R or RStudio icon. Throughout this module, we will be using RStudio:\n\n\n\nFig. 1. RStudio features.\n\n\nIf you would like to know more about the various features of RStudio, watch this video"
  },
  {
    "objectID": "03-data-wrangling.html#setting-the-working-directory",
    "href": "03-data-wrangling.html#setting-the-working-directory",
    "title": "3  Data Wrangling",
    "section": "3.3 Setting the working directory",
    "text": "3.3 Setting the working directory\nBefore we start any analysis, ensure to set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the data folder lives.\n\n#setwd('../data/sar.csv')\n#setwd('.')\n\nNote: It is good practice to not include spaces when naming folders and files. Use underscores or dots.\nYou can check your current working directory by typing:\n\ngetwd()\n\n[1] \"/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san\""
  },
  {
    "objectID": "03-data-wrangling.html#r-scripts-and-computational-notebooks",
    "href": "03-data-wrangling.html#r-scripts-and-computational-notebooks",
    "title": "3  Data Wrangling",
    "section": "3.4 R Scripts and Computational Notebooks",
    "text": "3.4 R Scripts and Computational Notebooks\nAn R script is a series of commands that you can execute at one time and help you save time. So you do not repeat the same steps every time you want to execute the same process with different datasets. An R script is just a plain text file with R commands in it.\nTo create an R script in RStudio, you need to\n\nOpen a new script file: File > New File > R Script\nWrite some code on your new script window by typing eg. mtcars\nRun the script. Click anywhere on the line of code, then hit Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run the command or select the code chunk and click run on the right-top corner of your script window. If do that, you should get:\n\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nSave the script: File > Save As, select your required destination folder, and enter any filename that you like, provided that it ends with the file extension .R\n\nAn R Notebook is an R Markdown document with descriptive text and code chunks that can be executed independently and interactively, with output visible immediately beneath a code chunk - see Xie, Allaire, and Grolemund (2019).\nTo create an R Notebook, you need to:\n\nOpen a new script file: File > New File > R Notebook\n\n\n\n\nFig. 2. YAML metadata for notebooks.\n\n\n\nInsert code chunks, either:\n\n\nuse the Insert command on the editor toolbar;\nuse the keyboard shortcut Ctrl + Alt + I or Cmd + Option + I (Mac); or,\ntype the chunk delimiters ```{r} and ```\n\nIn a chunk code you can produce text output, tables, graphics and write code! You can control these outputs via chunk options which are provided inside the curly brackets eg.\n\n\n\nFig. 3. Code chunk example. Details on the various options: https://rmarkdown.rstudio.com/lesson-3.html\n\n\n\nExecute code: hit “Run Current Chunk”, Ctrl + Shift + Enter or Cmd + Shift + Enter (Mac)\nSave an R notebook: File > Save As. A notebook has a *.Rmd extension and when it is saved a *.nb.html file is automatically created. The latter is a self-contained HTML file which contains both a rendered copy of the notebook with all current chunk outputs and a copy of the *.Rmd file itself.\n\nRstudio also offers a Preview option on the toolbar which can be used to create pdf, html and word versions of the notebook. To do this, choose from the drop-down list menu knit to ...\nFor this module, we will be using computational notebooks through Quarto; that is, Quarto Document. “Quarto is a multi-language, next generation version of R Markdown from RStudio, with many new new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.”\nTo create a Quarto Document, you need to:\n\nOpen a new script file: File > New File > Quarto Document\n\nQuarto Documents work in the same way as R Notebooks with small variations. You find a comprehensive guide on the Quarto website."
  },
  {
    "objectID": "03-data-wrangling.html#getting-help",
    "href": "03-data-wrangling.html#getting-help",
    "title": "3  Data Wrangling",
    "section": "3.5 Getting Help",
    "text": "3.5 Getting Help\nYou can use help or ? to ask for details for a specific function:\n\nhelp(sqrt) #or ?sqrt\n\nAnd using example provides examples for said function:\n\nexample(sqrt)\n\n\nsqrt> require(stats) # for spline\n\nsqrt> require(graphics)\n\nsqrt> xx <- -9:9\n\nsqrt> plot(xx, sqrt(abs(xx)),  col = \"red\")\n\n\n\n\n\nExample sqrt\n\n\n\n\n\nsqrt> lines(spline(xx, sqrt(abs(xx)), n=101), col = \"pink\")"
  },
  {
    "objectID": "03-data-wrangling.html#variables-and-objects",
    "href": "03-data-wrangling.html#variables-and-objects",
    "title": "3  Data Wrangling",
    "section": "3.6 Variables and objects",
    "text": "3.6 Variables and objects\nAn object is a data structure having attributes and methods. In fact, everything in R is an object!\nA variable is a type of data object. Data objects also include list, vector, matrices and text.\n\nCreating a data object\n\nIn R a variable can be created by using the symbol <- to assign a value to a variable name. The variable name is entered on the left <- and the value on the right. Note: Data objects can be given any name, provided that they start with a letter of the alphabet, and include only letters of the alphabet, numbers and the characters . and _. Hence AgeGroup, Age_Group and Age.Group are all valid names for an R data object. Note also that R is case-sensitive, to agegroup and AgeGroup would be treated as different data objects.\nTo save the value 28 to a variable (data object) labelled age, run the code:\n\nage <- 28\n\n\nInspecting a data object\n\nTo inspect the contents of the data object age run the following line of code:\n\nage\n\n[1] 28\n\n\nFind out what kind (class) of data object age is using:\n\nclass(age) \n\n[1] \"numeric\"\n\n\nInspect the structure of the age data object:\n\nstr(age) \n\n num 28\n\n\n\nThe vector data object\n\nWhat if we have more than one response? We can use the c( ) function to combine multiple values into one data vector object:\n\nage <- c(28, 36, 25, 24, 32)\nage\n\n[1] 28 36 25 24 32\n\nclass(age) #Still numeric..\n\n[1] \"numeric\"\n\nstr(age) #..but now a vector (set) of 5 separate values\n\n num [1:5] 28 36 25 24 32\n\n\nNote that on each line in the code above any text following the # character is ignored by R when executing the code. Instead, text following a # can be used to add comments to the code to make clear what the code is doing. Two marks of good code are a clear layout and clear commentary on the code.\n\n3.6.1 Basic Data Types\nThere are a number of data types. Four are the most common. In R, numeric is the default type for numbers. It stores all numbers as floating-point numbers (numbers with decimals). This is because most statistical calculations deal with numbers with up to two decimals.\n\nNumeric\n\n\nnum <- 4.5 # Decimal values\nclass(num)\n\n[1] \"numeric\"\n\n\n\nInteger\n\n\nint <- as.integer(4) # Natural numbers. Note integers are also numerics.\nclass(int)\n\n[1] \"integer\"\n\n\n\nCharacter\n\n\ncha <- \"are you enjoying this?\" # text or string. You can also type `as.character(\"are you enjoying this?\")`\nclass(cha)\n\n[1] \"character\"\n\n\n\nLogical\n\n\nlog <- 2 < 1 # assigns TRUE or FALSE. In this case, FALSE as 2 is greater than 1\nlog\n\n[1] FALSE\n\nclass(log)\n\n[1] \"logical\"\n\n\n\n\n3.6.2 Random Variables\nIn statistics, we differentiate between data to capture:\n\nQualitative attributes categorise objects eg.gender, marital status. To measure these attributes, we use Categorical data which can be divided into:\n\nNominal data in categories that have no inherent order eg. gender\nOrdinal data in categories that have an inherent order eg. income bands\n\nQuantitative attributes:\n\nDiscrete data: count objects of a certain category eg. number of kids, cars\nContinuous data: precise numeric measures eg. weight, income, length.\n\n\nIn R these three types of random variables are represented by the following types of R data object:\n\n\n\n\n\n\n\nvariables\n\n\nobjects\n\n\n\n\n\n\nnominal\n\n\nfactor\n\n\n\n\nordinal\n\n\nordered factor\n\n\n\n\ndiscrete\n\n\nnumeric\n\n\n\n\ncontinuous\n\n\nnumeric\n\n\n\n\n\nSurvey and R data types\n\n\nWe have already encountered the R data type numeric. The next section introduces the factor data type.\n\n3.6.2.1 Factor\nWhat is a factor?\nA factor variable assigns a numeric code to each possible category (level) in a variable. Behind the scenes, R stores the variable using these numeric codes to save space and speed up computing. For example, compare the size of a list of 10,000 males and females to a list of 10,000 1s and 0s. At the same time R also saves the category names associated with each numeric code (level). These are used for display purposes.\nFor example, the variable gender, converted to a factor, would be stored as a series of 1s and 2s, where 1 = female and 2 = male; but would be displayed in all outputs using their category labels of female and male.\nCreating a factor\nTo convert a numeric or character vector into a factor use the factor( ) function. For instance:\n\ngender <- c(\"female\",\"male\",\"male\",\"female\",\"female\") # create a gender variable\ngender <- factor(gender) # replace character vector with a factor version\ngender\n\n[1] female male   male   female female\nLevels: female male\n\nclass(gender)\n\n[1] \"factor\"\n\nstr(gender)\n\n Factor w/ 2 levels \"female\",\"male\": 1 2 2 1 1\n\n\nNow gender is a factor and is stored as a series of 1s and 2s, with 1s representing females and 2s representing males. The function levels( ) lists the levels (categories) associated with a given factor variable:\n\nlevels(gender)\n\n[1] \"female\" \"male\"  \n\n\nThe categories are reported in the order that they have been numbered (starting from 1). Hence from the output we can infer that females are coded as 1, and males as 2."
  },
  {
    "objectID": "03-data-wrangling.html#data-frames",
    "href": "03-data-wrangling.html#data-frames",
    "title": "3  Data Wrangling",
    "section": "3.7 Data Frames",
    "text": "3.7 Data Frames\nR stores different types of data using different types of data structure. Data are normally stored as a data.frame. A data frames contain one row per observation (e.g. wards) and one column per attribute (eg. population and health).\nWe create three variables wards, population (pop) and people with good health (ghealth). We use 2011 census data counts for total population and good health for wards in Liverpool.\n\nwards <- c(\"Allerton and Hunts Cross\",\"Anfield\",\"Belle Vale\",\"Central\",\"Childwall\",\"Church\",\"Clubmoor\",\"County\",\"Cressington\",\"Croxteth\",\"Everton\",\"Fazakerley\",\"Greenbank\",\"Kensington and Fairfield\",\"Kirkdale\",\"Knotty Ash\",\"Mossley Hill\",\"Norris Green\",\"Old Swan\",\"Picton\",\"Princes Park\",\"Riverside\",\"St Michael's\",\"Speke-Garston\",\"Tuebrook and Stoneycroft\",\"Warbreck\",\"Wavertree\",\"West Derby\",\"Woolton\",\"Yew Tree\")\n\npop <- c(14853,14510,15004,20340,13908,13974,15272,14045,14503,\n                14561,14782,16786,16132,15377,16115,13312,13816,15047,\n                16461,17009,17104,18422,12991,20300,16489,16481,14772,\n                14382,12921,16746)\n\nghealth <- c(7274,6124,6129,11925,7219,7461,6403,5930,7094,6992,\n                 5517,7879,8990,6495,6662,5981,7322,6529,7192,7953,\n                 7636,9001,6450,8973,7302,7521,7268,7013,6025,7717)\n\nNote that pop and ghealth and wards contains characters.\n\n3.7.1 Creating A Data Frame\nWe can create a data frame and examine its structure:\n\ndf <- data.frame(wards, pop, ghealth)\ndf # or use view(data)\n\n                      wards   pop ghealth\n1  Allerton and Hunts Cross 14853    7274\n2                   Anfield 14510    6124\n3                Belle Vale 15004    6129\n4                   Central 20340   11925\n5                 Childwall 13908    7219\n6                    Church 13974    7461\n7                  Clubmoor 15272    6403\n8                    County 14045    5930\n9               Cressington 14503    7094\n10                 Croxteth 14561    6992\n11                  Everton 14782    5517\n12               Fazakerley 16786    7879\n13                Greenbank 16132    8990\n14 Kensington and Fairfield 15377    6495\n15                 Kirkdale 16115    6662\n16               Knotty Ash 13312    5981\n17             Mossley Hill 13816    7322\n18             Norris Green 15047    6529\n19                 Old Swan 16461    7192\n20                   Picton 17009    7953\n21             Princes Park 17104    7636\n22                Riverside 18422    9001\n23             St Michael's 12991    6450\n24            Speke-Garston 20300    8973\n25 Tuebrook and Stoneycroft 16489    7302\n26                 Warbreck 16481    7521\n27                Wavertree 14772    7268\n28               West Derby 14382    7013\n29                  Woolton 12921    6025\n30                 Yew Tree 16746    7717\n\nstr(df) # or use glimpse(data) \n\n'data.frame':   30 obs. of  3 variables:\n $ wards  : chr  \"Allerton and Hunts Cross\" \"Anfield\" \"Belle Vale\" \"Central\" ...\n $ pop    : num  14853 14510 15004 20340 13908 ...\n $ ghealth: num  7274 6124 6129 11925 7219 ...\n\n\n\n\n3.7.2 Referencing Data Frames\nThroughout this module, you will need to refer to particular parts of a dataframe - perhaps a particular column (an area attribute); or a particular subset of respondents. Hence it is worth spending some time now mastering this particular skill.\nThe relevant R function, [ ], has the format [row,col] or, more generally, [set of rows, set of cols].\nRun the following commands to get a feel of how to extract different slices of the data:\n\ndf # whole data.frame\ndf[1, 1] # contents of first row and column\ndf[2, 2:3] # contents of the second row, second and third columns\ndf[1, ] # first row, ALL columns [the default if no columns specified]\ndf[ ,1:2] # ALL rows; first and second columns\ndf[c(1,3,5), ] # rows 1,3,5; ALL columns\ndf[ , 2] # ALL rows; second column (by default results containing only \n             #one column are converted back into a vector)\ndf[ , 2, drop=FALSE] # ALL rows; second column (returned as a data.frame)\n\nIn the above, note that we have used two other R functions:\n\n1:3 The colon operator tells R to produce a list of numbers including the named start and end points.\nc(1,3,5) Tells R to combine the contents within the brackets into one list of objects\n\nRun both of these fuctions on their own to get a better understanding of what they do.\nThree other methods for referencing the contents of a data.frame make direct use of the variable names within the data.frame, which tends to make for easier to read/understand code:\n\ndf[,\"pop\"] # variable name in quotes inside the square brackets\ndf$pop # variable name prefixed with $ and appended to the data.frame name\n# or you can use attach\nattach(df)\npop # but be careful if you already have an age variable in your local workspace\n\nWant to check the variables available, use the names( ):\n\nnames(df)\n\n[1] \"wards\"   \"pop\"     \"ghealth\""
  },
  {
    "objectID": "03-data-wrangling.html#sec_readdata",
    "href": "03-data-wrangling.html#sec_readdata",
    "title": "3  Data Wrangling",
    "section": "3.8 Read Data",
    "text": "3.8 Read Data\nEnsure your memory is clear\n\nrm(list=ls()) # rm for targeted deletion / ls for listing all existing objects\n\nThere are many commands to read / load data onto R. The command to use will depend upon the format they have been saved. Normally they are saved in csv format from Excel or other software packages. So we use either:\n\ndf <- read.table(\"path/file_name.csv\", header = FALSE, sep =\",\")\ndf <- read(\"path/file_name.csv\", header = FALSE)\ndf <- read.csv2(\"path/file_name.csv\", header = FALSE)\n\nTo read files in other formats, refer to this useful DataCamp tutorial\n\ncensus <- read.csv(\"data/census/census_data.csv\")\nhead(census)\n\n       code                     ward pop16_74 higher_managerial   pop ghealth\n1 E05000886 Allerton and Hunts Cross    10930              1103 14853    7274\n2 E05000887                  Anfield    10712               312 14510    6124\n3 E05000888               Belle Vale    10987               432 15004    6129\n4 E05000889                  Central    19174              1346 20340   11925\n5 E05000890                Childwall    10410              1123 13908    7219\n6 E05000891                   Church    10569              1843 13974    7461\n\n# NOTE: always ensure your are setting the correct directory leading to the data. \n# It may differ from your existing working directory\n\n\n3.8.1 Quickly inspect the data\n\nWhat class?\nWhat R data types?\nWhat data types?\n\n\n# 1\nclass(census)\n# 2 & 3\nstr(census)\n\nJust interested in the variable names:\n\nnames(census)\n\n[1] \"code\"              \"ward\"              \"pop16_74\"         \n[4] \"higher_managerial\" \"pop\"               \"ghealth\"          \n\n\nor want to view the data:\nView(census)"
  },
  {
    "objectID": "03-data-wrangling.html#manipulation-data",
    "href": "03-data-wrangling.html#manipulation-data",
    "title": "3  Data Wrangling",
    "section": "3.9 Manipulation Data",
    "text": "3.9 Manipulation Data\n\n3.9.1 Adding New Variables\nUsually you want to add / create new variables to your data frame using existing variables eg. computing percentages by dividing two variables. There are many ways in which you can do this i.e. referecing a data frame as we have done above, or using $ (e.g. census$pop). For this module, we’ll use tidyverse:\n\ncensus <- census %>% mutate(per_ghealth = ghealth / pop)\n\nNote we used a pipe operator %>%, which helps make the code more efficient and readable - more details, see Grolemund and Wickham (2019). When using the pipe operator, recall to first indicate the data frame before %>%.\nNote also the use a variable name before the = sign in brackets to indicate the name of the new variable after mutate.\n\n\n3.9.2 Selecting Variables\nUsually you want to select a subset of variables for your analysis as storing to large data sets in your R memory can reduce the processing speed of your machine. A selection of data can be achieved by using the select function:\n\nndf <- census %>% select(ward, pop16_74, per_ghealth)\n\nAgain first indicate the data frame and then the variable you want to select to build a new data frame. Note the code chunk above has created a new data frame called ndf. Explore it.\n\n\n3.9.3 Filtering Data\nYou may also want to filter values based on defined conditions. You may want to filter observations greater than a certain threshold or only areas within a certain region. For example, you may want to select areas with a percentage of good health population over 50%:\n\nndf2 <- census %>% filter(per_ghealth < 0.5)\n\nYou can use more than one variables to set conditions. Use “,” to add a condition.\n\n\n3.9.4 Joining Data Drames\nWhen working with spatial data, we often need to join data. To this end, you need a common unique id variable. Let’s say, we want to add a data frame containing census data on households for Liverpool, and join the new attributes to one of the existing data frames in the workspace. First we will read the data frame we want to join (ie. census_data2.csv).\n\n# read data\ncensus2 <- read.csv(\"data/census/census_data2.csv\")\n# visualise data structure\nstr(census2)\n\n'data.frame':   30 obs. of  3 variables:\n $ geo_code               : chr  \"E05000886\" \"E05000887\" \"E05000888\" \"E05000889\" ...\n $ households             : int  6359 6622 6622 7139 5391 5884 6576 6745 6317 6024 ...\n $ socialrented_households: int  827 1508 2818 1311 374 178 2859 1564 1023 1558 ...\n\n\nThe variable geo_code in this data frame corresponds to the code in the existing data frame and they are unique so they can be automatically matched by using the merge() function. The merge() function uses two arguments: x and y. The former refers to data frame 1 and the latter to data frame 2. Both of these two data frames must have a id variable containing the same information. Note they can have different names. Another key argument to include is all.x=TRUE which tells the function to keep all the records in x, but only those in y that match in case there are discrepancies in the id variable.\n\n# join data frames\njoin_dfs <- merge(census, census2, by.x=\"code\", by.y=\"geo_code\", all.x = TRUE)\n# check data\nhead(join_dfs)\n\n       code                     ward pop16_74 higher_managerial   pop ghealth\n1 E05000886 Allerton and Hunts Cross    10930              1103 14853    7274\n2 E05000887                  Anfield    10712               312 14510    6124\n3 E05000888               Belle Vale    10987               432 15004    6129\n4 E05000889                  Central    19174              1346 20340   11925\n5 E05000890                Childwall    10410              1123 13908    7219\n6 E05000891                   Church    10569              1843 13974    7461\n  per_ghealth households socialrented_households\n1   0.4897327       6359                     827\n2   0.4220538       6622                    1508\n3   0.4084911       6622                    2818\n4   0.5862832       7139                    1311\n5   0.5190538       5391                     374\n6   0.5339201       5884                     178\n\n\n\n\n3.9.5 Saving Data\nIt may also be convinient to save your R projects. They contains all the objects that you have created in your workspace by using the save.image( ) function:\n\nsave.image(\"week1_envs453.RData\")\n\nThis creates a file labelled “week1_envs453.RData” in your working directory. You can load this at a later stage using the load( ) function.\n\nload(\"week1_envs453.RData\")\n\nAlternatively you can save / export your data into a csv file. The first argument in the function is the object name, and the second: the name of the csv we want to create.\n\nwrite.csv(join_dfs, \"join_censusdfs.csv\")"
  },
  {
    "objectID": "03-data-wrangling.html#using-spatial-data-frames",
    "href": "03-data-wrangling.html#using-spatial-data-frames",
    "title": "3  Data Wrangling",
    "section": "3.10 Using Spatial Data Frames",
    "text": "3.10 Using Spatial Data Frames\nA core area of this module is learning to work with spatial data in R. R has various purposedly designed packages for manipulation of spatial data and spatial analysis techniques. Various R packages exist in CRAN eg. spatial, sgeostat, splancs, maptools, tmap, rgdal, spand and more recent development of sf - see Lovelace, Nowosad, and Muenchow (2019) for a great description and historical context for some of these packages.\nDuring this session, we will use sf.\nWe first need to import our spatial data. We will use a shapefile containing data at Output Area (OA) level for Liverpool. These data illustrates the hierarchical structure of spatial data.\n\n3.10.1 Read Spatial Data\n\noa_shp <- st_read(\"data/census/Liverpool_OA.shp\")\n\nReading layer `Liverpool_OA' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/census/Liverpool_OA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1584 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1\nProjected CRS: Transverse_Mercator\n\n\nExamine the input data. A spatial data frame stores a range of attributes derived from a shapefile including the geometry of features (e.g. polygon shape and location), attributes for each feature (stored in the .dbf), projection and coordinates of the shapefile’s bounding box - for details, execute:\n\n?st_read\n\nYou can employ the usual functions to visualise the content of the created data frame:\n\n# visualise variable names\nnames(oa_shp)\n\n [1] \"OA_CD\"    \"LSOA_CD\"  \"MSOA_CD\"  \"LAD_CD\"   \"pop\"      \"H_Vbad\"  \n [7] \"H_bad\"    \"H_fair\"   \"H_good\"   \"H_Vgood\"  \"age_men\"  \"age_med\" \n[13] \"age_60\"   \"S_Rent\"   \"Ethnic\"   \"illness\"  \"unemp\"    \"males\"   \n[19] \"geometry\"\n\n# data structure\nstr(oa_shp)\n\nClasses 'sf' and 'data.frame':  1584 obs. of  19 variables:\n $ OA_CD   : chr  \"E00176737\" \"E00033515\" \"E00033141\" \"E00176757\" ...\n $ LSOA_CD : chr  \"E01033761\" \"E01006614\" \"E01006546\" \"E01006646\" ...\n $ MSOA_CD : chr  \"E02006932\" \"E02001358\" \"E02001365\" \"E02001369\" ...\n $ LAD_CD  : chr  \"E08000012\" \"E08000012\" \"E08000012\" \"E08000012\" ...\n $ pop     : int  185 281 208 200 321 187 395 320 316 214 ...\n $ H_Vbad  : int  1 2 3 7 4 4 5 9 5 4 ...\n $ H_bad   : int  2 20 10 8 10 25 19 22 25 17 ...\n $ H_fair  : int  9 47 22 17 32 70 42 53 55 39 ...\n $ H_good  : int  53 111 71 52 112 57 131 104 104 53 ...\n $ H_Vgood : int  120 101 102 116 163 31 198 132 127 101 ...\n $ age_men : num  27.9 37.7 37.1 33.7 34.2 ...\n $ age_med : num  25 36 32 29 34 53 23 30 34 29 ...\n $ age_60  : num  0.0108 0.1637 0.1971 0.1 0.1402 ...\n $ S_Rent  : num  0.0526 0.176 0.0235 0.2222 0.0222 ...\n $ Ethnic  : num  0.3514 0.0463 0.0192 0.215 0.0779 ...\n $ illness : int  185 281 208 200 321 187 395 320 316 214 ...\n $ unemp   : num  0.0438 0.121 0.1121 0.036 0.0743 ...\n $ males   : int  122 128 95 120 158 123 207 164 157 94 ...\n $ geometry:sfc_MULTIPOLYGON of length 1584; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:14, 1:2] 335106 335130 335164 335173 335185 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:18] \"OA_CD\" \"LSOA_CD\" \"MSOA_CD\" \"LAD_CD\" ...\n\n# see first few observations\nhead(oa_shp)\n\nSimple feature collection with 6 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479\nProjected CRS: Transverse_Mercator\n      OA_CD   LSOA_CD   MSOA_CD    LAD_CD pop H_Vbad H_bad H_fair H_good\n1 E00176737 E01033761 E02006932 E08000012 185      1     2      9     53\n2 E00033515 E01006614 E02001358 E08000012 281      2    20     47    111\n3 E00033141 E01006546 E02001365 E08000012 208      3    10     22     71\n4 E00176757 E01006646 E02001369 E08000012 200      7     8     17     52\n5 E00034050 E01006712 E02001375 E08000012 321      4    10     32    112\n6 E00034280 E01006761 E02001366 E08000012 187      4    25     70     57\n  H_Vgood  age_men age_med     age_60     S_Rent     Ethnic illness      unemp\n1     120 27.94054      25 0.01081081 0.05263158 0.35135135     185 0.04379562\n2     101 37.71174      36 0.16370107 0.17600000 0.04626335     281 0.12101911\n3     102 37.08173      32 0.19711538 0.02352941 0.01923077     208 0.11214953\n4     116 33.73000      29 0.10000000 0.22222222 0.21500000     200 0.03597122\n5     163 34.19003      34 0.14018692 0.02222222 0.07788162     321 0.07428571\n6      31 56.09091      53 0.44919786 0.88524590 0.11764706     187 0.44615385\n  males                       geometry\n1   122 MULTIPOLYGON (((335106.3 38...\n2   128 MULTIPOLYGON (((335810.5 39...\n3    95 MULTIPOLYGON (((336738 3931...\n4   120 MULTIPOLYGON (((335914.5 39...\n5   158 MULTIPOLYGON (((339325 3914...\n6   123 MULTIPOLYGON (((338198.1 39...\n\n\nTASK:\n\nWhat are the geographical hierarchy in these data?\nWhat is the smallest geography?\nWhat is the largest geography?\n\n\n\n3.10.2 Basic Mapping\nAgain, many functions exist in CRAN for creating maps:\n\nplot to create static maps\ntmap to create static and interactive maps\nleaflet to create interactive maps\nmapview to create interactive maps\nggplot2 to create data visualisations, including static maps\nshiny to create web applications, including maps\n\nHere this notebook demonstrates the use of plot and tmap. First plot is used to map the spatial distribution of non-British-born population in Liverpool. First we only map the geometries on the right,\n\n3.10.2.1 Using plot\n\n# mapping geometry\nplot(st_geometry(oa_shp))\n\n\n\n\nOAs of Livepool\n\n\n\n\nand then:\n\n# map attributes, adding intervals\nplot(oa_shp[\"Ethnic\"], key.pos = 4, axes = TRUE, key.width = lcm(1.3), key.length = 1.,\n     breaks = \"jenks\", lwd = 0.1, border = 'grey') \n\n\n\n\nSpatial distribution of ethnic groups, Liverpool\n\n\n\n\nTASK:\n\nWhat is the key pattern emerging from this map?\n\n\n\n3.10.2.2 Using tmap\nSimilar to ggplot2, tmap is based on the idea of a ‘grammar of graphics’ which involves a separation between the input data and aesthetics (i.e. the way data are visualised). Each data set can be mapped in various different ways, including location as defined by its geometry, colour and other features. The basic building block is tm_shape() (which defines input data), followed by one or more layer elements such as tm_fill() and tm_dots().\n\n# ensure geometry is valid\noa_shp = sf::st_make_valid(oa_shp)\n\n# map\nlegend_title = expression(\"% ethnic pop.\")\nmap_oa = tm_shape(oa_shp) +\n  tm_fill(col = \"Ethnic\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .01)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 4) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position =  c(\"center\", \"bottom\")) # add scale bar\nmap_oa\n\n\n\n\nNote that the operation + is used to add new layers. You can set style themes by tm_style. To visualise the existing styles use tmap_style_catalogue(), and you can also evaluate the code chunk below if you would like to create an interactive map.\n\ntmap_mode(\"view\")\nmap_oa\n\nTASK:\n\nTry mapping other variables in the spatial data frame. Where do population aged 60 and over concentrate?\n\n\n\n\n3.10.3 Comparing geographies\nIf you recall, one of the key issues of working with spatial data is the modifiable area unit problem (MAUP) - see lecture notes. To get a sense of the effects of MAUP, we analyse differences in the spatial patterns of the ethnic population in Liverpool between Middle Layer Super Output Areas (MSOAs) and OAs. So we map these geographies together.\n\n# read data at the msoa level\nmsoa_shp <- st_read(\"data/census/Liverpool_MSOA.shp\")\n\nReading layer `Liverpool_MSOA' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/census/Liverpool_MSOA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1\nProjected CRS: Transverse_Mercator\n\n# ensure geometry is valid\nmsoa_shp = sf::st_make_valid(msoa_shp)\n\n# create a map\nmap_msoa = tm_shape(msoa_shp) +\n  tm_fill(col = \"Ethnic\", title = legend_title, palette = magma(256), style = \"cont\") + \n  tm_borders(col = \"white\", lwd = .01)  + \n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 4) + \n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position =  c(\"center\", \"bottom\")) \n\n# arrange maps \ntmap_arrange(map_msoa, map_oa) \n\n\n\n\nTASK:\n\nWhat differences do you see between OAs and MSOAs?\nCan you identify areas of spatial clustering? Where are they?"
  },
  {
    "objectID": "03-data-wrangling.html#useful-functions",
    "href": "03-data-wrangling.html#useful-functions",
    "title": "3  Data Wrangling",
    "section": "3.11 Useful Functions",
    "text": "3.11 Useful Functions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nread.csv()\nread csv files into data frames\n\n\nstr()\ninspect data structure\n\n\nmutate()\ncreate a new variable\n\n\nfilter()\nfilter observations based on variable values\n\n\n%>%\npipe operator - chain operations\n\n\nselect()\nselect variables\n\n\nmerge()\njoin dat frames\n\n\nst_read\nread spatial data (ie. shapefiles)\n\n\nplot()\ncreate a map based a spatial data set\n\n\ntm_shape(), tm_fill(), tm_borders()\ncreate a map using tmap functions\n\n\ntm_arrange\ndisplay multiple maps in a single “metaplot”\n\n\n\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2019. R for Data Science. O’Reilly, US. https://r4ds.had.co.nz.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. Chapman; Hall/CRC. https://doi.org/10.1201/9780203730058.\n\n\nWilliamson, Paul. 2018. “Survey Analysis.”\n\n\nXie, Yihui, JJ Allaire, and Garrett Grolemund. 2019. R Markdown: The Definitive Guide. CRC Press, Taylor & Francis, Chapman & Hall Book. https://bookdown.org/yihui/rmarkdown/."
  },
  {
    "objectID": "04-points.html#dependencies",
    "href": "04-points.html#dependencies",
    "title": "4  Point Data Analysis",
    "section": "4.1 Dependencies",
    "text": "4.1 Dependencies\nWe will rely on the following libraries in this section, all of them included in Section 1.4.1:\n\n# For pretty table\nlibrary(knitr)\n# All things geodata\nlibrary(sf)\nlibrary(sp)\n# Pretty graphics\nlibrary(ggplot2)\nlibrary(gridExtra)\n# Thematic maps\nlibrary(tmap)\n# Pretty maps\nlibrary(ggmap)\n# For all your interpolation needs\nlibrary(gstat)\n# For data manipulation\nlibrary(plyr)\n\nBefore we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the house_transactions folder with the data lives.\n\nsetwd('.')"
  },
  {
    "objectID": "04-points.html#data",
    "href": "04-points.html#data",
    "title": "4  Point Data Analysis",
    "section": "4.2 Data",
    "text": "4.2 Data\nFor this session, we will use the set of Airbnb properties for San Diego (US), borrowed from the “Geographic Data Science with Python” book (see here for more info on the dataset source). This covers the point location of properties advertised on the Airbnb website in the San Diego region.\nLet us start by reading the data, which comes in a GeoJSON:\n\ndb <- st_read(\"data/abb_sd/regression_db.geojson\")\n\nReading layer `regression_db' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/abb_sd/regression_db.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6110 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2812 ymin: 32.57349 xmax: -116.9553 ymax: 33.08311\nGeodetic CRS:  WGS 84\n\n\nWe can then examine the columns of the table with the colnames method:\n\ncolnames(db)\n\n [1] \"accommodates\"       \"bathrooms\"          \"bedrooms\"          \n [4] \"beds\"               \"neighborhood\"       \"pool\"              \n [7] \"d2balboa\"           \"coastal\"            \"price\"             \n[10] \"log_price\"          \"id\"                 \"pg_Apartment\"      \n[13] \"pg_Condominium\"     \"pg_House\"           \"pg_Other\"          \n[16] \"pg_Townhouse\"       \"rt_Entire_home.apt\" \"rt_Private_room\"   \n[19] \"rt_Shared_room\"     \"geometry\"          \n\n\nThe rest of this session will focus on two main elements of the table: the spatial dimension (as stored in the point coordinates), and the nightly price values, expressed in USD and contained in the price column. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands:\n\n# Create the histogram\nhist <- qplot(data=db,x=price)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\nhist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nRaw AirBnb prices in San Diego\n\n\n\n\nThis basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to shrink these differences is to take logarithms. The original table already contains an additional column with the logarithm of each price (log_price).\n\n# Create the histogram\nhist <- qplot(data=db, x=log_price)\nhist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLog of AirBnb price in San Diego\n\n\n\n\nTo obtain the spatial distribution of these houses, we need to focus on the geometry column. The easiest, quickest (and also “dirtiest”) way to get a sense of what the data look like over space is using plot:\n\nplot(st_geometry(db))\n\n\n\n\nSpatial distribution of AirBnb in San Diego\n\n\n\n\nNow this has the classic problem of cluttering: some portions of the map have so many points that we can’t tell what the distribution is like. To get around this issue, there are two solutions: binning and smoothing."
  },
  {
    "objectID": "04-points.html#binning",
    "href": "04-points.html#binning",
    "title": "4  Point Data Analysis",
    "section": "4.3 Binning",
    "text": "4.3 Binning\nThe two-dimensional sister of histograms are binning maps: we divide each of the two dimensions into “buckets”, and count how many points fall within each bucket. Unlike histograms, we encode that count with a color gradient rather than a bar chart over an additional dimension (for that, we would need a 3D plot). These “buckets” can be squares (left) or hexagons (right):\n\n      # Squared binning\n# Set up plot\nsqbin <- ggplot() + \n# Add 2D binning with the XY coordinates as\n# a dataframe\n  geom_bin2d(\n    data=as.data.frame(st_coordinates(db)), \n    aes(x=X, y=Y)\n  )\n      # Hex binning\n# Set up plot\nhexbin <- ggplot() +\n# Add hex binning with the XY coordinates as\n# a dataframe \n  geom_hex(\n    data=as.data.frame(st_coordinates(db)),\n    aes(x=X, y=Y)\n  ) +\n# Use viridis for color encoding (recommended)\n  scale_fill_continuous(type = \"viridis\")\n      # Bind in subplots\ngrid.arrange(sqbin, hexbin, ncol=2)\n\nWarning: Computation failed in `stat_binhex()`\nCaused by error in `compute_group()`:\n! The package `hexbin` is required for `stat_binhex()`"
  },
  {
    "objectID": "04-points.html#kde",
    "href": "04-points.html#kde",
    "title": "4  Point Data Analysis",
    "section": "4.4 KDE",
    "text": "4.4 KDE\nKernel Density Estimation (KDE) is a technique that creates a continuous representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions.\n\n4.4.1 One-dimensional KDE\nKDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before:\n\n# Create the base\nbase <- ggplot(db, aes(x=log_price))\n# Histogram\nhist <- base + \n  geom_histogram(bins=50, aes(y=..density..))\n# Overlay density plot\nkde <- hist + \n  geom_density(fill=\"#FF6666\", alpha=0.5, colour=\"#FF6666\")\nkde\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHistogram and KDE of the log of AirBnb prices in San Diego\n\n\n\n\nThe key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts.\n\n\n4.4.2 Two-dimensional (spatial) KDE\nGeography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates, X and Y (or latitude and longitude). Thanks to that, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a surface, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above.\nTo create a spatial KDE in R, we can use general tooling for non-spatial points, such as the stat_density2d_filled method:\n\n# Create the KDE surface\nkde <- ggplot(data = db) +\n  stat_density2d_filled(\n    data = as.data.frame(st_coordinates(db)), \n    aes(x = X, y = Y, alpha = ..level..),\n    n = 16\n  ) +\n  # Tweak the color gradient\n  scale_color_viridis_c() +\n  # White theme\n  theme_bw()\n# Tip! Add invisible points to improve proportions\nkde + geom_sf(alpha=0)\n\n\n\n\nKDE of AirBnb properties in San Diego\n\n\n\n\nThis approach generates a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. To do it, we will leverage the library ggmap, which is designed to play nicely with the ggplot2 family (hence the seemingly counterintuitive example above). Before we can plot them with the online map, we need to reproject them though.\n\n# Reproject coordinates\nlon_lat <- st_transform(db, crs = 4326) %>%\n  st_coordinates() %>%\n  as.data.frame()\n# Basemap\nqmplot(\n  X, \n  Y, \n  data = lon_lat, \n  geom=\"blank\"\n) +\n  # KDE\n  stat_density2d_filled(\n    data = lon_lat, \n    aes(x = X, y = Y, alpha = ..level..),\n    n = 16\n  ) +\n  # Tweak the color gradient\n  scale_color_viridis_c()\n\n\n\n\nKDE of AirBnb properties in San Diego"
  },
  {
    "objectID": "04-points.html#spatial-interpolation",
    "href": "04-points.html#spatial-interpolation",
    "title": "4  Point Data Analysis",
    "section": "4.5 Spatial Interpolation",
    "text": "4.5 Spatial Interpolation\nThe previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house locations, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example).\nMany times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house prices in AirBnb, fits into this category as well: although where a house is advertised may be of relevance, more often we are interested in finding out what the “surface of price” looks like. Rather than where are most houses being advertised? we usually want to know where the most expensive or most affordable houses are located.\nIn cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is spatial interpolation, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g. temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible1. In this chapter, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, Banerjee, Carlin, and Gelfand (2014) or Cressie (2015) are hard but good overviews.\n\n4.5.1 Inverse Distance Weight (IDW) interpolation\nThe technique we will cover here is called Inverse Distance Weighting, or IDW for convenience. Brunsdon and Comber (2015) offer a good description:\n\nIn the inverse distance weighting (IDW) approach to interpolation, to estimate the value of \\(z\\) at location \\(x\\) a weighted mean of nearby observations is taken […]. To accommodate the idea that observations of \\(z\\) at points closer to \\(x\\) should be given more importance in the interpolation, greater weight is given to these points […]\n— Page 204\n\nThe math2 is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. The idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing.\nEnough chat, let’s code3.\nFrom what we have just mentioned, there are a few steps to perform an IDW spatial interpolation:\n\nCreate a regular grid over the area where we have house transactions.\nObtain IDW estimates for each point in the grid, based on the values of \\(k\\) nearest neighbors.\nPlot a smoothed version of the grid, effectively representing the surface of house prices.\n\nLet us go in detail into each of them4. First, let us set up a grid for the extent of the bounding box of our data (not the use of pipe, %>%, operator to chain functions):\n\nsd.grid <- db %>%\n  st_bbox() %>%\n  st_as_sfc() %>%\n  st_make_grid(\n    n = 100,\n    what = \"centers\"\n  ) %>%\n  st_as_sf() %>%\n  cbind(., st_coordinates(.))\n\nThe object sd.grid is a regular grid with 10,000 (\\(100 \\times 100\\)) equally spaced cells:\n\nsd.grid\n\nSimple feature collection with 10000 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -116.9569 ymax: 33.08056\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           X        Y                          x\n1  -117.2795 32.57604 POINT (-117.2795 32.57604)\n2  -117.2763 32.57604 POINT (-117.2763 32.57604)\n3  -117.2730 32.57604  POINT (-117.273 32.57604)\n4  -117.2698 32.57604 POINT (-117.2698 32.57604)\n5  -117.2665 32.57604 POINT (-117.2665 32.57604)\n6  -117.2632 32.57604 POINT (-117.2632 32.57604)\n7  -117.2600 32.57604   POINT (-117.26 32.57604)\n8  -117.2567 32.57604 POINT (-117.2567 32.57604)\n9  -117.2535 32.57604 POINT (-117.2535 32.57604)\n10 -117.2502 32.57604 POINT (-117.2502 32.57604)\n\n\nNow, sd.grid only contain the location of points to which we wish to interpolate. That is, we now have our “target” geography for which we’d like to have AirBnb prices, but we don’t have price estimates. For that, on to the IDW, which will generate estimates for locations in sd.grid based on the observed prices in db. Again, this is hugely simplified by gstat:\n\nidw.hp <- idw(\n  price ~ 1,         # Formula for IDW\n  locations = db,    # Initial locations with values\n  newdata=sd.grid,   # Locations we want predictions for\n  nmax = 150         # Limit the number of neighbours for IDW\n)\n\n[inverse distance weighted interpolation]\n\n\nBoom! We’ve got it. Let us pause for a second to see how we just did it. First, we pass price ~ 1. This specifies the formula we are using to model house prices. The name on the left of ~ represents the variable we want to explain, while everything to its right captures the explanatory variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply write 1. Then we specify the original locations for which we do have house prices (our original db object), and the points where we want to interpolate the house prices (the sd.grid object we just created above). One more note: by default, idw uses all the available observations, weighted by distance, to provide an estimate for a given point. If you want to modify that and restrict the maximum number of neighbors to consider, you need to tweak the argument nmax, as we do above by using the 150 nearest observations to each point5.\nThe object we get from idw is another spatial table, just as db, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table:\n\nhead(idw.hp)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -117.2632 ymax: 32.57604\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  295.6100       NA POINT (-117.2795 32.57604)\n2  295.1651       NA POINT (-117.2763 32.57604)\n3  296.5927       NA  POINT (-117.273 32.57604)\n4  288.2252       NA POINT (-117.2698 32.57604)\n5  281.5522       NA POINT (-117.2665 32.57604)\n6  268.3567       NA POINT (-117.2632 32.57604)\n\n\nThe column we will pay attention to is var1.pred. For a hypothetical house advertised at the location in the first row of point in sd.grid, the price IDW would guess it would cost, based on prices nearby, is the first element of column var1.pred in idw.hp.\n\n\n4.5.2 A surface of housing prices\nOnce we have the IDW object computed, we can plot it to explore the distribution, not of AirBnb locations in this case, but of house prices over the geography of San Diego. To do this using ggplot2, we first append the coordinates of each grid cell as columns of the table:\n\nidw.hp = idw.hp %>%\n  cbind(st_coordinates(.))\n\nNow, we can visualise the surface using standard ggplot2 tools:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster()\n\n\n\n\nAnd we can “dress it up” a bit further:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster() +\n  scale_fill_viridis_b() +\n  theme_void() +\n  geom_sf(alpha=0)\n\n\n\n\nLooking at this, we can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because ggmap –the library we have been using to display tiles from cloud providers– does not play well with our own rasters (i.e. the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now6.\n\n\n4.5.3 “What should the next house’s price be?”\nThe last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist working for Airbnb and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location.\nLet us say, a new house is going to be advertised on the coordinates X = -117.02259063720702, Y = 32.76511965117273 as expressed in longitude and latitude. In that case, we can do as follows:\n\npt <- c(X = -117.02259063720702, Y = 32.76511965117273) %>%\n  st_point() %>%\n  st_sfc() %>%\n  st_sf(crs = \"EPSG:4326\") %>%\n  st_transform(st_crs(db))\nidw.one <- idw(price ~ 1, locations=db, newdata=pt)\n\n[inverse distance weighted interpolation]\n\nidw.one\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.0226 ymin: 32.76512 xmax: -117.0226 ymax: 32.76512\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  171.4141       NA POINT (-117.0226 32.76512)\n\n\nAnd, as show above, the estimated value is $171.41413347."
  },
  {
    "objectID": "04-points.html#questions",
    "href": "04-points.html#questions",
    "title": "4  Point Data Analysis",
    "section": "4.6 Questions",
    "text": "4.6 Questions\nWe will be using the Madrid AirBnb dataset:\n\nmad_abb <- st_read(\"data/assignment_1_madrid/madrid_abb.gpkg\")\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\nThis is fairly similar in spirit to the one from San Diego we have relied on for the chapter, although the column set is not exactly the same:\n\ncolnames(mad_abb)\n\n [1] \"price\"           \"price_usd\"       \"log1p_price_usd\" \"accommodates\"   \n [5] \"bathrooms_text\"  \"bathrooms\"       \"bedrooms\"        \"beds\"           \n [9] \"neighbourhood\"   \"room_type\"       \"property_type\"   \"WiFi\"           \n[13] \"Coffee\"          \"Gym\"             \"Parking\"         \"km_to_retiro\"   \n[17] \"geom\"           \n\n\nFor this set of questions, the only two columns we will need is geom, which contains the point geometries, and price_usd, which record the price of the AirBnb property in USD.\nWith this at hand, answer the following questions:\n\nCreate a KDE that represents the density of locations of AirBnb properties in Madrid\nUsing inverse distance weighting, create a surface of AirBnb prices\n\n\n\n\n\nBanerjee, Sudipto, Bradley P Carlin, and Alan E Gelfand. 2014. Hierarchical Modeling and Analysis for Spatial Data. Crc Press.\n\n\nBivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with r. Springer New York. https://doi.org/10.1007/978-1-4614-7618-4.\n\n\nBrunsdon, Chris, and Lex Comber. 2015. An Introduction to r for Spatial Analysis & Mapping. Sage.\n\n\nCressie, Noel. 2015. Statistics for Spatial Data. John Wiley & Sons.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. Chapman; Hall/CRC. https://doi.org/10.1201/9780203730058."
  },
  {
    "objectID": "05-flows.html#dependencies",
    "href": "05-flows.html#dependencies",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.1 Dependencies",
    "text": "5.1 Dependencies\nWe will rely on the following libraries in this section, all of them included in Section 1.4.1:\n\n# Spatial Data management\nlibrary(sf)\nlibrary(rgdal)\n# Pretty graphics\nlibrary(ggplot2)\n# Thematic maps\nlibrary(tmap)\n# Pretty maps\nlibrary(ggmap)\n# Simulation methods\nlibrary(arm)\n\nIn this chapter we will show a slightly different way of managing spatial data in R. Although most of the functionality will be similar to that seen in previous chapters, we will not rely on the “sf stack” and we will instead show how to read and manipulate data using the more traditional sp stack. Although this approach is being slowly phased out, it is still important to be aware of its existence and its differences with more modern approaches.\nBefore we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the sf_bikes folder with the data lives.\n\nsetwd('.')"
  },
  {
    "objectID": "05-flows.html#data",
    "href": "05-flows.html#data",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.2 Data",
    "text": "5.2 Data\nIn this note, we will use data from the city of San Francisco representing bike trips on their public bike share system. The original source is the SF Open Data portal (link) and the dataset comprises both the location of each station in the Bay Area as well as information on trips (station of origin to station of destination) undertaken in the system from September 2014 to August 2015 and the following year. Since this note is about modeling and not data preparation, a cleanly reshaped version of the data, together with some additional information, has been created and placed in the sf_bikes folder. The data file is named flows.geojson and, in case you are interested, the (Python) code required to created from the original files in the SF Data Portal is also available on the flows_prep.ipynb notebook [url], also in the same folder.\nLet us then directly load the file with all the information necessary:\n\ndb <- readOGR('./data/sf_bikes/flows.geojson')\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nWarning: OGR support is provided by the sf and terra packages among others\n\n\nOGR data source with driver: GeoJSON \nSource: \"/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/sf_bikes/flows.geojson\", layer: \"flows\"\nwith 1722 features\nIt has 9 fields\n\nrownames(db@data) <- db$flow_id\ndb@data$flow_id <- NULL\n\nNote how the interface is slightly different since we are reading a GeoJSON file instead of a shapefile.\nThe data contains the geometries of the flows, as calculated from the Google Maps API, as well as a series of columns with characteristics of each flow:\n\nhead(db@data)\n\n      dest orig straight_dist street_dist total_down total_up trips15 trips16\n39-41   41   39      1452.201   1804.1150  11.205753 4.698162      68      68\n39-42   42   39      1734.861   2069.1557  10.290236 2.897886      23      29\n39-45   45   39      1255.349   1747.9928  11.015596 4.593927      83      50\n39-46   46   39      1323.303   1490.8361   3.511543 5.038044     258     163\n39-47   47   39       715.689    769.9189   0.000000 3.282495     127      73\n39-48   48   39      1996.778   2740.1290  11.375186 3.841296      81      56\n\n\nwhere orig and dest are the station IDs of the origin and destination, street/straight_dist is the distance in metres between stations measured along the street network or as-the-crow-flies, total_down/up is the total downhil and climb in the trip, and tripsXX contains the amount of trips undertaken in the years of study."
  },
  {
    "objectID": "05-flows.html#seeing-flows",
    "href": "05-flows.html#seeing-flows",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.3 “Seeing” flows",
    "text": "5.3 “Seeing” flows\nThe easiest way to get a quick preview of what the data looks like spatially is to make a simple plot:\n\nplot(db)\n\n\n\n\nPotential routes\n\n\n\n\nEqually, if we want to visualize a single route, we can simply subset the table. For example, to get the shape of the trip from station 39 to station 48, we can:\n\none39to48 <- db[ which(\n          db@data$orig == 39 & db@data$dest == 48\n          ) , ]\nplot(one39to48)\n\n\n\n\nTrip from station 39 to 48\n\n\n\n\nor, for the most popular route, we can:\n\nmost_pop <- db[ which(\n          db@data$trips15 == max(db@data$trips15)\n          ) , ]\nplot(most_pop)\n\n\n\n\nMost popular trip\n\n\n\n\nThese however do not reveal a lot: there is no geographical context (why are there so many routes along the NE?) and no sense of how volumes of bikers are allocated along different routes. Let us fix those two.\nThe easiest way to bring in geographical context is by overlaying the routes on top of a background map of tiles downloaded from the internet. Let us download this using ggmap:\n\nsf_bb <- c(\n  left=db@bbox['x', 'min'],\n  right=db@bbox['x', 'max'],\n  bottom=db@bbox['y', 'min'],\n  top=db@bbox['y', 'max']\n  )\nSanFran <- get_stamenmap(\n  sf_bb, \n  zoom = 14, \n  maptype = \"toner-lite\"\n  )\n\nand make sure it looks like we intend it to look:\n\nggmap(SanFran)\n\n\n\n\nNow to combine tiles and routes, we need to pull out the coordinates that make up each line. For the route example above, this would be:\n\nxys1 <- as.data.frame(coordinates(most_pop))\n\nNow we can plot the route1 (note we also dim down the background to focus the attention on flows):\n\nggmap(SanFran, darken=0.5) + \n  geom_path(\n    aes(x=X1, y=X2), \n    data=xys1,\n    size=1,\n    color=rgb(0.996078431372549, 0.7019607843137254, 0.03137254901960784),\n    lineend='round'\n    )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nNow we can plot all of the lines by using a short for loop to build up the table:\n\n# Set up shell data.frame\nlines <- data.frame(\n  lat = numeric(0), \n  lon = numeric(0), \n  trips = numeric(0),\n  id = numeric(0)\n)\n# Run loop\nfor(x in 1:nrow(db)){\n  # Pull out row\n  r <- db[x, ]\n  # Extract lon/lat coords\n  xys <- as.data.frame(coordinates(r))\n  names(xys) <- c('lon', 'lat')\n  # Insert trips and id\n  xys['trips'] <- r@data$trips15\n  xys['id'] <- x\n  # Append them to `lines`\n  lines <- rbind(lines, xys)\n}\n\nNow we can go on and plot all of them:\n\nggmap(SanFran, darken=0.75) + \n  geom_path(\n    aes(x=lon, y=lat, group=id),\n    data=lines,\n    size=0.1,\n    color=rgb(0.996078431372549, 0.7019607843137254, 0.03137254901960784),\n    lineend='round'\n  )\n\n\n\n\nFinally, we can get a sense of the distribution of the flows by associating a color gradient to each flow based on its number of trips:\n\nggmap(SanFran, darken=0.75) + \n  geom_path(\n    aes(x=lon, y=lat, group=id, colour=trips),\n    data=lines,\n    size=log1p(lines$trips / max(lines$trips)),\n    lineend='round'\n  ) +\n  scale_colour_gradient(\n    low='grey', high='#07eda0'\n  ) +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\nNote how we transform the size so it’s a proportion of the largest trip and then it is compressed with a logarithm."
  },
  {
    "objectID": "05-flows.html#modelling-flows",
    "href": "05-flows.html#modelling-flows",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.4 Modelling flows",
    "text": "5.4 Modelling flows\nNow we have an idea of the spatial distribution of flows, we can begin to think about modeling them. The core idea in this section is to fit a model that can capture the particular characteristics of our variable of interest (the volume of trips) using a set of predictors that describe the nature of a given flow. We will start from the simplest model and then progressively build complexity until we get to a satisfying point. Along the way, we will be exploring each model using concepts from Gelman and Hill (2006) such as predictive performance checks2 (PPC)\nBefore we start running regressions, let us first standardize the predictors so we can interpret the intercept as the average flow when all the predictors take the average value, and so we can interpret the model coefficients as changes in standard deviation units:\n\n# Scale all the table\ndb_std <- as.data.frame(scale(db@data))\n# Reset trips as we want the original version\ndb_std$trips15 <- db@data$trips15\ndb_std$trips16 <- db@data$trips16\n# Reset origin and destination station and express them as factors\ndb_std$orig <- as.factor(db@data$orig)\ndb_std$dest <- as.factor(db@data$dest)\n\nBaseline model\nOne of the simplest possible models we can fit in this context is a linear model that explains the number of trips as a function of the straight distance between the two stations and total amount of climb and downhill. We will take this as the baseline on which we can further build later:\n\nm1 <- lm('trips15 ~ straight_dist + total_up + total_down', data=db_std)\nsummary(m1)\n\n\nCall:\nlm(formula = \"trips15 ~ straight_dist + total_up + total_down\", \n    data = db_std)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-261.9 -168.3 -102.4   30.8 3527.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    182.070      8.110  22.451  < 2e-16 ***\nstraight_dist   17.906      9.108   1.966   0.0495 *  \ntotal_up       -44.100      9.353  -4.715 2.61e-06 ***\ntotal_down     -20.241      9.229  -2.193   0.0284 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 336.5 on 1718 degrees of freedom\nMultiple R-squared:  0.02196,   Adjusted R-squared:  0.02025 \nF-statistic: 12.86 on 3 and 1718 DF,  p-value: 2.625e-08\n\n\nTo explore how good this model is, we will be comparing the predictions the model makes about the number of trips each flow should have with the actual number of trips. A first approach is to simply plot the distribution of both variables:\n\nplot(\n  density(m1$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual'),\n  col=c('black', 'red'),\n  lwd=1\n)\ntitle(main=\"Predictive check, point estimates - Baseline model\")\n\n\n\n\nThe plot makes pretty obvious that our initial model captures very few aspects of the distribution we want to explain. However, we should not get too attached to this plot just yet. What it is showing is the distribution of predicted point estimates from our model. Since our model is not deterministic but inferential, there is a certain degree of uncertainty attached to its predictions, and that is completely absent from this plot.\nGenerally speaking, a given model has two sources of uncertainty: predictive, and inferential. The former relates to the fact that the equation we fit does not capture all the elements or in the exact form they enter the true data generating process; the latter has to do with the fact that we never get to know the true value of the model parameters only guesses (estimates) subject to error and uncertainty. If you think of our linear model above as\n\\[\nT_{ij} = X_{ij}\\beta + \\epsilon_{ij}\n\\] where \\(T_{ij}\\) represents the number of trips undertaken between station \\(i\\) and \\(j\\), \\(X_{ij}\\) is the set of explanatory variables (length, climb, descent, etc.), and \\(\\epsilon_{ij}\\) is an error term assumed to be distributed as a normal distribution \\(N(0, \\sigma)\\); then predictive uncertainty comes from the fact that there are elements to some extent relevant for \\(y\\) that are not accounted for and thus subsummed into \\(\\epsilon_{ij}\\). Inferential uncertainty comes from the fact that we never get to know \\(\\beta\\) but only an estimate of it which is also subject to uncertainty itself.\nTaking these two sources into consideration means that the black line in the plot above represents only the behaviour of our model we expect if the error term is absent (no predictive uncertainty) and the coefficients are the true estimates (no inferential uncertainty). However, this is not necessarily the case as our estimate for the uncertainty of the error term is certainly not zero, and our estimates for each parameter are also subject to a great deal of inferential variability. We do not know to what extent other outcomes would be just as likely. Predictive checking relates to simulating several feasible scenarios under our model and use those to assess uncertainty and to get a better grasp of the quality of our predictions.\nTechnically speaking, to do this, we need to build a mechanism to obtain a possible draw from our model and then repeat it several times. The first part of those two steps can be elegantly dealt with by writing a short function that takes a given model and a set of predictors, and produces a possible random draw from such model:\n\ngenerate_draw <- function(m){\n  # Set up predictors matrix\n  x <- model.matrix(m)\n  # Obtain draws of parameters (inferential uncertainty)\n  sim_bs <- sim(m, 1)\n  # Predicted value\n  mu <- x %*% sim_bs@coef[1, ]\n  # Draw\n  n <- length(mu)\n  y_hat <- rnorm(n, mu, sim_bs@sigma[1])\n  return(y_hat)\n}\n\nThis function takes a model m and the set of covariates x used and returns a random realization of predictions from the model. To get a sense of how this works, we can get and plot a realization of the model, compared to the expected one and the actual values:\n\nnew_y <- generate_draw(m1)\n\nplot(\n  density(m1$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n                   max(density(m1$fitted.values)$y), \n                   max(density(db_std$trips15)$y)\n                   )\n                )\n         ),\n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlines(\n  density(new_y), \n  col='green',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual', 'Simulated'),\n  col=c('black', 'red', 'green'),\n  lwd=1\n)\n\n\n\n\nOnce we have this “draw engine”, we can set it to work as many times as we want using a simple for loop. In fact, we can directly plot these lines as compared to the expected one and the trip count:\n\nplot(\n  density(m1$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n               max(density(m1$fitted.values)$y), \n               max(density(db_std$trips15)$y)\n               )\n            )\n     ),\n  col='white',\n  main=''\n)\n# Loop for realizations\nfor(i in 1:250){\n  tmp_y <- generate_draw(m1)\n  lines(density(tmp_y),\n        col='grey',\n        lwd=0.1\n        )\n}\n#\nlines(\n  density(m1$fitted.values), \n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Actual', 'Predicted', 'Simulated (n=250)'),\n  col=c('red', 'black', 'grey'),\n  lwd=1\n)\ntitle(main=\"Predictive check - Baseline model\")\n\n\n\n\nThe plot shows there is a significant mismatch between the fitted values, which are much more concentrated around small positive values, and the realizations of our “inferential engine”, which depict a much less concentrated distribution of values. This is likely due to the combination of two different reasons: on the one hand, the accuracy of our estimates may be poor, causing them to jump around a wide range of potential values and hence resulting in very diverse predictions (inferential uncertainty); on the other hand, it may be that the amount of variation we are not able to account for in the model3 is so large that the degree of uncertainty contained in the error term of the model is very large, hence resulting in such a flat predictive distribution.\nIt is important to keep in mind that the issues discussed in the paragraph above relate only to the uncertainty behind our model, not to the point predictions derived from them, which are a mechanistic result of the minimization of the squared residuals and hence are not subject to probability or inference. That allows them in this case to provide a fitted distribution much more accurate apparently (black line above). However, the lesson to take from this model is that, even if the point predictions (fitted values) are artificially accurate4, our capabilities to infer about the more general underlying process are fairly limited.\nImproving the model\nThe bad news from the previous section is that our initial model is not great at explaining bike trips. The good news is there are several ways in which we can improve this. In this section we will cover three main extensions that exemplify three different routes you can take when enriching and augmenting models in general, and spatial interaction ones in particular5. These three routes are aligned around the following principles:\n\nUse better approximations to model your dependent variable.\nRecognize the structure of your data.\nGet better predictors.\n\n\nUse better approximations to model your dependent variable\n\nStandard OLS regression assumes that the error term and, since the predictors are deterministic, the dependent variable are distributed following a normal (gaussian) distribution. This is usually a good approximation for several phenomena of interest, but maybe not the best one for trips along routes: for one, we know trips cannot be negative, which the normal distribution does not account for6; more subtly, their distribution is not really symmetric but skewed with a very long tail on the right. This is common in variables that represent counts and that is why usually it is more appropriate to fit a model that relies on a distribution different from the normal.\nOne of the most common distributions for this cases is the Poisson, which can be incorporated through a general linear model (or GLM). The underlying assumption here is that instead of \\(T_{ij} \\sim N(\\mu_{ij}, \\sigma)\\), our model now follows:\n\\[\nT_{ij} \\sim Poisson (\\exp^{X_{ij}\\beta})\n\\]\nAs usual, such a model is easy to run in R:\n\nm2 <- glm(\n  'trips15 ~ straight_dist + total_up + total_down', \n  data=db_std,\n  family=poisson,\n)\n\nNow let’s see how much better, if any, this approach is. To get a quick overview, we can simply plot the point predictions:\n\nplot(\n  density(m2$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n               max(density(m2$fitted.values)$y), \n               max(density(db_std$trips15)$y)\n               )\n            )\n   ),\n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual'),\n  col=c('black', 'red'),\n  lwd=1\n)\ntitle(main=\"Predictive check, point estimates - Poisson model\")\n\n\n\n\nTo incorporate uncertainty to these predictions, we need to tweak our generate_draw function so it accommodates the fact that our model is not linear anymore.\n\ngenerate_draw_poi <- function(m){\n  # Set up predictors matrix\n  x <- model.matrix(m)\n  # Obtain draws of parameters (inferential uncertainty)\n  sim_bs <- sim(m, 1)\n  # Predicted value\n  xb <- x %*% sim_bs@coef[1, ]\n  #xb <- x %*% m$coefficients\n  # Transform using the link function\n  mu <- exp(xb)\n  # Obtain a random realization\n  y_hat <- rpois(n=length(mu), lambda=mu)\n  return(y_hat)\n}\n\nAnd then we can examine both point predictions an uncertainty around them:\n\nplot(\n  density(m2$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n               max(density(m2$fitted.values)$y), \n               max(density(db_std$trips15)$y)\n               )\n            )\n   ),\n  col='white',\n  main=''\n)\n# Loop for realizations\nfor(i in 1:250){\n  tmp_y <- generate_draw_poi(m2)\n  lines(\n    density(tmp_y),\n    col='grey',\n    lwd=0.1\n  )\n}\n#\nlines(\n  density(m2$fitted.values), \n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual', 'Simulated (n=250)'),\n  col=c('black', 'red', 'grey'),\n  lwd=1\n)\ntitle(main=\"Predictive check - Poisson model\")\n\n\n\n\nVoila! Although the curve is still a bit off, centered too much to the right of the actual data, our predictive simulation leaves the fitted values right in the middle. This speaks to a better fit of the model to the actual distribution othe original data follow.\n\nRecognize the structure of your data\n\nSo far, we’ve treated our dataset as if it was flat (i.e. comprise of fully independent realizations) when in fact it is not. Most crucially, our baseline model does not account for the fact that every observation in the dataset pertains to a trip between two stations. This means that all the trips from or to the same station probably share elements which likely help explain how many trips are undertaken between stations. For example, think of trips to and from a station located in the famous Embarcadero, a popular tourist spot. Every route to and from there probably has more trips due to the popularity of the area and we are currently not acknowledging it in the model.\nA simple way to incorporate these effects into the model is through origin and destination fixed effects. This approach shares elements with both spatial fixed effects and multilevel modeling and essentially consists of including a binary variable for every origin and destination station. In mathematical notation, this equates to:\n\\[\nT_{ij} = X_{ij}\\beta + \\delta_i + \\delta_j + \\epsilon_{ij}\n\\]\nwhere \\(\\delta_i\\) and \\(\\delta_j\\) are origin and destination station fixed effects7, and the rest is as above. This strategy accounts for all the unobserved heterogeneity associated with the location of the station. Technically speaking, we simply need to introduce orig and dest in the the model:\n\nm3 <- glm(\n  'trips15 ~ straight_dist + total_up + total_down + orig + dest', \n  data=db_std,\n  family=poisson\n)\n\nAnd with our new model, we can have a look at how well it does at predicting the overall number of trips8:\n\nplot(\n  density(m3$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n               max(density(m3$fitted.values)$y), \n               max(density(db_std$trips15)$y)\n               )\n            )\n   ),\n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual'),\n  col=c('black', 'red'),\n  lwd=1\n)\ntitle(main=\"Predictive check - Orig/dest FE Poisson model\")\n\n\n\n\nThat looks significantly better, doesn’t it? In fact, our model now better accounts for the long tail where a few routes take a lot of trips. This is likely because the distribution of trips is far from random across stations and our origin and destination fixed effects do a decent job at accounting for that structure. However our model is still notably underpredicting less popular routes and overpredicting routes with above average number of trips. Maybe we should think about moving beyond a simple linear model.\n\nGet better predictors\n\nThe final extension is, in principle, always available but, in practice, it can be tricky to implement. The core idea is that your baseline model might not have the best measurement of the phenomena you want to account for. In our example, we can think of the distance between stations. So far, we have been including the distance measured “as the crow flies” between stations. Although in some cases this is a good approximation (particularly when distances are long and likely route taken is as close to straight as possible), in some cases like ours, where the street layout and the presence of elevation probably matter more than the actual final distance pedalled, this is not necessarily a safe assumption.\nAs an exampe of this approach, we can replace the straight distance measurements for more refined ones based on the Google Maps API routes. This is very easy as all we need to do (once the distances have been calculated!) is to swap straight_dist for street_dist:\n\nm4 <- glm(\n  'trips15 ~ street_dist + total_up + total_down + orig + dest', \n  data=db_std,\n  family=poisson\n)\n\nAnd we can similarly get a sense of our predictive fitting with:\n\nplot(\n  density(m4$fitted.values), \n  xlim=c(-100, max(db_std$trips15)),\n  ylim=c(0, max(c(\n               max(density(m4$fitted.values)$y), \n               max(density(db_std$trips15)$y)\n               )\n            )\n   ),\n  col='black',\n  main=''\n)\nlines(\n  density(db_std$trips15), \n  col='red',\n  main=''\n)\nlegend(\n  'topright', \n  c('Predicted', 'Actual'),\n  col=c('black', 'red'),\n  lwd=1\n)\ntitle(main=\"Predictive check - Orig/dest FE Poisson model\")\n\n\n\n\nHard to tell any noticeable difference, right? To see if there is any, we can have a look at the estimates obtained:\n\nsummary(m4)$coefficients['street_dist', ]\n\n      Estimate     Std. Error        z value       Pr(>|z|) \n -9.961619e-02   2.688731e-03  -3.704952e+01  1.828096e-300 \n\n\nAnd compare this to that of the straight distances in the previous model:\n\nsummary(m3)$coefficients['straight_dist', ]\n\n      Estimate     Std. Error        z value       Pr(>|z|) \n -7.820014e-02   2.683052e-03  -2.914596e+01  9.399407e-187 \n\n\nAs we can see, the differences exist but they are not massive. Let’s use this example to learn how to interpret coefficients in a Poisson model9. Effectively, these estimates can be understood as multiplicative effects. Since our model fits\n\\[\nT_{ij} \\sim Poisson (\\exp^{X_{ij}\\beta})\n\\]\nwe need to transform \\(\\beta\\) through an exponential in order to get a sense of the effect of distance on the number of trips. This means that for the street distance, our original estimate is \\(\\beta_{street} = -0.0996\\), but this needs to be translated through the exponential into \\(e^{-0.0996} = 0.906\\). In other words, since distance is expressed in standard deviations10, we can expect a 10% decrease in the number of trips for an increase of one standard deviation (about 1Km) in the distance between the stations. This can be compared with \\(e^{-0.0782} = 0.925\\) for the straight distances, or a reduction of about 8% the number of trips for every increase of a standard deviation (about 720m)."
  },
  {
    "objectID": "05-flows.html#predicting-flows",
    "href": "05-flows.html#predicting-flows",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.5 Predicting flows",
    "text": "5.5 Predicting flows\nSo far we have put all of our modeling efforts in understanding the model we fit and improving such model so it fits our data as closely as possible. This is essential in any modelling exercise but should be far from a stopping point. Once we’re confident our model is a decent representation of the data generating process, we can start exploiting it. In this section, we will cover one specific case that showcases how a fitted model can help: out-of-sample forecasts.\nIt is August 2015, and you have just started working as a data scientist for the bikeshare company that runs the San Francisco system. You join them as they’re planning for the next academic year and, in order to plan their operations (re-allocating vans, station maintenance, etc.), they need to get a sense of how many people are going to be pedalling across the city and, crucially, where they are going to be pedalling through. What can you do to help them?\nThe easiest approach is to say “well, a good guess for how many people will be going between two given stations this coming year is how many went through last year, isn’t it?”. This is one prediction approach. However, you could see how, even if the same process governs over both datasets (2015 and 2016), each year will probably have some idiosyncracies and thus looking too closely into one year might not give the best possible answer for the next one. Ideally, you want a good stylized synthesis that captures the bits that stay constant over time and thus can be applied in the future and that ignores those aspects that are too particular to a given point in time. That is the rationale behind using a fitted model to obtain predictions.\nHowever good any theory though, the truth is in the pudding. So, to see if a modeling approach is better at producing forecasts than just using the counts from last year, we can put them to a test. The way this is done when evaluating the predictive performance of a model (as this is called in the literature) relies on two basic steps: a) obtain predictions from a given model and b) compare those to the actual values (in our case, with the counts for 2016 in trips16) and get a sense of “how off” they are. We have essentially covered a) above; for b), there are several measures to use. We will use one of the most common ones, the root mean squared error (RMSE), which roughly gives a sense of the average difference between a predicted vector and the real deal:\n\\[\nRMSE = \\sqrt{ \\sum_{ij} (\\hat{T_{ij}} - T_{ij})^2}\n\\]\nwhere \\(\\hat{T_{ij}}\\) is the predicted amount of trips between stations \\(i\\) and \\(j\\). RMSE is straightforward in R and, since we will use it a couple of times, let’s write a short function to make our lives easier:\n\nrmse <- function(t, p){\n  se <- (t - p)^2\n  mse <- mean(se)\n  rmse <- sqrt(mse)\n  return(rmse)\n}\n\nwhere t stands for the vector of true values, and p is the vector of predictions. Let’s give it a spin to make sure it works:\n\nrmse_m4 <- rmse(db_std$trips16, m4$fitted.values)\nrmse_m4\n\n[1] 256.2197\n\n\nThat means that, on average, predictions in our best model m4 are 256 trips off. Is this good? Bad? Worse? It’s hard to say but, being practical, what we can say is whether this better than our alternative. Let us have a look at the RMSE of the other models as well as that of simply plugging in last year’s counts:11\n\nrmses <- data.frame(\n  model=c(\n    'OLS', \n    'Poisson', \n    'Poisson + FE', \n    'Poisson + FE + street dist.',\n    'Trips-2015'\n  ),\n  RMSE=c(\n  rmse(db_std$trips16, m1$fitted.values),\n  rmse(db_std$trips16, m2$fitted.values),\n  rmse(db_std$trips16, m3$fitted.values),\n  rmse(db_std$trips16, m4$fitted.values),\n  rmse(db_std$trips16, db_std$trips15)\n  )\n)\nrmses\n\n                        model     RMSE\n1                         OLS 323.6135\n2                     Poisson 320.8962\n3                Poisson + FE 254.4468\n4 Poisson + FE + street dist. 256.2197\n5                  Trips-2015 131.0228\n\n\nThe table is both encouraging and disheartning at the same time. On the one hand, all the modeling techniques covered above behave as we would expect: the baseline model displays the worst predicting power of all, and every improvement (except the street distances!) results in notable decreases of the RMSE. This is good news. However, on the other hand, all of our modelling efforts fall short of given a better guess than simply using the previous year’s counts. Why? Does this mean that we should not pay attention to modeling and inference? Not really. Generally speaking, a model is as good at predicting as it is able to mimic the underlying process that gave rise to the data in the first place. The results above point to a case where our model is not picking up all the factors that determine the amount of trips undertaken in a give route. This could be improved by enriching the model with more/better predictors, as we have seen above. Also, the example above seems to point to a case where those idiosyncracies in 2015 that the model does not pick up seem to be at work in 2016 as well. This is great news for our prediction efforts this time, but we have no idea why this is the case and, for all that matters, it could change the coming year. Besides the elegant quantification of uncertainty, the true advantage of a modeling approach in this context is that, if well fit, it is able to pick up the fundamentals that apply over and over. This means that, if next year we’re not as lucky as this one and previous counts are not good predictors but the variables we used in our model continue to have a role in determining the outcome, the data scientist should be luckier and hit a better prediction."
  },
  {
    "objectID": "05-flows.html#questions",
    "href": "05-flows.html#questions",
    "title": "5  Spatial Interaction Modelling",
    "section": "5.6 Questions",
    "text": "5.6 Questions\nWe will be using again the Madrid AirBnb dataset: \n\nmad_abb <- st_read('./data/assignment_1_madrid/madrid_abb.gpkg')\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\nThe columns to use here are:\n\nprice_usd: price expressed in USD\nlog1p_price_usd: logarithm of the price expressed in USD\naccommodates: number of people the property accommodates\nbathrooms: number of bathrooms the property includes\nbedrooms: number of bedrooms the property includes\nbeds: number of beds the property includes\n\nWith these data at hand, accomplish the following challenges:\n\nSet up a baseline regression model where you explain the price of a property as a function of its characteristics:\n\n\\[\nP_i = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\n\nFit a parallel model that uses the log of price as dependent variable:\n\n\\[\n\\log(P_i) = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\n\nPerform a predictive check analysis of both models, discussing how they compare, which one you would prefer, and why\n\n\n\n\n\nFotheringham, A Stewart, and Morton E O’Kelly. 1989. Spatial Interaction Models: Formulations and Applications. Vol. 1. Kluwer Academic Publishers Dordrecht.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nRowe, Francisco, Robin Lovelace, and Adam Dennett. 2022. “Spatial Interaction Modelling: A Manifesto.” http://dx.doi.org/10.31219/osf.io/xcdms.\n\n\nSingleton, Alex. 2017. “Geographic Data Science for Urban Analytics.” http://www.alex-singleton.com/GDS_UA_2017/."
  },
  {
    "objectID": "06-spatial-econometrics.html#dependencies",
    "href": "06-spatial-econometrics.html#dependencies",
    "title": "6  Spatial Econometrics",
    "section": "6.1 Dependencies",
    "text": "6.1 Dependencies\nWe will rely on the following libraries in this section, all of them included in Section 1.4.1:\n\n# Layout\nlibrary(tufte)\n# For pretty table\nlibrary(knitr)\n# For string parsing\nlibrary(stringr)\n# Spatial Data management\nlibrary(rgdal)\n# Pretty graphics\nlibrary(ggplot2)\n# Pretty maps\nlibrary(ggmap)\n# For all your interpolation needs\nlibrary(gstat)\n# For data manipulation\nlibrary(dplyr)\n# Spatial regression\nlibrary(spdep)\n\nBefore we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file and where the house_transactions folder with the data lives.\n\nsetwd('.')"
  },
  {
    "objectID": "06-spatial-econometrics.html#data",
    "href": "06-spatial-econometrics.html#data",
    "title": "6  Spatial Econometrics",
    "section": "6.2 Data",
    "text": "6.2 Data\nTo explore ideas in spatial regression, we will the set of Airbnb properties for San Diego (US), borrowed from the “Geographic Data Science with Python” book (see here for more info on the dataset source). This covers the point location of properties advertised on the Airbnb website in the San Diego region.\nLet us load the data:\n\ndb <- st_read('data/abb_sd/regression_db.geojson')\n\nReading layer `regression_db' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/abb_sd/regression_db.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6110 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2812 ymin: 32.57349 xmax: -116.9553 ymax: 33.08311\nGeodetic CRS:  WGS 84\n\n\nThe table contains the followig variables:\n\nnames(db)\n\n [1] \"accommodates\"       \"bathrooms\"          \"bedrooms\"          \n [4] \"beds\"               \"neighborhood\"       \"pool\"              \n [7] \"d2balboa\"           \"coastal\"            \"price\"             \n[10] \"log_price\"          \"id\"                 \"pg_Apartment\"      \n[13] \"pg_Condominium\"     \"pg_House\"           \"pg_Other\"          \n[16] \"pg_Townhouse\"       \"rt_Entire_home.apt\" \"rt_Private_room\"   \n[19] \"rt_Shared_room\"     \"geometry\"          \n\n\nFor most of this chapter, we will be exploring determinants and strategies for modelling the price of a property advertised in AirBnb. To get a first taste of what this means, we can create a plot of prices within the area of San Diego:\n\ndb %>%\n  ggplot(aes(color = price)) +\n  geom_sf() + \n  scale_color_viridis_c() +\n  theme_void()"
  },
  {
    "objectID": "06-spatial-econometrics.html#non-spatial-regression-a-refresh",
    "href": "06-spatial-econometrics.html#non-spatial-regression-a-refresh",
    "title": "6  Spatial Econometrics",
    "section": "6.3 Non-spatial regression, a refresh",
    "text": "6.3 Non-spatial regression, a refresh\nBefore we discuss how to explicitly include space into the linear regression framework, let us show how basic regression can be carried out in R, and how you can interpret the results. By no means is this a formal and complete introduction to regression so, if that is what you are looking for, the first part of Gelman and Hill (2006), in particular chapters 3 and 4, are excellent places to check out.\nThe core idea of linear regression is to explain the variation in a given (dependent) variable as a linear function of a series of other (explanatory) variables. For example, in our case, we may want to express/explain the price of a property advertised on AirBnb as a function of some of its characteristics, such as the number of people it accommodates, and how many bathrooms, bedrooms and beds it features. At the individual level, we can express this as:\n\\[\n\\log(P_i) = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\nwhere \\(P_i\\) is the price of house \\(i\\), \\(Acc_i\\), \\(Bath_i\\), \\(Bedr_i\\) and \\(Beds_i\\) are the count of people it accommodates, bathrooms, bedrooms and beds that house \\(i\\) has, respectively. The parameters \\(\\beta_{1,2, 3, 4}\\) give us information about in which way and to what extent each variable is related to the price, and \\(\\alpha\\), the constant term, is the average house price when all the other variables are zero. The term \\(\\epsilon_i\\) is usually referred to as the “error” and captures elements that influence the price of a house but are not accounted for explicitly. We can also express this relation in matrix form, excluding subindices for \\(i\\) as:\n\\[\n\\log(P) = \\alpha + \\beta_1 Acc + \\beta_2 Bath + \\beta_3 Bedr + \\beta_4 Beds + \\epsilon\n\\] where each term can be interpreted in terms of vectors instead of scalars (wit the exception of the parameters \\((\\alpha, \\beta_{1, 2, 3, 4})\\), which are scalars). Note we are using the logarithm of the price, since this allows us to interpret the coefficients as roughly the percentage change induced by a unit increase in the explanatory variable of the estimate.\nRemember a regression can be seen as a multivariate extension of bivariate correlations. Indeed, one way to interpret the \\(\\beta_k\\) coefficients in the equation above is as the degree of correlation between the explanatory variable \\(k\\) and the dependent variable, keeping all the other explanatory variables constant. When you calculate simple bivariate correlations, the coefficient of a variable is picking up the correlation between the variables, but it is also subsuming into it variation associated with other correlated variables –also called confounding factors1. Regression allows you to isolate the distinct effect that a single variable has on the dependent one, once we control for those other variables.\nPractically speaking, running linear regressions in R is straightforward. For example, to fit the model specified in the equation above, we only need one line of code:\n\nm1 <- lm('log_price ~ accommodates + bathrooms + bedrooms + beds', db)\n\nWe use the command lm, for linear model, and specify the equation we want to fit using a string that relates the dependent variable (the log of the price, log_price) with a set of explanatory ones (accommodates, bathrooms, bedrooms, beds) by using a tilde ~ that is akin to the \\(=\\) symbol in the mathematical equation above. Since we are using names of variables that are stored in a table, we need to pass the table object (db) as well.\nIn order to inspect the results of the model, the quickest way is to call summary:\n\nsummary(m1)\n\n\nCall:\nlm(formula = \"log_price ~ accommodates + bathrooms + bedrooms + beds\", \n    data = db)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8486 -0.3234 -0.0095  0.3023  3.3975 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.018133   0.013947  288.10   <2e-16 ***\naccommodates  0.176851   0.005323   33.23   <2e-16 ***\nbathrooms     0.150981   0.012526   12.05   <2e-16 ***\nbedrooms      0.111700   0.012537    8.91   <2e-16 ***\nbeds         -0.076974   0.007927   -9.71   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5366 on 6105 degrees of freedom\nMultiple R-squared:  0.5583,    Adjusted R-squared:  0.558 \nF-statistic:  1929 on 4 and 6105 DF,  p-value: < 2.2e-16\n\n\nA full detailed explanation of the output is beyond the scope of the chapter, but we will highlight the relevant bits for our main purpose. This is concentrated on the Coefficients section, which gives us the estimates for the \\(\\beta_k\\) coefficients in our model. These estimates are the raw equivalent of the correlation coefficient between each explanatory variable and the dependent one, once the “polluting” effect of the other variables included in the model has been accounted for2. Results are as expected for the most part: houses tend to be significantly more expensive if they accommodate more people (an extra person increases the price by 17.7%, approximately), have more bathrooms (15.1%), or bedrooms (11.2%). Perhaps counter intuitively, an extra bed available seems to decrease the price by about -7.7%. However, keep in mind that this is the case, everything else equal. Hence, more beds per room and bathroom (ie. a more crowded house) is a bit cheaper."
  },
  {
    "objectID": "06-spatial-econometrics.html#spatial-regression-a-very-first-dip",
    "href": "06-spatial-econometrics.html#spatial-regression-a-very-first-dip",
    "title": "6  Spatial Econometrics",
    "section": "6.4 Spatial regression: a (very) first dip",
    "text": "6.4 Spatial regression: a (very) first dip\nSpatial regression is about explicitly introducing space or geographical context into the statistical framework of a regression. Conceptually, we want to introduce space into our model whenever we think it plays an important role in the process we are interested in, or when space can act as a reasonable proxy for other factors we cannot but should include in our model. As an example of the former, we can imagine how houses at the seafront are probably more expensive than those in the second row, given their better views. To illustrate the latter, we can think of how the character of a neighborhood is important in determining the price of a house; however, it is very hard to identify and quantify “character” per se, although it might be easier to get at its spatial variation, hence a case of space as a proxy.\nSpatial regression is a large field of development in the econometrics and statistics literature. In this brief introduction, we will consider two related but very different processes that give rise to spatial effects: spatial heterogeneity and spatial dependence. For more rigorous treatments of the topics introduced here, the reader is referred to Anselin (2003), Anselin and Rey (2014), and Gibbons, Overman, and Patacchini (2014)."
  },
  {
    "objectID": "06-spatial-econometrics.html#spatial-heterogeneity",
    "href": "06-spatial-econometrics.html#spatial-heterogeneity",
    "title": "6  Spatial Econometrics",
    "section": "6.5 Spatial heterogeneity",
    "text": "6.5 Spatial heterogeneity\nSpatial heterogeneity (SH) arises when we cannot safely assume the process we are studying operates under the same “rules” throughout the geography of interest. In other words, we can observe SH when there are effects on the outcome variable that are intrinsically linked to specific locations. A good example of this is the case of seafront houses above: we are trying to model the price of a house and, the fact some houses are located under certain conditions (i.e. by the sea), makes their price behave differently. This somewhat abstract concept of SH can be made operational in a model in several ways. We will explore the following two: spatial fixed-effects (FE); and spatial regimes, which is a generalization of FE.\nSpatial FE\nLet us consider the house price example from the previous section to introduce a more general illustration that relates to the second motivation for spatial effects (“space as a proxy”). Given we are only including two explanatory variables in the model, it is likely we are missing some important factors that play a role at determining the price at which a house is sold. Some of them, however, are likely to vary systematically over space (e.g. different neighborhood characteristics). If that is the case, we can control for those unobserved factors by using traditional dummy variables but basing their creation on a spatial rule. For example, let us include a binary variable for every neighbourhood, as provided by AirBnB, indicating whether a given house is located within such area (1) or not (0). Neighbourhood membership is expressed on the neighborhood column:\n\ndb %>%\n  ggplot(aes(color = neighborhood)) +\n  geom_sf() + \n  theme_void()\n\n\n\n\nMathematically, we are now fitting the following equation:\n\\[\n\\log(P_i) = \\alpha_r + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\nwhere the main difference is that we are now allowing the constant term, \\(\\alpha\\), to vary by neighbourhood \\(r\\), \\(\\alpha_r\\).\nProgrammatically, we can fit this model with lm:\n\n# Include `-1` to eliminate the constant term and include a dummy for every area\nm2 <- lm(\n  'log_price ~ neighborhood + accommodates + bathrooms + bedrooms + beds - 1', \n  db\n)\nsummary(m2)\n\n\nCall:\nlm(formula = \"log_price ~ neighborhood + accommodates + bathrooms + bedrooms + beds - 1\", \n    data = db)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4549 -0.2920 -0.0203  0.2741  3.5323 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \nneighborhoodBalboa Park              3.994775   0.036539  109.33   <2e-16 ***\nneighborhoodBay Ho                   3.780025   0.086081   43.91   <2e-16 ***\nneighborhoodBay Park                 3.941847   0.055788   70.66   <2e-16 ***\nneighborhoodCarmel Valley            4.034052   0.062811   64.23   <2e-16 ***\nneighborhoodCity Heights West        3.698788   0.065502   56.47   <2e-16 ***\nneighborhoodClairemont Mesa          3.658339   0.051438   71.12   <2e-16 ***\nneighborhoodCollege Area             3.649859   0.064979   56.17   <2e-16 ***\nneighborhoodCore                     4.433447   0.058864   75.32   <2e-16 ***\nneighborhoodCortez Hill              4.294790   0.057648   74.50   <2e-16 ***\nneighborhoodDel Mar Heights          4.300659   0.060912   70.61   <2e-16 ***\nneighborhoodEast Village             4.241146   0.032019  132.46   <2e-16 ***\nneighborhoodGaslamp Quarter          4.473863   0.052493   85.23   <2e-16 ***\nneighborhoodGrant Hill               4.001481   0.058825   68.02   <2e-16 ***\nneighborhoodGrantville               3.664989   0.080168   45.72   <2e-16 ***\nneighborhoodKensington               4.073520   0.087322   46.65   <2e-16 ***\nneighborhoodLa Jolla                 4.400145   0.026772  164.36   <2e-16 ***\nneighborhoodLa Jolla Village         4.066151   0.087263   46.60   <2e-16 ***\nneighborhoodLinda Vista              3.817940   0.063128   60.48   <2e-16 ***\nneighborhoodLittle Italy             4.390651   0.052433   83.74   <2e-16 ***\nneighborhoodLoma Portal              4.034473   0.036173  111.53   <2e-16 ***\nneighborhoodMarina                   4.046133   0.052178   77.55   <2e-16 ***\nneighborhoodMidtown                  4.032038   0.030280  133.16   <2e-16 ***\nneighborhoodMidtown District         4.356943   0.071756   60.72   <2e-16 ***\nneighborhoodMira Mesa                3.570523   0.061543   58.02   <2e-16 ***\nneighborhoodMission Bay              4.251309   0.023318  182.32   <2e-16 ***\nneighborhoodMission Valley           4.012410   0.083766   47.90   <2e-16 ***\nneighborhoodMoreno Mission           4.028288   0.063342   63.59   <2e-16 ***\nneighborhoodNormal Heights           3.791895   0.054730   69.28   <2e-16 ***\nneighborhoodNorth Clairemont         3.498107   0.076432   45.77   <2e-16 ***\nneighborhoodNorth Hills              3.959403   0.026823  147.61   <2e-16 ***\nneighborhoodNorthwest                3.810201   0.078158   48.75   <2e-16 ***\nneighborhoodOcean Beach              4.152695   0.032352  128.36   <2e-16 ***\nneighborhoodOld Town                 4.127737   0.046523   88.72   <2e-16 ***\nneighborhoodOtay Ranch               3.722902   0.091633   40.63   <2e-16 ***\nneighborhoodPacific Beach            4.116749   0.022711  181.27   <2e-16 ***\nneighborhoodPark West                4.216829   0.050370   83.72   <2e-16 ***\nneighborhoodRancho Bernadino         3.873962   0.080780   47.96   <2e-16 ***\nneighborhoodRancho Penasquitos       3.772037   0.068808   54.82   <2e-16 ***\nneighborhoodRoseville                4.070468   0.065299   62.34   <2e-16 ***\nneighborhoodSan Carlos               3.935042   0.093205   42.22   <2e-16 ***\nneighborhoodScripps Ranch            3.641239   0.085190   42.74   <2e-16 ***\nneighborhoodSerra Mesa               3.912127   0.066630   58.71   <2e-16 ***\nneighborhoodSouth Park               3.987019   0.060141   66.30   <2e-16 ***\nneighborhoodUniversity City          3.772504   0.039638   95.17   <2e-16 ***\nneighborhoodWest University Heights  4.043161   0.048238   83.82   <2e-16 ***\naccommodates                         0.150283   0.005086   29.55   <2e-16 ***\nbathrooms                            0.132287   0.011886   11.13   <2e-16 ***\nbedrooms                             0.147631   0.011960   12.34   <2e-16 ***\nbeds                                -0.074622   0.007405  -10.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4971 on 6061 degrees of freedom\nMultiple R-squared:  0.9904,    Adjusted R-squared:  0.9904 \nF-statistic: 1.28e+04 on 49 and 6061 DF,  p-value: < 2.2e-16\n\n\nEconometrically speaking, what the postcode FE we have introduced imply is that, instead of comparing all house prices across San Diego as equal, we only derive variation from within each postcode. In our particular case, estimating spatial FE in our particular example also gives you an indirect measure of area desirability: since they are simple dummies in a regression explaining the price of a house, their estimate tells us about how much people are willing to pay to live in a given area. We can visualise this “geography of desirability” by plotting the estimates of each fixed effect on a map:\n\n# Extract neighborhood names from coefficients\nnei.names <- m2$coefficients %>%\n  as.data.frame() %>%\n  row.names() %>%\n  str_replace(\"neighborhood\", \"\")\n# Set up as Data Frame\nnei.fes <- data.frame(\n  coef = m2$coefficients,\n  nei = nei.names,\n  row.names = nei.names\n) %>%\n  right_join(\n    db, by = c(\"nei\" = \"neighborhood\")\n)\n# Plot\nnei.fes %>%\n  st_as_sf() %>%\n  ggplot(aes(color = coef)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_void()\n\n\n\n\nWe can see how neighborhoods in the left (west) tend to have higher prices. What we can’t see, but it is represented there if you are familiar with the geography of San Diego, is that the city is bounded by the Pacific ocean on the left, suggesting neighbourhoods by the beach tend to be more expensive.\nRemember that the interpretation of a \\(\\beta_k\\) coefficient is the effect of variable \\(k\\), given all the other explanatory variables included remain constant. By including a single variable for each area, we are effectively forcing the model to compare as equal only house prices that share the same value for each variable; in other words, only houses located within the same area. Introducing FE affords you a higher degree of isolation of the effects of the variables you introduce in your model because you can control for unobserved effects that align spatially with the distribution of the FE you introduce (by neighbourhood, in our case).\nSpatial regimes\nAt the core of estimating spatial FEs is the idea that, instead of assuming the dependent variable behaves uniformly over space, there are systematic effects following a geographical pattern that affect its behaviour. In other words, spatial FEs introduce econometrically the notion of spatial heterogeneity. They do this in the simplest possible form: by allowing the constant term to vary geographically. The other elements of the regression are left untouched and hence apply uniformly across space. The idea of spatial regimes (SRs) is to generalize the spatial FE approach to allow not only the constant term to vary but also any other explanatory variable. This implies that the equation we will be estimating is: \\[\n\\log(P_i) = \\alpha_r + \\beta_{1r} Acc_i + \\beta_{2r} Bath_i + \\beta_{3r} Bedr_i + \\beta_{4r} Beds_i + \\epsilon_i\n\\]\nwhere we are not only allowing the constant term to vary by region (\\(\\alpha_r\\)), but also every other parameter (\\(\\beta_{kr}\\)).\nAlso, given we are going to allow every coefficient to vary by regime, we will need to explicitly set a constant term that we can allow to vary:\n\ndb$one <- 1\n\nThen, the estimation leverages the capabilities in model description of R formulas:\n\n# `:` notation implies interaction variables\nm3 <- lm(\n  'log_price ~ 0 + (accommodates + bathrooms + bedrooms + beds):(neighborhood)', \n  db\n)\nsummary(m3)\n\n\nCall:\nlm(formula = \"log_price ~ 0 + (accommodates + bathrooms + bedrooms + beds):(neighborhood)\", \n    data = db)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4790  -0.0096   1.0931   1.7599   6.1073 \n\nCoefficients:\n                                                  Estimate Std. Error t value\naccommodates:neighborhoodBalboa Park              0.063528   0.093237   0.681\naccommodates:neighborhoodBay Ho                  -0.259615   0.335007  -0.775\naccommodates:neighborhoodBay Park                -0.355401   0.232720  -1.527\naccommodates:neighborhoodCarmel Valley            0.129786   0.187193   0.693\naccommodates:neighborhoodCity Heights West        0.447371   0.231998   1.928\naccommodates:neighborhoodClairemont Mesa          0.711353   0.177821   4.000\naccommodates:neighborhoodCollege Area            -0.346152   0.188071  -1.841\naccommodates:neighborhoodCore                     0.125864   0.148417   0.848\naccommodates:neighborhoodCortez Hill              0.715958   0.126562   5.657\naccommodates:neighborhoodDel Mar Heights          0.829195   0.214067   3.874\naccommodates:neighborhoodEast Village             0.214642   0.077394   2.773\naccommodates:neighborhoodGaslamp Quarter          0.451443   0.197637   2.284\naccommodates:neighborhoodGrant Hill               1.135176   0.167771   6.766\naccommodates:neighborhoodGrantville               0.300907   0.280369   1.073\naccommodates:neighborhoodKensington               0.668742   0.450243   1.485\naccommodates:neighborhoodLa Jolla                 0.520882   0.055887   9.320\naccommodates:neighborhoodLa Jolla Village         0.566452   0.413185   1.371\naccommodates:neighborhoodLinda Vista              0.523975   0.282219   1.857\naccommodates:neighborhoodLittle Italy             0.603908   0.121899   4.954\naccommodates:neighborhoodLoma Portal              0.487743   0.127870   3.814\naccommodates:neighborhoodMarina                   0.431384   0.172628   2.499\naccommodates:neighborhoodMidtown                  0.618058   0.087992   7.024\naccommodates:neighborhoodMidtown District         0.430398   0.191682   2.245\naccommodates:neighborhoodMira Mesa               -0.018199   0.310167  -0.059\naccommodates:neighborhoodMission Bay              0.440951   0.049454   8.916\naccommodates:neighborhoodMission Valley           0.144530   0.507925   0.285\naccommodates:neighborhoodMoreno Mission           0.100471   0.229460   0.438\naccommodates:neighborhoodNormal Heights           0.413682   0.198584   2.083\naccommodates:neighborhoodNorth Clairemont        -0.242723   0.307090  -0.790\naccommodates:neighborhoodNorth Hills              0.262840   0.083258   3.157\naccommodates:neighborhoodNorthwest               -0.229157   0.255656  -0.896\naccommodates:neighborhoodOcean Beach              0.754771   0.079097   9.542\naccommodates:neighborhoodOld Town                 0.177176   0.159714   1.109\naccommodates:neighborhoodOtay Ranch              -0.333536   0.309545  -1.078\naccommodates:neighborhoodPacific Beach            0.345475   0.057599   5.998\naccommodates:neighborhoodPark West                0.909020   0.156013   5.827\naccommodates:neighborhoodRancho Bernadino        -0.118939   0.256750  -0.463\naccommodates:neighborhoodRancho Penasquitos       0.121845   0.228456   0.533\naccommodates:neighborhoodRoseville                0.316929   0.226110   1.402\naccommodates:neighborhoodSan Carlos               0.191248   0.318706   0.600\naccommodates:neighborhoodScripps Ranch            0.347638   0.127239   2.732\naccommodates:neighborhoodSerra Mesa               0.495491   0.282281   1.755\naccommodates:neighborhoodSouth Park               0.334378   0.256708   1.303\naccommodates:neighborhoodUniversity City          0.107605   0.113883   0.945\naccommodates:neighborhoodWest University Heights  0.190215   0.212040   0.897\nbathrooms:neighborhoodBalboa Park                 2.275321   0.225032  10.111\nbathrooms:neighborhoodBay Ho                      3.312231   0.530568   6.243\nbathrooms:neighborhoodBay Park                    2.231649   0.365655   6.103\nbathrooms:neighborhoodCarmel Valley               1.191058   0.224138   5.314\nbathrooms:neighborhoodCity Heights West           2.517235   0.550272   4.575\nbathrooms:neighborhoodClairemont Mesa             3.737297   0.427366   8.745\nbathrooms:neighborhoodCollege Area                3.370263   0.413479   8.151\nbathrooms:neighborhoodCore                        3.635188   0.490640   7.409\nbathrooms:neighborhoodCortez Hill                 1.631032   0.299654   5.443\nbathrooms:neighborhoodDel Mar Heights             1.346206   0.342828   3.927\nbathrooms:neighborhoodEast Village                2.600489   0.190932  13.620\nbathrooms:neighborhoodGaslamp Quarter             3.183092   0.527615   6.033\nbathrooms:neighborhoodGrant Hill                  2.770976   0.416838   6.648\nbathrooms:neighborhoodGrantville                  2.177175   0.693599   3.139\nbathrooms:neighborhoodKensington                  1.284044   0.671482   1.912\nbathrooms:neighborhoodLa Jolla                    0.852667   0.099413   8.577\nbathrooms:neighborhoodLa Jolla Village            0.984426   1.193870   0.825\nbathrooms:neighborhoodLinda Vista                 2.359895   0.393392   5.999\nbathrooms:neighborhoodLittle Italy                2.600567   0.275834   9.428\nbathrooms:neighborhoodLoma Portal                 2.575164   0.249679  10.314\nbathrooms:neighborhoodMarina                      3.317139   0.656533   5.053\nbathrooms:neighborhoodMidtown                     0.899736   0.112205   8.019\nbathrooms:neighborhoodMidtown District            3.143440   0.594875   5.284\nbathrooms:neighborhoodMira Mesa                   2.858280   0.512511   5.577\nbathrooms:neighborhoodMission Bay                 1.764929   0.122421  14.417\nbathrooms:neighborhoodMission Valley              2.666000   1.365483   1.952\nbathrooms:neighborhoodMoreno Mission              3.234512   0.557898   5.798\nbathrooms:neighborhoodNormal Heights              3.505139   0.467965   7.490\nbathrooms:neighborhoodNorth Clairemont            2.574847   0.613471   4.197\nbathrooms:neighborhoodNorth Hills                 2.584724   0.191541  13.494\nbathrooms:neighborhoodNorthwest                   2.877519   0.569924   5.049\nbathrooms:neighborhoodOcean Beach                 1.702208   0.207508   8.203\nbathrooms:neighborhoodOld Town                    2.249120   0.302755   7.429\nbathrooms:neighborhoodOtay Ranch                  2.818736   1.132794   2.488\nbathrooms:neighborhoodPacific Beach               2.272803   0.130607  17.402\nbathrooms:neighborhoodPark West                   2.676739   0.308257   8.683\nbathrooms:neighborhoodRancho Bernadino            0.856723   0.555198   1.543\nbathrooms:neighborhoodRancho Penasquitos          0.677767   0.414569   1.635\nbathrooms:neighborhoodRoseville                   1.109625   0.360103   3.081\nbathrooms:neighborhoodSan Carlos                  2.489815   0.511232   4.870\nbathrooms:neighborhoodScripps Ranch               2.459862   0.469601   5.238\nbathrooms:neighborhoodSerra Mesa                  2.968934   0.602807   4.925\nbathrooms:neighborhoodSouth Park                  2.895471   0.521793   5.549\nbathrooms:neighborhoodUniversity City             3.125387   0.347825   8.986\nbathrooms:neighborhoodWest University Heights     2.188257   0.390408   5.605\nbedrooms:neighborhoodBalboa Park                  0.605655   0.245384   2.468\nbedrooms:neighborhoodBay Ho                       0.836163   0.631871   1.323\nbedrooms:neighborhoodBay Park                     1.060944   0.430737   2.463\nbedrooms:neighborhoodCarmel Valley                0.521954   0.480497   1.086\nbedrooms:neighborhoodCity Heights West           -0.272600   0.663983  -0.411\nbedrooms:neighborhoodClairemont Mesa             -0.742539   0.450344  -1.649\nbedrooms:neighborhoodCollege Area                -0.306621   0.410476  -0.747\nbedrooms:neighborhoodCore                        -0.786470   0.395991  -1.986\nbedrooms:neighborhoodCortez Hill                  0.793039   0.380195   2.086\nbedrooms:neighborhoodDel Mar Heights             -0.071069   0.369070  -0.193\nbedrooms:neighborhoodEast Village                -0.186076   0.213572  -0.871\nbedrooms:neighborhoodGaslamp Quarter             -0.294024   0.342057  -0.860\nbedrooms:neighborhoodGrant Hill                  -0.456825   0.425374  -1.074\nbedrooms:neighborhoodGrantville                   0.907259   0.770945   1.177\nbedrooms:neighborhoodKensington                  -0.257195   1.009326  -0.255\nbedrooms:neighborhoodLa Jolla                    -0.152098   0.133726  -1.137\nbedrooms:neighborhoodLa Jolla Village             4.291700   1.882046   2.280\nbedrooms:neighborhoodLinda Vista                 -0.485372   0.642684  -0.755\nbedrooms:neighborhoodLittle Italy                 0.057475   0.306357   0.188\nbedrooms:neighborhoodLoma Portal                 -0.406484   0.250607  -1.622\nbedrooms:neighborhoodMarina                      -0.831114   0.511626  -1.624\nbedrooms:neighborhoodMidtown                      0.696852   0.167900   4.150\nbedrooms:neighborhoodMidtown District             0.010614   0.509151   0.021\nbedrooms:neighborhoodMira Mesa                   -0.197692   0.780959  -0.253\nbedrooms:neighborhoodMission Bay                 -0.330540   0.121602  -2.718\nbedrooms:neighborhoodMission Valley               0.514998   1.295767   0.397\nbedrooms:neighborhoodMoreno Mission              -0.584689   0.596044  -0.981\nbedrooms:neighborhoodNormal Heights              -0.127744   0.391691  -0.326\nbedrooms:neighborhoodNorth Clairemont             0.281306   0.695297   0.405\nbedrooms:neighborhoodNorth Hills                  0.380444   0.178477   2.132\nbedrooms:neighborhoodNorthwest                    0.288603   0.607295   0.475\nbedrooms:neighborhoodOcean Beach                 -0.038069   0.207927  -0.183\nbedrooms:neighborhoodOld Town                    -0.319724   0.375203  -0.852\nbedrooms:neighborhoodOtay Ranch                   0.015564   1.332279   0.012\nbedrooms:neighborhoodPacific Beach               -0.037912   0.139026  -0.273\nbedrooms:neighborhoodPark West                   -0.696514   0.413881  -1.683\nbedrooms:neighborhoodRancho Bernadino             1.034776   0.579798   1.785\nbedrooms:neighborhoodRancho Penasquitos           0.674520   0.519260   1.299\nbedrooms:neighborhoodRoseville                    0.881011   0.592962   1.486\nbedrooms:neighborhoodSan Carlos                  -0.394191   0.540343  -0.730\nbedrooms:neighborhoodScripps Ranch                1.107455   0.336101   3.295\nbedrooms:neighborhoodSerra Mesa                   0.253001   0.620774   0.408\nbedrooms:neighborhoodSouth Park                  -0.595844   0.407811  -1.461\nbedrooms:neighborhoodUniversity City              0.203783   0.455767   0.447\nbedrooms:neighborhoodWest University Heights      0.242873   0.359245   0.676\nbeds:neighborhoodBalboa Park                      0.041556   0.173183   0.240\nbeds:neighborhoodBay Ho                          -0.402544   0.495241  -0.813\nbeds:neighborhoodBay Park                         0.283958   0.410776   0.691\nbeds:neighborhoodCarmel Valley                    0.150416   0.288268   0.522\nbeds:neighborhoodCity Heights West               -0.217526   0.497878  -0.437\nbeds:neighborhoodClairemont Mesa                 -1.109581   0.308998  -3.591\nbeds:neighborhoodCollege Area                     0.594892   0.312780   1.902\nbeds:neighborhoodCore                             0.602559   0.277027   2.175\nbeds:neighborhoodCortez Hill                     -0.609996   0.143559  -4.249\nbeds:neighborhoodDel Mar Heights                 -0.708476   0.257299  -2.754\nbeds:neighborhoodEast Village                     0.399909   0.148641   2.690\nbeds:neighborhoodGaslamp Quarter                  0.240245   0.319910   0.751\nbeds:neighborhoodGrant Hill                      -1.315807   0.186724  -7.047\nbeds:neighborhoodGrantville                      -0.382590   0.469011  -0.816\nbeds:neighborhoodKensington                       0.133474   0.664698   0.201\nbeds:neighborhoodLa Jolla                         0.001347   0.085013   0.016\nbeds:neighborhoodLa Jolla Village                -2.878676   1.020652  -2.820\nbeds:neighborhoodLinda Vista                     -0.142372   0.278211  -0.512\nbeds:neighborhoodLittle Italy                    -0.569868   0.099961  -5.701\nbeds:neighborhoodLoma Portal                     -0.255510   0.222956  -1.146\nbeds:neighborhoodMarina                           0.024175   0.429466   0.056\nbeds:neighborhoodMidtown                         -0.346866   0.137915  -2.515\nbeds:neighborhoodMidtown District                -0.464781   0.337775  -1.376\nbeds:neighborhoodMira Mesa                        0.319934   0.426799   0.750\nbeds:neighborhoodMission Bay                     -0.108936   0.067105  -1.623\nbeds:neighborhoodMission Valley                  -0.502441   0.879795  -0.571\nbeds:neighborhoodMoreno Mission                   0.492514   0.439355   1.121\nbeds:neighborhoodNormal Heights                  -0.532907   0.227211  -2.345\nbeds:neighborhoodNorth Clairemont                 0.562363   0.704213   0.799\nbeds:neighborhoodNorth Hills                     -0.279430   0.123678  -2.259\nbeds:neighborhoodNorthwest                        0.742017   0.474903   1.562\nbeds:neighborhoodOcean Beach                     -0.667651   0.137647  -4.850\nbeds:neighborhoodOld Town                         0.459210   0.287008   1.600\nbeds:neighborhoodOtay Ranch                       0.235723   0.983870   0.240\nbeds:neighborhoodPacific Beach                   -0.179242   0.087511  -2.048\nbeds:neighborhoodPark West                       -0.873297   0.225334  -3.876\nbeds:neighborhoodRancho Bernadino                 0.378088   0.348640   1.084\nbeds:neighborhoodRancho Penasquitos               0.147457   0.344820   0.428\nbeds:neighborhoodRoseville                       -0.391529   0.328609  -1.191\nbeds:neighborhoodSan Carlos                       0.115338   0.621666   0.186\nbeds:neighborhoodScripps Ranch                   -1.654484   0.338331  -4.890\nbeds:neighborhoodSerra Mesa                      -1.018812   0.705888  -1.443\nbeds:neighborhoodSouth Park                       0.452815   0.406052   1.115\nbeds:neighborhoodUniversity City                 -0.345822   0.232779  -1.486\nbeds:neighborhoodWest University Heights          0.146128   0.364075   0.401\n                                                 Pr(>|t|)    \naccommodates:neighborhoodBalboa Park             0.495668    \naccommodates:neighborhoodBay Ho                  0.438397    \naccommodates:neighborhoodBay Park                0.126774    \naccommodates:neighborhoodCarmel Valley           0.488131    \naccommodates:neighborhoodCity Heights West       0.053861 .  \naccommodates:neighborhoodClairemont Mesa         6.40e-05 ***\naccommodates:neighborhoodCollege Area            0.065740 .  \naccommodates:neighborhoodCore                    0.396446    \naccommodates:neighborhoodCortez Hill             1.61e-08 ***\naccommodates:neighborhoodDel Mar Heights         0.000108 ***\naccommodates:neighborhoodEast Village            0.005565 ** \naccommodates:neighborhoodGaslamp Quarter         0.022395 *  \naccommodates:neighborhoodGrant Hill              1.45e-11 ***\naccommodates:neighborhoodGrantville              0.283202    \naccommodates:neighborhoodKensington              0.137520    \naccommodates:neighborhoodLa Jolla                 < 2e-16 ***\naccommodates:neighborhoodLa Jolla Village        0.170446    \naccommodates:neighborhoodLinda Vista             0.063414 .  \naccommodates:neighborhoodLittle Italy            7.47e-07 ***\naccommodates:neighborhoodLoma Portal             0.000138 ***\naccommodates:neighborhoodMarina                  0.012484 *  \naccommodates:neighborhoodMidtown                 2.40e-12 ***\naccommodates:neighborhoodMidtown District        0.024780 *  \naccommodates:neighborhoodMira Mesa               0.953213    \naccommodates:neighborhoodMission Bay              < 2e-16 ***\naccommodates:neighborhoodMission Valley          0.776000    \naccommodates:neighborhoodMoreno Mission          0.661505    \naccommodates:neighborhoodNormal Heights          0.037279 *  \naccommodates:neighborhoodNorth Clairemont        0.429327    \naccommodates:neighborhoodNorth Hills             0.001602 ** \naccommodates:neighborhoodNorthwest               0.370103    \naccommodates:neighborhoodOcean Beach              < 2e-16 ***\naccommodates:neighborhoodOld Town                0.267333    \naccommodates:neighborhoodOtay Ranch              0.281298    \naccommodates:neighborhoodPacific Beach           2.12e-09 ***\naccommodates:neighborhoodPark West               5.96e-09 ***\naccommodates:neighborhoodRancho Bernadino        0.643202    \naccommodates:neighborhoodRancho Penasquitos      0.593817    \naccommodates:neighborhoodRoseville               0.161071    \naccommodates:neighborhoodSan Carlos              0.548479    \naccommodates:neighborhoodScripps Ranch           0.006311 ** \naccommodates:neighborhoodSerra Mesa              0.079258 .  \naccommodates:neighborhoodSouth Park              0.192775    \naccommodates:neighborhoodUniversity City         0.344762    \naccommodates:neighborhoodWest University Heights 0.369719    \nbathrooms:neighborhoodBalboa Park                 < 2e-16 ***\nbathrooms:neighborhoodBay Ho                     4.60e-10 ***\nbathrooms:neighborhoodBay Park                   1.11e-09 ***\nbathrooms:neighborhoodCarmel Valley              1.11e-07 ***\nbathrooms:neighborhoodCity Heights West          4.87e-06 ***\nbathrooms:neighborhoodClairemont Mesa             < 2e-16 ***\nbathrooms:neighborhoodCollege Area               4.37e-16 ***\nbathrooms:neighborhoodCore                       1.45e-13 ***\nbathrooms:neighborhoodCortez Hill                5.45e-08 ***\nbathrooms:neighborhoodDel Mar Heights            8.71e-05 ***\nbathrooms:neighborhoodEast Village                < 2e-16 ***\nbathrooms:neighborhoodGaslamp Quarter            1.71e-09 ***\nbathrooms:neighborhoodGrant Hill                 3.25e-11 ***\nbathrooms:neighborhoodGrantville                 0.001704 ** \nbathrooms:neighborhoodKensington                 0.055892 .  \nbathrooms:neighborhoodLa Jolla                    < 2e-16 ***\nbathrooms:neighborhoodLa Jolla Village           0.409651    \nbathrooms:neighborhoodLinda Vista                2.10e-09 ***\nbathrooms:neighborhoodLittle Italy                < 2e-16 ***\nbathrooms:neighborhoodLoma Portal                 < 2e-16 ***\nbathrooms:neighborhoodMarina                     4.49e-07 ***\nbathrooms:neighborhoodMidtown                    1.28e-15 ***\nbathrooms:neighborhoodMidtown District           1.31e-07 ***\nbathrooms:neighborhoodMira Mesa                  2.55e-08 ***\nbathrooms:neighborhoodMission Bay                 < 2e-16 ***\nbathrooms:neighborhoodMission Valley             0.050935 .  \nbathrooms:neighborhoodMoreno Mission             7.07e-09 ***\nbathrooms:neighborhoodNormal Heights             7.88e-14 ***\nbathrooms:neighborhoodNorth Clairemont           2.74e-05 ***\nbathrooms:neighborhoodNorth Hills                 < 2e-16 ***\nbathrooms:neighborhoodNorthwest                  4.58e-07 ***\nbathrooms:neighborhoodOcean Beach                2.85e-16 ***\nbathrooms:neighborhoodOld Town                   1.25e-13 ***\nbathrooms:neighborhoodOtay Ranch                 0.012863 *  \nbathrooms:neighborhoodPacific Beach               < 2e-16 ***\nbathrooms:neighborhoodPark West                   < 2e-16 ***\nbathrooms:neighborhoodRancho Bernadino           0.122861    \nbathrooms:neighborhoodRancho Penasquitos         0.102129    \nbathrooms:neighborhoodRoseville                  0.002070 ** \nbathrooms:neighborhoodSan Carlos                 1.14e-06 ***\nbathrooms:neighborhoodScripps Ranch              1.68e-07 ***\nbathrooms:neighborhoodSerra Mesa                 8.66e-07 ***\nbathrooms:neighborhoodSouth Park                 3.00e-08 ***\nbathrooms:neighborhoodUniversity City             < 2e-16 ***\nbathrooms:neighborhoodWest University Heights    2.18e-08 ***\nbedrooms:neighborhoodBalboa Park                 0.013608 *  \nbedrooms:neighborhoodBay Ho                      0.185782    \nbedrooms:neighborhoodBay Park                    0.013803 *  \nbedrooms:neighborhoodCarmel Valley               0.277400    \nbedrooms:neighborhoodCity Heights West           0.681416    \nbedrooms:neighborhoodClairemont Mesa             0.099236 .  \nbedrooms:neighborhoodCollege Area                0.455100    \nbedrooms:neighborhoodCore                        0.047070 *  \nbedrooms:neighborhoodCortez Hill                 0.037033 *  \nbedrooms:neighborhoodDel Mar Heights             0.847309    \nbedrooms:neighborhoodEast Village                0.383650    \nbedrooms:neighborhoodGaslamp Quarter             0.390058    \nbedrooms:neighborhoodGrant Hill                  0.282895    \nbedrooms:neighborhoodGrantville                  0.239317    \nbedrooms:neighborhoodKensington                  0.798872    \nbedrooms:neighborhoodLa Jolla                    0.255422    \nbedrooms:neighborhoodLa Jolla Village            0.022623 *  \nbedrooms:neighborhoodLinda Vista                 0.450143    \nbedrooms:neighborhoodLittle Italy                0.851191    \nbedrooms:neighborhoodLoma Portal                 0.104857    \nbedrooms:neighborhoodMarina                      0.104332    \nbedrooms:neighborhoodMidtown                     3.37e-05 ***\nbedrooms:neighborhoodMidtown District            0.983369    \nbedrooms:neighborhoodMira Mesa                   0.800169    \nbedrooms:neighborhoodMission Bay                 0.006583 ** \nbedrooms:neighborhoodMission Valley              0.691053    \nbedrooms:neighborhoodMoreno Mission              0.326658    \nbedrooms:neighborhoodNormal Heights              0.744334    \nbedrooms:neighborhoodNorth Clairemont            0.685798    \nbedrooms:neighborhoodNorth Hills                 0.033080 *  \nbedrooms:neighborhoodNorthwest                   0.634643    \nbedrooms:neighborhoodOcean Beach                 0.854736    \nbedrooms:neighborhoodOld Town                    0.394173    \nbedrooms:neighborhoodOtay Ranch                  0.990680    \nbedrooms:neighborhoodPacific Beach               0.785097    \nbedrooms:neighborhoodPark West                   0.092450 .  \nbedrooms:neighborhoodRancho Bernadino            0.074358 .  \nbedrooms:neighborhoodRancho Penasquitos          0.193994    \nbedrooms:neighborhoodRoseville                   0.137390    \nbedrooms:neighborhoodSan Carlos                  0.465713    \nbedrooms:neighborhoodScripps Ranch               0.000990 ***\nbedrooms:neighborhoodSerra Mesa                  0.683614    \nbedrooms:neighborhoodSouth Park                  0.144046    \nbedrooms:neighborhoodUniversity City             0.654804    \nbedrooms:neighborhoodWest University Heights     0.499025    \nbeds:neighborhoodBalboa Park                     0.810374    \nbeds:neighborhoodBay Ho                          0.416352    \nbeds:neighborhoodBay Park                        0.489423    \nbeds:neighborhoodCarmel Valley                   0.601836    \nbeds:neighborhoodCity Heights West               0.662195    \nbeds:neighborhoodClairemont Mesa                 0.000332 ***\nbeds:neighborhoodCollege Area                    0.057226 .  \nbeds:neighborhoodCore                            0.029663 *  \nbeds:neighborhoodCortez Hill                     2.18e-05 ***\nbeds:neighborhoodDel Mar Heights                 0.005914 ** \nbeds:neighborhoodEast Village                    0.007156 ** \nbeds:neighborhoodGaslamp Quarter                 0.452696    \nbeds:neighborhoodGrant Hill                      2.04e-12 ***\nbeds:neighborhoodGrantville                      0.414683    \nbeds:neighborhoodKensington                      0.840859    \nbeds:neighborhoodLa Jolla                        0.987357    \nbeds:neighborhoodLa Jolla Village                0.004812 ** \nbeds:neighborhoodLinda Vista                     0.608851    \nbeds:neighborhoodLittle Italy                    1.25e-08 ***\nbeds:neighborhoodLoma Portal                     0.251837    \nbeds:neighborhoodMarina                          0.955112    \nbeds:neighborhoodMidtown                         0.011927 *  \nbeds:neighborhoodMidtown District                0.168872    \nbeds:neighborhoodMira Mesa                       0.453518    \nbeds:neighborhoodMission Bay                     0.104565    \nbeds:neighborhoodMission Valley                  0.567962    \nbeds:neighborhoodMoreno Mission                  0.262337    \nbeds:neighborhoodNormal Heights                  0.019038 *  \nbeds:neighborhoodNorth Clairemont                0.424572    \nbeds:neighborhoodNorth Hills                     0.023899 *  \nbeds:neighborhoodNorthwest                       0.118233    \nbeds:neighborhoodOcean Beach                     1.26e-06 ***\nbeds:neighborhoodOld Town                        0.109654    \nbeds:neighborhoodOtay Ranch                      0.810658    \nbeds:neighborhoodPacific Beach                   0.040583 *  \nbeds:neighborhoodPark West                       0.000108 ***\nbeds:neighborhoodRancho Bernadino                0.278202    \nbeds:neighborhoodRancho Penasquitos              0.668932    \nbeds:neighborhoodRoseville                       0.233515    \nbeds:neighborhoodSan Carlos                      0.852819    \nbeds:neighborhoodScripps Ranch                   1.03e-06 ***\nbeds:neighborhoodSerra Mesa                      0.148987    \nbeds:neighborhoodSouth Park                      0.264826    \nbeds:neighborhoodUniversity City                 0.137433    \nbeds:neighborhoodWest University Heights         0.688164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.81 on 5930 degrees of freedom\nMultiple R-squared:  0.8759,    Adjusted R-squared:  0.8721 \nF-statistic: 232.4 on 180 and 5930 DF,  p-value: < 2.2e-16\n\n\nThis allows us to get a separate constant term and estimate of the impact of each variable for every neighborhood"
  },
  {
    "objectID": "06-spatial-econometrics.html#spatial-dependence",
    "href": "06-spatial-econometrics.html#spatial-dependence",
    "title": "6  Spatial Econometrics",
    "section": "6.6 Spatial dependence",
    "text": "6.6 Spatial dependence\nAs we have just discussed, SH is about effects of phenomena that are explicitly linked to geography and that hence cause spatial variation and clustering of values. This encompasses many of the kinds of spatial effects we may be interested in when we fit linear regressions. However, in other cases, our interest is on the effect of the spatial configuration of the observations, and the extent to which that has an effect on the outcome we are considering. For example, we might think that the price of a house not only depends on the number of bathrooms it has but, if we take number of bathrooms as a proxy for size and status, also whether it is surrounded by other houses with many bathrooms. This kind of spatial effect is fundamentally different from SH in that is it not related to inherent characteristics of the geography but relates to the characteristics of the observations in our dataset and, specially, to their spatial arrangement. We call this phenomenon by which the values of observations are related to each other through distance spatial dependence (Anselin 1988).\nSpatial Weights\nThere are several ways to introduce spatial dependence in an econometric framework, with varying degrees of econometric sophistication (see Anselin 2003 for a good overview). Common to all of them however is the way space is formally encapsulated: through spatial weights matrices (\\(W\\))3 These are \\(NxN\\) matrices with zero diagonals and every \\(w_{ij}\\) cell with a value that represents the degree of spatial connectivity/interaction between observations \\(i\\) and \\(j\\). If they are not connected at all, \\(w_{ij}=0\\), otherwise \\(w_{ij}>0\\) and we call \\(i\\) and \\(j\\) neighbors. The exact value in the latter case depends on the criterium we use to define neighborhood relations. These matrices also tend to be row-standardized so the sum of each row equals to one.\nA related concept to spatial weight matrices is that of spatial lag. This is an operator that multiplies a given variable \\(y\\) by a spatial weight matrix:\n\\[\ny_{lag} = W y\n\\]\nIf \\(W\\) is row-standardized, \\(y_{lag}\\) is effectively the average value of \\(y\\) in the neighborhood of each observation. The individual notation may help clarify this:\n\\[\ny_{lag-i} = \\displaystyle \\sum_j w_{ij} y_j\n\\]\nwhere \\(y_{lag-i}\\) is the spatial lag of variable \\(y\\) at location \\(i\\), and \\(j\\) sums over the entire dataset. If \\(W\\) is row-standardized, \\(y_{lag-i}\\) becomes an average of \\(y\\) weighted by the spatial criterium defined in \\(W\\).\nGiven that spatial weights matrices are not the focus of this tutorial, we will stick to a very simple case. Since we are dealing with points, we will use \\(K\\)-nn weights, which take the \\(k\\) nearest neighbors of each observation as neighbors and assign a value of one, assigning everyone else a zero. We will use \\(k=50\\) to get a good degree of variation and sensible results.\n\n# Create knn list of each house\nhnn <- db %>%\n  st_coordinates() %>%\n  as.matrix() %>%\n  knearneigh(k = 50)\n# Create nb object\nhnb <- knn2nb(hnn)\n# Create spatial weights matrix (note it row-standardizes by default)\nhknn <- nb2listw(hnb)\n\nWe can inspect the weights created by simply typing the name of the object:\n\nhknn\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 6110 \nNumber of nonzero links: 305500 \nPercentage nonzero weights: 0.8183306 \nAverage number of links: 50 \nNon-symmetric neighbours list\n\nWeights style: W \nWeights constants summary:\n     n       nn   S0       S1       S2\nW 6110 37332100 6110 220.5032 24924.44\n\n\nExogenous spatial effects\nLet us come back to the house price example we have been working with. So far, we have hypothesized that the price of an AirBnb property in San Diego can be explained using information about its own characteristics, and the neighbourhood it belongs to. However, we can hypothesise that the price of a house is also affected by the characteristics of the houses surrounding it. Considering it as a proxy for larger and more luxurious houses, we will use the number of bathrooms of neighboring houses as an additional explanatory variable. This represents the most straightforward way to introduce spatial dependence in a regression, by considering not only a given explanatory variable, but also its spatial lag.\nIn our example case, in addition to including the number of bathrooms of the property, we will include its spatial lag. In other words, we will be saying that it is not only the number of bathrooms in a house but also that of the surrounding properties that helps explain the final price at which a house is advertised for. Mathematically, this implies estimating the following model:\n\\[\n\\log(P_i) = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i+ \\beta_5 Bath_{lag-i} + \\epsilon_i\n\\]\nLet us first compute the spatial lag of bathrooms:\n\ndb$w_bathrooms <- lag.listw(hknn, db$bathrooms)\n\nAnd then we can include it in our previous specification. Note that we apply the log to the lag, not the reverse:\n\nm5 <- lm(\n  'log_price ~ accommodates + bedrooms + beds + bathrooms + w_bathrooms',\n  db\n)\n\nsummary(m5)\n\n\nCall:\nlm(formula = \"log_price ~ accommodates + bedrooms + beds + bathrooms + w_bathrooms\", \n    data = db)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8869 -0.3243 -0.0206  0.2931  3.5132 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.579448   0.032337 110.692   <2e-16 ***\naccommodates  0.173226   0.005233  33.100   <2e-16 ***\nbedrooms      0.103116   0.012327   8.365   <2e-16 ***\nbeds         -0.075071   0.007787  -9.641   <2e-16 ***\nbathrooms     0.117268   0.012507   9.376   <2e-16 ***\nw_bathrooms   0.353021   0.023572  14.976   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5271 on 6104 degrees of freedom\nMultiple R-squared:  0.574, Adjusted R-squared:  0.5736 \nF-statistic:  1645 on 5 and 6104 DF,  p-value: < 2.2e-16\n\n\nAs we can see, the lag is not only significative and positive, but its effect seems to be even larger that that of the property itself. Taken literally, this implies that the average number of bathrooms in AirBnb’s nearby has a larger effect on the final price of a given AirBnb than its own number of bathrooms. There are several ways to interpret this. One is that, if we take the spatial lag of bathrooms, as we said above, to be a proxy for the types of houses surrounding a property, this is probably a better predictor of how wealthy an area is than the number of bathrooms of a single property, which is more variable. If we also assume that the area where an AirBnb is located has a bigger effect on price than the number of bathrooms, we can start seeing an answer to the apparent puzzle.\nA note on more advanced spatial regression\nIntroducing a spatial lag of an explanatory variable, as we have just seen, is the most straightforward way of incorporating the notion of spatial dependence in a linear regression framework. It does not require additional changes, it can be estimated with OLS, and the interpretation is rather similar to interpreting non-spatial variables. The field of spatial econometrics however is a much broader one and has produced over the last decades many techniques to deal with spatial effects and spatial dependence in different ways. Although this might be an over simplification, one can say that most of such efforts for the case of a single cross-section are focused on two main variations: the spatial lag and the spatial error model. Both are similar to the case we have seen in that they are based on the introduction of a spatial lag, but they differ in the component of the model they modify and affect.\nThe spatial lag model introduces a spatial lag of the dependent variable. In the example we have covered, this would translate into:\n\\[\n\\log(P_i) = \\alpha + \\rho \\log(P_i) + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\nAlthough it might not seem very different from the previous equation, this model violates the exogeneity assumption, crucial for OLS to work.\nEqually, the spatial error model includes a spatial lag in the error term of the equation:\n\\[\n\\log(P_i) = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + u_i\n\\]\n\\[\nu_i = u_{lag-i} + \\epsilon_i\n\\]\nAgain, although similar, one can show this specification violates the assumptions about the error term in a classical OLS model.\nBoth the spatial lag and error model violate some of the assumptions on which OLS relies and thus render the technique unusable. Much of the efforts have thus focused on coming up with alternative methodologies that allow unbiased, robust, and efficient estimation of such models. A survey of those is beyond the scope of this note, but the interested reader is referred to Anselin (1988), Anselin (2003), and Anselin and Rey (2014) for further reference."
  },
  {
    "objectID": "06-spatial-econometrics.html#predicting-house-prices",
    "href": "06-spatial-econometrics.html#predicting-house-prices",
    "title": "6  Spatial Econometrics",
    "section": "6.7 Predicting house prices",
    "text": "6.7 Predicting house prices\nSo far, we have seen how to exploit the output of a regression model to evaluate the role different variables play in explaining another one of interest. However, once fit, a model can also be used to obtain predictions of the dependent variable given a new set of values for the explanatory variables. We will finish this session by dipping our toes in predicting with linear models.\nThe core idea is that once you have estimates for the way in which the explanatory variables can be combined to explain the dependent one, you can plug new values on the explanatory side of the model and combine them following the model estimates to obtain predictions. In the example we have worked with, you can imagine this application would be useful to obtain valuations of a house, given we know its characteristics.\nConceptually, predicting in linear regression models involves using the estimates of the parameters to obtain a value for the dependent variable:\n\\[\n\\bar{\\log(P_i)} = \\bar{\\alpha} + \\bar{\\beta_1} Acc_i^* + \\bar{\\beta_2 Bath_i^*} + \\bar{\\beta_3} Bedr_i^* + \\bar{\\beta_4} Beds_i^*\n\\] where \\(\\bar{\\log{P_i}}\\) is our predicted value, and we include the \\(\\bar{}\\) sign to note that it is our estimate obtained from fitting the model. We use the \\(^*\\) sign to note that those can be new values for the explanatory variables, not necessarily those used to fit the model.\nTechnically speaking, prediction in linear models is relatively streamlined in R. Suppose we are given data for a new house which is to be put on the AirBnb platform. We know it accommodates four people, and has two bedrooms, three beds, and one bathroom. We also know that the surrounding properties have, on average, 1.5 bathrooms. Let us record the data first:\n\nnew.house <- data.frame(\n  accommodates = 4, \n  bedrooms = 2,\n  beds = 3,\n  bathrooms = 1,\n  w_bathrooms = 1.5\n)\n\nTo obtain the prediction for its price, we can use the predict method:\n\nnew.price <- predict(m5, new.house)\nnew.price\n\n       1 \n4.900168 \n\n\nNow remember we were using the log of the price as dependent variable. If we want to recover the actual price of the house, we need to take its exponent:\n\nexp(new.price)\n\n       1 \n134.3123 \n\n\nAccording to our model, the house would be worth $134.3123448."
  },
  {
    "objectID": "06-spatial-econometrics.html#questions",
    "href": "06-spatial-econometrics.html#questions",
    "title": "6  Spatial Econometrics",
    "section": "6.8 Questions",
    "text": "6.8 Questions\nWe will be using again the Madrid AirBnb dataset: \n\nmad_abb <- st_read('./data/assignment_1_madrid/madrid_abb.gpkg')\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\n\ncolnames(mad_abb)\n\n [1] \"price\"           \"price_usd\"       \"log1p_price_usd\" \"accommodates\"   \n [5] \"bathrooms_text\"  \"bathrooms\"       \"bedrooms\"        \"beds\"           \n [9] \"neighbourhood\"   \"room_type\"       \"property_type\"   \"WiFi\"           \n[13] \"Coffee\"          \"Gym\"             \"Parking\"         \"km_to_retiro\"   \n[17] \"geom\"           \n\n\n\nIn addition to those we have already seen, the columns to use here are:\n\nneighbourhood: a column with the name of the neighbourhood in which the property is located\n\nWith this at hand, answer the following questions:\n\nFit a baseline model with only property characteristics explaining the log of price\n\n\\[\n\\log(P_i) = \\alpha + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\n\nAugment the model with fixed effects at the neighbourhood level\n\n\\[\n\\log(P_i) = \\alpha_r + \\beta_1 Acc_i + \\beta_2 Bath_i + \\beta_3 Bedr_i + \\beta_4 Beds_i + \\epsilon_i\n\\]\n\n[Optional] Augment the model with spatial regimes at the neighbourhood level:\n\n\\[\n\\log(P_i) = \\alpha_r + \\beta_{r1} Acc_i + \\beta_{r2} Bath_i + \\beta_{r3} Bedr_i + \\beta_{r4} Beds_i + \\epsilon_{ri}\n\\]\n\nFit a model that augments the baseline in 1. with the spatial lag of a variable you consider interesting. Motivate this choice. Note that to complete this, you will need to also generate a spatial weights matrix.\n\nIn each instance, provide a brief interpretation (no more thana few lines for each) that demonstrates your understanding of theunderlying concepts behind your approach. \n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Vol. 4. Springer Science & Business Media.\n\n\n———. 2003. “Spatial Externalities, Spatial Multipliers, and Spatial Econometrics.” International Regional Science Review 26 (2): 153–66.\n\n\n———. 2007. “Spatial Regression Analysis in r–a Workbook.” Center for Spatially Integrated Social Science. http://csiss.org/GISPopSci/workshops/2011/PSU/readings/W15_Anselin2007.pdf.\n\n\nAnselin, Luc, and Sergio J. Rey. 2014. Modern Spatial Econometrics in Practice: A Guide to GeoDa, GeoDaSpace and PySAL. GeoDa Press LLC.\n\n\nArribas-Bel, Dani. 2014. “Spatial Data, Analysis, and Regression-a Mini Course.” REGION 1 (1): R1. http://darribas.org/sdar_mini.\n\n\n———. 2019. “A Course on Geographic Data Science.” The Journal of Open Source Education 2 (14). https://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nGibbons, Stephen, Henry G Overman, and Eleonora Patacchini. 2014. “Spatial Methods.”\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press.\n\n\nRowe, Francisco. 2022. “Introduction to Geographic Data Science.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/VHY2P."
  },
  {
    "objectID": "07-multilevel-01.html#dependencies",
    "href": "07-multilevel-01.html#dependencies",
    "title": "7  Multilevel Modelling - Part 1",
    "section": "7.1 Dependencies",
    "text": "7.1 Dependencies\nThis chapter uses the following libraries which are listed in the [Dependency list] Section of Chapter 1:\n\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis) # nice colour schemes\n# Fitting multilevel models\nlibrary(lme4)\n# Tools for extracting information generated by lme4\nlibrary(merTools)\n# Exportable regression tables\nlibrary(jtools)\nlibrary(stargazer)\nlibrary(sjPlot)"
  },
  {
    "objectID": "07-multilevel-01.html#data",
    "href": "07-multilevel-01.html#data",
    "title": "7  Multilevel Modelling - Part 1",
    "section": "7.2 Data",
    "text": "7.2 Data\nFor this chapter, we will data for Liverpool from England’s 2011 Census. The original source is the Office of National Statistics and the dataset comprises a number of selected variables capturing demographic, health and socio-economic attributes of the local resident population at four geographic levels: Output Area (OA), Lower Super Output Area (LSOA), Middle Super Output Area (MSOA) and Local Authority District (LAD). The variables include population counts and percentages. For a description of the variables, see the readme file in the mlm data folder.1\nLet us read the data:\n\n# clean workspace\nrm(list=ls())\n# read data\noa_shp <- st_read(\"data/mlm/OA.shp\")\n\nWe can now attach and visualise the structure of the data.\n\n# attach data frame\nattach(oa_shp)\n\nThe following object is masked from package:viridis:\n\n    unemp\n\n# sort data by oa\noa_shp <- oa_shp[order(oa_cd),]\nhead(oa_shp)\n\nSimple feature collection with 6 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335056 ymin: 389163 xmax: 336155 ymax: 389642\nProjected CRS: Transverse_Mercator\n      oa_cd   lsoa_cd   msoa_cd    lad_cd      ward_nm  dstrt_nm    cnty_nm\n1 E00032987 E01006515 E02001383 E08000012    Riverside Liverpool Merseyside\n2 E00032988 E01006514 E02001383 E08000012 Princes Park Liverpool Merseyside\n3 E00032989 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside\n4 E00032990 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside\n5 E00032991 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside\n6 E00032992 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside\n  cntry_nm pop     age_60     unemp      lat      long    males   lt_ill\n1  England 198 0.11616162 0.1130435 53.39821 -2.976786 46.46465 19.19192\n2  England 348 0.16954023 0.1458333 53.39813 -2.969072 58.33333 33.62069\n3  England 333 0.09009009 0.1049724 53.39778 -2.965290 64.26426 23.72372\n4  England 330 0.15151515 0.1329787 53.39802 -2.963597 59.69697 23.03030\n5  England 320 0.04687500 0.1813725 53.39706 -2.968030 60.62500 25.00000\n6  England 240 0.05833333 0.2519685 53.39679 -2.966494 57.91667 28.33333\n    Bhealth VBhealth  no_qual   manprof                       geometry\n1  6.565657 1.515152 24.69136  7.643312 MULTIPOLYGON (((335187 3894...\n2 10.344828 1.436782 14.84848 13.375796 MULTIPOLYGON (((335834 3895...\n3  6.606607 2.102102 15.38462 10.204082 MULTIPOLYGON (((335975.2 38...\n4  5.151515 2.424242 17.91531 15.224913 MULTIPOLYGON (((336030.8 38...\n5  8.750000 2.187500 12.58278 11.333333 MULTIPOLYGON (((335804.9 38...\n6  6.666667 2.916667 27.47748  5.479452 MULTIPOLYGON (((335804.9 38...\n\n\n\n\n\nFig. 1. Data Structure.\n\n\nThe data are hierarchically structured: OAs nested within LSOAs; LSOAs nested within MSOAs; and, MSOAs nested within LADs. Observations nested within higher geographical units may be correlated.\nThis is one type of hierarchical structure. There is a range of data structures:\n\nStrict nested data structures eg. an individual unit is nested within only one higher unit\nRepeated measures structures eg. various measurements for an individual unit\nCrossed classified structures eg. individuals may work and live in different neighbourhoods\nMultiple membership structure eg. individuals may have two different work places\n\nWhy should we care about the structure of the data?\n\nDraw correct statistical inference: Failing to recognise hierarchical structures will lead to underestimated standard errors of regression coefficients and an overstatement of statistical significance. Standard errors for the coefficients of higher-level predictor variables will be the most affected by ignoring grouping.\nLink context to individual units: We can link and understand the extent of group effects on individual outcomes eg. how belonging to a certain socio-economic group influences on future career opportunities.\nSpatial dependency: Recognising the hierarchical structure of data may help mitigate the effects of severe spatial autocorrelation.\n\nQuickly, let us get a better idea about the data and look at the number of OAs nested within LSOAs and MSOAs\n\n# mean of nested OAs within LSOAs and MSOAs\nlsoa_cd %>% table() %>%\n  mean() %>%\n  round(, 2)\n\n[1] 5\n\nmsoa_cd %>% table() %>%\n  mean() %>%\n  round(, 2)\n\n[1] 26\n\n# number of OAs nested within LSOAs and MSOAs\nlsoa_cd %>% table() %>%\n  sort() %>%\n  plot()\n\n\n\nmsoa_cd %>% table() %>%\n  sort() %>%\n  plot()"
  },
  {
    "objectID": "07-multilevel-01.html#modelling",
    "href": "07-multilevel-01.html#modelling",
    "title": "7  Multilevel Modelling - Part 1",
    "section": "7.3 Modelling",
    "text": "7.3 Modelling\nWe should now be persuaded that ignoring the hierarchical structure of data may be a major issue. Let us now use a simple example to understand the intuition of multilevel model using the census data. We will seek to understand the spatial distribution of the proportion of population in unemployment in Liverpool, particularly why and where concentrations in this proportion occur. To illustrate the advantages of taking a multilevel modelling approach, we will start by estimating a linear regression model and progressively building complexity. We will first estimate a model and then explain the intuition underpinning the process. We will seek to gain a general understanding of multilevel modelling. If you are interested in the statistical and mathemathical formalisation of the underpinning concepts, please refer to Gelman and Hill (2006).\nWe first need to want to understand our dependent variable: its density ditribution;\n\nggplot(data = oa_shp) +\ngeom_density(alpha=0.8, colour=\"black\", fill=\"lightblue\", aes(x = unemp)) +\n   theme_classic()\n\n\n\n\n\nsummary(unemp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.05797 0.10256 0.11581 0.16129 0.50000 \n\n\nand, its spatial distribution:\n\n# ensure geometry is valid\noa_shp = sf::st_make_valid(oa_shp)\n\n# create a map\nlegend_title = expression(\"% unemployment\")\nmap_oa = tm_shape(oa_shp) +\n  tm_fill(col = \"unemp\", title = legend_title, palette = magma(256, begin = 0.25, end = 1), style = \"cont\") + \n  tm_borders(col = \"white\", lwd = .01)  + \n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 4) + \n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position =  c(\"center\", \"bottom\")) \nmap_oa\n\n\n\n\nLet us look at those areas:\n\n# high %s\noa_shp %>% filter(unemp > 0.2) %>% \n  dplyr::select(oa_cd, pop, unemp) \n\nSimple feature collection with 203 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 333993.8 ymin: 379748.5 xmax: 345600.2 ymax: 397681.5\nProjected CRS: Transverse_Mercator\nFirst 10 features:\n       oa_cd pop     unemp                       geometry\n1  E00032992 240 0.2519685 POLYGON ((335804.9 389421.6...\n2  E00033008 345 0.2636364 POLYGON ((335080 388528, 33...\n3  E00033074 299 0.2075472 POLYGON ((336947.3 387766.7...\n4  E00033075 254 0.2288136 POLYGON ((336753.6 387465.2...\n5  E00033080 197 0.2647059 POLYGON ((338196 387079, 33...\n6  E00033103 298 0.2148148 POLYGON ((340484 385429.6, ...\n7  E00033116 190 0.2156863 POLYGON ((341960.7 386422.1...\n8  E00033134 190 0.2674419 POLYGON ((337137 393089.6, ...\n9  E00033137 289 0.2661290 POLYGON ((337363.8 392122.4...\n10 E00033138 171 0.3561644 POLYGON ((337481.5 392166.2...\n\n\n\n7.3.1 Baseline Linear Regression Model\nNow let us estimate a simple linear regression model with the intercept only:\n\n# specify a model equation\neq1 <- unemp ~ 1\nmodel1 <- lm(formula = eq1, data = oa_shp)\n\n# estimates\nsummary(model1)\n\n\nCall:\nlm(formula = eq1, data = oa_shp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11581 -0.05784 -0.01325  0.04548  0.38419 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.115812   0.001836   63.09   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07306 on 1583 degrees of freedom\n\n\nTo understand the differences between the linear regression model and multilevel models, let us consider the model we have estimated:\n\\[y_{i} = \\beta_{0} + e_{i}\\] where \\(y_{i}\\) represents the proportion of the unemployed resident population in the OA \\(i\\); \\(\\beta_{0}\\) is the regression intercept and measures the average proportion of the unemployed resident population across OAs; and, \\(e_{i}\\) is the error term. But how do we deal with the hierarchical structure of the data?\n\n7.3.1.1 Limitations\nBefore looking at the answer, let’s first understand some of the key limitations of the linear regression model to handle the hierarchical structure of data. A key limitation of the linear regression model is that it only captures average relationships in the data. It does not capture variations in the relationship between variables across areas or groups. Another key limitation is that the linear regression model can capture associations at either macro or micro levels, but it does not simultaneously measure their interdependencies.\nTo illustrate this, let us consider the regression intercept. It indicates that the average percentage of unemployed population at the OA level is 0.12 but this model ignores any spatial clustering ie. the percentage of unemployed population tends to be similar across OAs nested within a same LSOA or MSOA. A side effect of ignoring this is that our standard errors are biased, and thus claims about statistical significance based on them would be misleading. Additionally, this situation also means we cannot explore variations in the percentage of unemployed population across LSOAs or MSOAs ie. how the percentage of unemployed population may be dependent on various contextual factors at these geographical scales.\n\n\n7.3.1.2 Fixed Effect Approach\nAn alternative approach is to adopt a fixed effects approach, or no-pooling model; that is, adding dummy variables indicating the group classification into the regression model eg. the way OAs is nested within LSOAs (or MSOAs). This approach has limitations. First, there is high risk of overfitting. The number of groups may be too large, relative to the number of observations. Second, the estimation of multiple parameters may be required so that measuring differences between groups may be challenging. Third, a fixed effects approach does not allow including group-level explanatory variables. You can try fitting a linear regression model extending our estimated model to include dummy variables for individual LSOAs (and/or MSOAs) so you can compare this to the multilevel model below.\nAn alternative is fitting separate linear regression models for each group. This approach is not always possible if there are groups with small sizes."
  },
  {
    "objectID": "07-multilevel-01.html#multilevel-modelling-random-intercept-model",
    "href": "07-multilevel-01.html#multilevel-modelling-random-intercept-model",
    "title": "7  Multilevel Modelling - Part 1",
    "section": "7.4 Multilevel Modelling: Random Intercept Model",
    "text": "7.4 Multilevel Modelling: Random Intercept Model\nWe use multilevel modelling to account for the hierarchical nature of the data by explicitly recognising that OAs are nested within LSOAs and MSOAs. Multilevel models can easily be estimated using in R using the package lme4. We implement an two-level model to allow for variation across LSOAs. We estimate an only intercept model allowing for variation across LSOAs. In essence, we are estimating a model with varying intercept coefficient by LSOA. As you can see in the code chunk below, the equation has an additional component. This is the group component or LSOA effect. The (1 | lsoa_cd) means that we are allowing the intercept, represented by 1, to vary by LSOA.\n\n# specify a model equation\neq2 <- unemp ~ 1 + (1 | lsoa_cd)\nmodel2 <- lmer(eq2, data = oa_shp)\n\n# estimates\nsummary(model2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ 1 + (1 | lsoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4382.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8741 -0.5531 -0.1215  0.4055  5.8207 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n lsoa_cd  (Intercept) 0.002701 0.05197 \n Residual             0.002575 0.05074 \nNumber of obs: 1584, groups:  lsoa_cd, 298\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.114316   0.003277   34.89\n\n\nWe can estimate a three-level model by adding (1 | msoa_cd) to allow the intercept to also vary by MSOAs and account for the nesting structure of LSOAs within MSOAs.\n\n# specify a model equation\neq3 <- unemp ~ 1 + (1 | lsoa_cd) + (1 | msoa_cd)\nmodel3 <- lmer(eq3, data = oa_shp)\n\n# estimates\nsummary(model3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ 1 + (1 | lsoa_cd) + (1 | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4529.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5624 -0.5728 -0.1029  0.4228  6.1363 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n lsoa_cd  (Intercept) 0.0007603 0.02757 \n msoa_cd  (Intercept) 0.0020735 0.04554 \n Residual             0.0025723 0.05072 \nNumber of obs: 1584, groups:  lsoa_cd, 298; msoa_cd, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.115288   0.006187   18.64\n\n\nWe see two sets of coefficients: fixed effects and random effects. Fixed effects correspond to the standard linear regression coefficients. Their interpretation is as usual. Random effects are the novelty. It is a term in multilevel modelling and refers to varying coefficients i.e. the randomness in the probability of the model for the group-level coefficients. Specifically they relate to estimates of the average variance and standard deviation within groups (i.e. LSOAs or MSOAs). Intiutively, variance and standard deviation indicate the extent to which the intercept, on average, varies by LSOAs and MSOAs.\n\n\n\nFig. 2. Variation of observations around their level 1 group mean.\n\n\n\n\n\nFig. 3. Variation of level 1 group mean around their level 2 group mean.\n\n\n\n\n\nFig. 4. Grand mean.\n\n\nMore formally, we first estimated the simplest regression model which is an intercept-only model and equivalent to the sample mean (i.e. the fixed part of the model):\n\\[y_{ijk} = \\mu + e_{ijk}\\] and then we made the random part of the model (\\(e_{ijk}\\)) more complex to account for the hierarchical structure of the data by estimating the following three-level regression model:\n\\[y_{ijk} = \\mu + u_{i..} + u_{ij.} + e_{ijk}\\]\nwhere \\(y_{ijk}\\) represents the proportion of unemployed population in OA \\(i\\) nested within LSOA \\(j\\) and MSOA \\(k\\); \\(\\mu\\) represents the sample mean and the fixed part of the model; \\(e_{ijk}\\) is the deviation of an observation from its LSOA mean; \\(u_{ij.}\\) is the deviation of the LSOA mean from its MSOA mean; \\(u_{i..}\\) is the deviation of the MSOA mean from the fixed part of the model \\(\\mu\\). Conceptually, this model is decomposing the variance of the model in terms of the hierarchical structure of the data. It is partitioning the observation’s residual into three parts or variance components. These components measure the relative extent of variation of each hierarchical level ie. LSOA, MSOA and grand means. To estimate the set of residuals, they are assumed to follow a normal distribution and are obtained after fitting the model and are based on the estimates of the model parameters (i.e. intercept and variances of the random parameters).\nLet’s now return to our three-level model (reported again below), we see that the intercept or fixed part of the model is the same as for the linear regression. The multilevel model reports greater standard errors. Multilevel models capture the hierarchical structure of the data and thus more precisely estimate the standard errors for our parameters.\n\n# report model 3\nsummary(model3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ 1 + (1 | lsoa_cd) + (1 | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4529.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5624 -0.5728 -0.1029  0.4228  6.1363 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n lsoa_cd  (Intercept) 0.0007603 0.02757 \n msoa_cd  (Intercept) 0.0020735 0.04554 \n Residual             0.0025723 0.05072 \nNumber of obs: 1584, groups:  lsoa_cd, 298; msoa_cd, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.115288   0.006187   18.64\n\n\n\n7.4.1 Interpretation\n\nFixed effects\n\nWe start by examining the fixed effects or estimated model averaging over LSOAs and MSOAs, \\(y_{ijk} = 0.115288\\) which can also be called by executing:\n\nfixef(model3)\n\n(Intercept) \n  0.1152881 \n\n\nTh estimated intercept indicates that the overall mean taken across LSOAs and MSOAs is estimated as 0.115288 and is statistically significant at 5% significance.\n\nRandom effects\n\nThe set of random effects contains three estimates of variance and standard deviation and refer to the variance components discussed above. The lsoa_cd, msoa_cd and Residual estimates indicate that the extent of estimated LSOA-, MSOA- and individual-level variance is 0.0007603, 0.0020735 and 0.0025723, respectively.\n\n\n7.4.2 Variance Partition Coefficient (VPC)\nThe purpose of multilevel models is to partition variance in the outcome between the different groupings in the data. We thus often want to know the percentage of variation in the dependent variable accounted by differences across groups i.e. what proportion of the total variance is attributable to variation within-groups, or how much is found between-groups. The statistic to obtain this is termed the variance partition coefficient (VPC), or intraclass correlation.2 For our case, the VPC at the LSOA level indicates that 14% of the variation in percentage of unemployed resident population across OAs can be explained by differences across LSOAs. What is the VPC at the MSOA level?\n\nvpc_lsoa <- 0.0007603 / (0.0007603 + 0.0020735 + 0.0025723)\nvpc_lsoa * 100\n\n[1] 14.06374\n\n\nYou can also obtain the VPC by executing:\n\nsumm(model3)\n\n\n\n\n  \n    Observations \n    1584 \n  \n  \n    Dependent variable \n    unemp \n  \n  \n    Type \n    Mixed effects linear regression \n  \n\n \n\n  \n    AIC \n    -4521.26 \n  \n  \n    BIC \n    -4499.79 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.00 \n  \n  \n    Pseudo-R² (total) \n    0.52 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n \n\n  \n    (Intercept) \n    0.12 \n    0.01 \n    18.63 \n    59.98 \n    0.00 \n  \n\n\n  p values calculated using Kenward-Roger standard errors and d.f. \n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    lsoa_cd \n    (Intercept) \n    0.03 \n  \n  \n    msoa_cd \n    (Intercept) \n    0.05 \n  \n  \n    Residual \n     \n    0.05 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    lsoa_cd \n    298 \n    0.14 \n  \n  \n    msoa_cd \n    61 \n    0.38 \n  \n\n\n\n\n\n\n7.4.3 Uncertainty of Estimates\nYou may have noticed that lme4 does not provide p-values, because of various reasons as explained by Doug Bates, one of the author of lme4. These explanations mainly refer to the complexity of dealing with varying sample sizes at a given hierarchical level. The number of observations at each hierarchical level varies across individual groupings (i.e. LSOA or MSOA). It may even be one single observation. This has implications for the distributional assumptions, denominator degrees of freedom and how to approximate a “best” solution. Various approaches exist to compute the statistical significance of estimates. We use the confint function available within lme4 to obtain confidence intervals.\n\nconfint(model3, level = 0.95)\n\nComputing profile confidence intervals ...\n\n\n                 2.5 %     97.5 %\n.sig01      0.02360251 0.03189046\n.sig02      0.03707708 0.05562307\n.sigma      0.04882281 0.05273830\n(Intercept) 0.10307341 0.12751103\n\n\n.sig01 refers to the LSOA level; .sig02 refers to the MSOA level; and, .sigma refers to the OA level.\n\n\n7.4.4 Assessing Group-level Variation\nEstimated regression coefficients\nIn multilevel modelling, our primary interest is in knowing differences across groups. To visualise the estimated model within each group (ie. LSOA and MSOA), we type:\n\ncoef_m3 <- coef(model3)\nhead(coef_m3$lsoa_cd,5)\n\n          (Intercept)\nE01006512  0.09915456\nE01006513  0.09889615\nE01006514  0.09297051\nE01006515  0.09803754\nE01006518  0.09642939\n\n\nThe results indicate that the estimated regression line is \\(y = 0.09915456\\) for LSOA E01006512; \\(y = 0.09889615\\) for LSOA E01006513 and so forth. Try getting the estimated model within each MSOA.\nRandom effects\nWe can look at the estimated group-level (or LSOA-level and MSOA-level) errors; that is, random effects:\n\nranef_m3 <- ranef(model3)\nhead(ranef_m3$lsoa_cd, 5)\n\n          (Intercept)\nE01006512 -0.01613353\nE01006513 -0.01639194\nE01006514 -0.02231758\nE01006515 -0.01725055\nE01006518 -0.01885870\n\n\nGroup-level errors indicate how much the intercept is shifted up or down in particular groups (ie. LSOAs or MSOAs). Thus, for example, in LSOA E01006512, the estimated intercept is -0.01613353 lower than average, so that the regression line is (0.1152881 - 0.01613353) = 0.09915457 which is what we observed from the call to coef().\nWe can also obtain group-level errors (random effects) by using a simulation approach, labelled “Empirical Bayes” and discussed here. To this end, we run:\n\n# obtain estimates\nmerTools::REsim(model3) %>% head(10)\n\n   groupFctr   groupID        term         mean       median          sd\n1    lsoa_cd E01006512 (Intercept) -0.016002410 -0.016552729 0.018910787\n2    lsoa_cd E01006513 (Intercept) -0.015120650 -0.015246887 0.017554913\n3    lsoa_cd E01006514 (Intercept) -0.020370126 -0.022097832 0.018284528\n4    lsoa_cd E01006515 (Intercept) -0.016206147 -0.017888919 0.018293444\n5    lsoa_cd E01006518 (Intercept) -0.020501000 -0.021752564 0.018446518\n6    lsoa_cd E01006519 (Intercept) -0.017057628 -0.016702244 0.009553431\n7    lsoa_cd E01006520 (Intercept) -0.025363202 -0.026071376 0.019551379\n8    lsoa_cd E01006521 (Intercept)  0.006068284  0.006052444 0.020684900\n9    lsoa_cd E01006522 (Intercept)  0.019264901  0.020312684 0.019363446\n10   lsoa_cd E01006523 (Intercept)  0.002203160  0.000432903 0.018512164\n\n\nThe results contain the estimated mean, median and standard deviation for the intercept within each group (e.g. LSOA). The mean estimates are similar to those obtained from ranef with some small differences due to rounding.\nTo gain an undertanding of the general pattern of the random effects, we can use caterpillar plots via plotREsim - reported below. The plot on the right shows the estimated random effects for each MSOA and their respective interval estimate. Note that random effects are on average zero, represented by the red horizontal line. Intervals that do not include zero are in bold. Also note that the width of the confidence interval depends on the standard error of the respective residual estimate, which is inversely related to the size of the sample. The residuals represent an observation departures from the grand mean, so an observation whose confidence interval does not overlap the line at zero (representing the mean proportion of unemployed population across all areas) is said to differ significantly from the average at the 5% level.\n\n# plot\nplotREsim(REsim(model3)) \n\n\n\n\nFocusing on the plot on the right, we see MSOAs whose mean proportion of unemployed population, assuming no explanatory variables, is lower than average. On the right-hand side of the plot, you will see MSOAs whose mean proportion is higher than average. The MSOAs with the smallest residuals include the districts of Allerton and Hunt Cross, Church, Childwall, Wavertree and Woolton. What districts do we have at the other extreme?\n\nre <- REsim(model3)\noa_shp %>% dplyr::select(msoa_cd, ward_nm, unemp) %>%\n    filter(as.character(msoa_cd) == \"E02001387\" | as.character(msoa_cd) == \"E02001393\")\n\nSimple feature collection with 49 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 339178.6 ymin: 386244.2 xmax: 341959.9 ymax: 389646.7\nProjected CRS: Transverse_Mercator\nFirst 10 features:\n     msoa_cd                  ward_nm      unemp                       geometry\n1  E02001393 Allerton and Hunts Cross 0.03246753 POLYGON ((341333.6 387163.2...\n2  E02001393 Allerton and Hunts Cross 0.03684211 POLYGON ((340658.2 387205.6...\n3  E02001393                   Church 0.04098361 POLYGON ((339908.1 387222.3...\n4  E02001393 Allerton and Hunts Cross 0.05982906 POLYGON ((340306 386587, 34...\n5  E02001393                   Church 0.01212121 POLYGON ((339974.2 387118.5...\n6  E02001393                   Church 0.09219858 POLYGON ((340181.4 386957.8...\n7  E02001393                   Church 0.01986755 POLYGON ((340301.2 386582.2...\n8  E02001393                   Church 0.04615385 POLYGON ((340375.9 386918.6...\n9  E02001393 Allerton and Hunts Cross 0.04117647 POLYGON ((340435.3 386337.4...\n10 E02001393 Allerton and Hunts Cross 0.02272727 POLYGON ((340681.7 386614.4...\n\n\nWe can also map the MSOA-level random effects. To this end, we first need to read a shapefile containing data at the MSOA level and merge it with the random effects estimates.\n\n# read data\nmsoa_shp <- st_read(\"data/mlm/MSOA.shp\")\n\nReading layer `MSOA' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/mlm/MSOA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1\nProjected CRS: Transverse_Mercator\n\n# create a dataframe for MSOA-level random effects\nre_msoa <- re %>% filter(groupFctr == \"msoa_cd\")\nstr(re_msoa)\n\n'data.frame':   61 obs. of  6 variables:\n $ groupFctr: chr  \"msoa_cd\" \"msoa_cd\" \"msoa_cd\" \"msoa_cd\" ...\n $ groupID  : chr  \"E02001347\" \"E02001348\" \"E02001349\" \"E02001350\" ...\n $ term     : chr  \"(Intercept)\" \"(Intercept)\" \"(Intercept)\" \"(Intercept)\" ...\n $ mean     : num  -0.01242 -0.02377 -0.03153 0.00429 0.02328 ...\n $ median   : num  -0.0144 -0.0228 -0.03 0.0038 0.0222 ...\n $ sd       : num  0.0314 0.0324 0.032 0.0315 0.0173 ...\n\n# merge data\nmsoa_shp <- merge(x = msoa_shp, y = re_msoa, by.x = \"MSOA_CD\", by.y = \"groupID\")\n\nNow we can create our map:\n\n# ensure geometry is valid\nmsoa_shp = sf::st_make_valid(msoa_shp)\n\n# create a map\nlegend_title = expression(\"MSOA-level residuals\")\nmap_msoa = tm_shape(msoa_shp) +\n  tm_fill(col = \"mean\", title = legend_title, palette = magma(256, begin = 0, end = 1), style = \"cont\") + \n  tm_borders(col = \"white\", lwd = .01)  + \n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 4) + \n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position =  c(\"center\", \"bottom\")) \nmap_msoa\n\n\n\n\n\n\n7.4.5 Adding Individual-level Predictors\nIn this example, \\(\\mu\\) represents the sample mean but it could include a collection of independent variables or predictors. To explain the logic, we will assume that unemployment is strongly associated to long-term illness. We could expect that long-term illness (lt_ill) will reduce the chances of working and therefore being unemployed. Note that our focus is on the relationship, not on establishing causation. Specifically we want to estimate the relationship between unemployment and long-term illness and we are interested in variations in OA-level unemployment by MSOAs so we will estimate the following two-level model:\nOA-level:\n\\[y_{ij} = \\beta_{0j} + \\beta_{1}x_{ij} + e_{ij}\\] MSOA-level:\n\\[\\beta_{0j} = \\beta_{0} + u_{0j}\\] Replacing the first equation into the second, we have:\n\\[y_{ij} = (\\beta_{0} + u_{0j}) + \\beta_{1}x_{ij} + e_{ij}\\] where \\(y\\) the proportion of unemployed population in OA \\(i\\) within MSOA \\(j\\); \\(\\beta_{0}\\) is the fixed intercept (averaging over all MSOAs); \\(u_{0j}\\) represents the MSOA-level residuals or random effects; \\(\\beta_{0}\\) and \\(u_{0j}\\) together represent the varying-intercept; \\(\\beta_{1}\\) is the slope coefficient; \\(x_{ij}\\) represents the percentage of long-term illness population; and, \\(e_{ij}\\) is the individual-level residuals.\nWe estimate the model executing:\n\n# change to proportion\noa_shp$lt_ill <- lt_ill/100\n\n# specify a model equation\neq4 <- unemp ~ lt_ill + (1 | msoa_cd)\nmodel4 <- lmer(eq4, data = oa_shp)\n\n# estimates\nsummary(model4)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ lt_ill + (1 | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4711.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1941 -0.5718 -0.0906  0.4507  5.9393 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n msoa_cd  (Intercept) 0.001421 0.03769 \n Residual             0.002674 0.05171 \nNumber of obs: 1584, groups:  msoa_cd, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.04682    0.00625   7.492\nlt_ill       0.29588    0.01615  18.317\n\nCorrelation of Fixed Effects:\n       (Intr)\nlt_ill -0.600\n\n\nFixed effects: model averaging over MSOAs\n\nfixef(model4)\n\n(Intercept)      lt_ill \n 0.04681959  0.29588110 \n\n\nyields an estimated regression line in an average McSOA: \\(y = 0.04681959 + 0.29588110x\\)\nRandom effects: MSOA-level errors\n\nranef_m4 <- ranef(model4)\nhead(ranef_m4$msoa_cd, 5)\n\n           (Intercept)\nE02001347 -0.017474815\nE02001348 -0.021203807\nE02001349 -0.022469313\nE02001350 -0.003539869\nE02001351  0.008502813\n\n\nyields an estimated intercept for MSOA E02001347 which is 0.017474815 lower than the average with a regression line: (0.04681959 - 0.017474815) + 0.29588110x = 0.02934478 + 0.29588110x. You can confirm this by looking at the estimated model within each MSOA by executing (remove the # sign):\n\n#coef(model4)\n\nFixed effect correlations\nIn the bottom of the output, we have the correlations between the fixed-effects estimates. In our example, it refers to the correlation between \\(\\beta_{0}\\) and \\(\\beta_{1}\\). It is negative indicating that in MSOAs where the relationship between unemployment and long-term illness is greater, as measured by \\(\\beta_{1}\\), the average proportion of unemployed people tends to be smaller, as captured by \\(\\beta_{0}\\).\n\n\n7.4.6 Adding Group-level Predictors\nWe can also add group-level predictors. We use the formulation:\nOA-level:\n\\[y_{ij} = \\beta_{0j} + \\beta_{1}x_{ij} + e_{ij}\\]\nMSOA-level:\n\\[\\beta_{0j} = \\beta_{0} + \\gamma_{1}m_{j} + u_{0j}\\]\nwhere \\(x_{ij}\\) is the OA-level proportion of population suffering long-term illness and \\(m_{j}\\) is the MSOA-level proportion of male population. We first need to create this group-level predictor:\n\n# detach OA shp and attach MSOA shp\ndetach(oa_shp)\nattach(msoa_shp)\n\nThe following object is masked from package:viridis:\n\n    unemp\n\n# group-level predictor\nmsoa_shp$pr_male <- males/pop\n\n# remove geometries\nmsoa_df <- `st_geometry<-`(msoa_shp, NULL)\n\n# select variables\nmsoa_df <- msoa_df %>% dplyr::select(MSOA_CD, pop, pr_male)\n\n# merge data sets\noa_shp <- merge(x=oa_shp, y=msoa_df, by.x = \"msoa_cd\", by.y=\"MSOA_CD\")\n\n# inspect data\nhead(oa_shp[1:10, c(\"msoa_cd\", \"oa_cd\", \"unemp\", \"pr_male\")])\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 337693.5 ymin: 396068.2 xmax: 339430.9 ymax: 397790\nProjected CRS: Transverse_Mercator\n    msoa_cd     oa_cd      unemp   pr_male                       geometry\n1 E02001347 E00033730 0.10322581 0.4775905 POLYGON ((338376 397059, 33...\n2 E02001347 E00033722 0.06306306 0.4775905 POLYGON ((337929.4 397669.9...\n3 E02001347 E00033712 0.09090909 0.4775905 POLYGON ((338830 396068.2, ...\n4 E02001347 E00033739 0.09401709 0.4775905 POLYGON ((339140.3 397191, ...\n5 E02001347 E00033719 0.05855856 0.4775905 POLYGON ((338128.8 397658.6...\n6 E02001347 E00033711 0.12195122 0.4775905 POLYGON ((339163.2 396833.6...\n\n\nWe can now estimate our model:\n\ndetach(msoa_shp)\nattach(oa_shp)\n\nThe following object is masked from package:viridis:\n\n    unemp\n\n# specify a model equation\neq5 <- unemp ~ lt_ill + pr_male + (1 | msoa_cd)\nmodel5 <- lmer(eq5, data = oa_shp)\n\n# estimates\nsummary(model5)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ lt_ill + pr_male + (1 | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4712.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.2162 -0.5696 -0.0929  0.4549  5.9370 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n msoa_cd  (Intercept) 0.001391 0.03729 \n Residual             0.002674 0.05171 \nNumber of obs: 1584, groups:  msoa_cd, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.07746    0.08768  -0.883\nlt_ill       0.29781    0.01620  18.389\npr_male      0.25059    0.17642   1.420\n\nCorrelation of Fixed Effects:\n        (Intr) lt_ill\nlt_ill  -0.118       \npr_male -0.997  0.075\n\n\nThis model includes the proportion of males and intercepts that vary by MSOA. The lmer() function only accepts predictors at the individual level, so we have included data on the proportion of male population at this level. Explore and interpret the model running the functions below:\n\n# fixed effects\nfixef(model5)\n\n(Intercept)      lt_ill     pr_male \n -0.0774607   0.2978084   0.2505913 \n\n\n\n# random effects\nranef_m5 <- ranef(model5)\nhead(ranef_m5$msoa_cd, 5)\n\n           (Intercept)\nE02001347 -0.013625261\nE02001348 -0.019757846\nE02001349 -0.023709992\nE02001350  0.003003861\nE02001351  0.003508477\n\n\nAdding group-level predictors tends to improve inferences for group coefficients. Examine the confidence intervals, in order to evalute how the precision of our estimates of the MSOA intercepts have changed. Have confidence intervals for the intercepts of Model 4 and 5 increased or reduced? Hint: look at how to get the confidence intervals above."
  },
  {
    "objectID": "07-multilevel-01.html#questions",
    "href": "07-multilevel-01.html#questions",
    "title": "7  Multilevel Modelling - Part 1",
    "section": "7.5 Questions",
    "text": "7.5 Questions\nFor the second assignment, we will be using a different dataset comprising information on COVID-19 cases, census data and the Index of Multiple Deprivation (IMD) for England. The data set is similar in structured to that used in this chapter. It is hierarchically organised into 149 Upper Tier Local Authority Districts (UTLADs) within 9 Regions and has 508 variables - see Chapter @ref(datasets) for a more detailed description of the data.\n\nsdf <- st_read(\"data/assignment_2_covid/covid19_eng.gpkg\")\n\nReading layer `covid19_eng' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_2_covid/covid19_eng.gpkg' \n  using driver `GPKG'\nSimple feature collection with 149 features and 507 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n\nHere we see a selection of 10 variables for 5 UTLADs.\n\nhead(sdf[1:5,c(3,4,9,10,381,385,386,387,403,406)])\n\nSimple feature collection with 5 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 418871.2 ymin: 506329.3 xmax: 478441.5 ymax: 537152\nProjected CRS: OSGB36 / British National Grid\n             ctyua19nm     Region X2020.01.31 X2020.02.01 IMD...Average.score\n1           Hartlepool North East           0           0              35.037\n2        Middlesbrough North East           0           0              40.460\n3 Redcar and Cleveland North East           0           0              29.792\n4     Stockton-on-Tees North East           0           0              25.790\n5           Darlington North East           0           0              25.657\n  Residents Households Dwellings Age_85plus White_British_and_Irish\n1     92028      40434     42102       1856                   89117\n2    138412      57203     59956       2465                  119680\n3    135177      59605     61899       3113                  132343\n4    191610      79159     82237       3481                  179501\n5    105564      46670     48644       2550                   99226\n                            geom\n1 MULTIPOLYGON (((447097 5371...\n2 MULTIPOLYGON (((449862.8 52...\n3 MULTIPOLYGON (((455939.7 52...\n4 MULTIPOLYGON (((444126.1 52...\n5 MULTIPOLYGON (((423475.7 52...\n\n\n\nctyua19nm: Upper Tier Local Authority District name\nRegion: Region name\nX2020.01.31: COVID-19 cases on January 31st 2020\nX2020.02.01: COVID-19 cases on February 1st 2020\nIMD...Average.score: Average IMD score for UTLADs - see File 11: upper-tier local authority summaries for information on this and associated indicators.\nResidents: Number of residents\nHouseholds: Number of households\nDwellings: Number of dwellings\nAge_85plus: Number of people aged 85 and over\nWhite_British_and_Irish: Number of white British and Irish people\n\nNote that variable names relating to the daily COVID-19 cases are organised in the following way: X stands for daily COVID-19 cases, followed by the year (i.e. 2020, 2021); month (i.e. January to December); and day (i.e. 01 to 31).\nUsing these data, you are required to address the following challenges:\n\nFit a varying-intercept model with no explanatory variables. Let the intercept to vary by region.\nFit a varying-intercept model with including at least three explanatory variables.\nCompute the Variance Partition Coefficient (VPC) for the models estimated according to points 1 and 2 above.\nCreate caterpillar plots to visualise the varying intercepts.\n\nAnalyse and discuss: 1. the extent of variation in the dependent variables at the two geographical scales (variation at which geographical scale explains most of variance in your dependent variable); 2. the varying intercept estimate(s) from your model(s) (what can they tell you about the difference between groups / areas? are they statistically significantly different?);\nEnsure you appropriately describe the structure of the data and identify the various geographical scales of analysis (i.e. level-1 and level-2 units)\nIn addressing the challenges in this and following chapters, you have some flexibility to be creative. A set of key factors to consider:\n\nDependent Variable: We will seek to explain daily COVID-19 cases, and you will need to make a decision as to:\n\n\nDaily vs cumulative COVID-19 cases. Given that we will be focusing on cross-sectional models (i.e. models for one snapshot), you can focus on modelling daily cases at one specific date or cumulative daily cases over a period of time.\nTime period. You can select the date or period of time that you will focus your analysis on.\nUse risk of COVID-19 infection. The dependent variable should be the risk or rate of COVID-19 infection.\n\nFor example, the risk of COVID-19 infection for the period (i.e. between Dec. 1st, 2020 - January 29th, 2021) comprising the third wave of the pandemic in the United Kingdom can be computed as:\n\n# computing cumulative COVID cases for  01/12/2020 - 29/01/2021\nsdf[, 509] <- sdf %>% dplyr::select(\"X2020.12.01\":\"X2021.01.29\") %>% # select COVID cases 01/12/2020 - 29/01/2021\n  mutate(cum_covid = rowSums(across(where(is.numeric)))) %>% # sum daily cases\n  dplyr::select(cum_covid) %>% # select cumulative cases\n   st_set_geometry(., NULL) # set geometry to NULL\n\n# computing risk of infection\nsdf <- sdf %>%  mutate(\n  covid19_r = round((cum_covid / Residents ) * 1000) \n  )\n\n\nExplanatory variables:\n\n\nAt least 3. Use at least 3 explanatory variables. There is no maximum limit but consider your model to be parsimonious.\nChoice your set. Select the set of variables you consider appropriate / interesting. Make sure you justify your choice based on evidence and/or theory.\nPercentages / Proportions. Use percentages or proportions to capture the composition of places, rather than numbers of people, households or dwellings. For this, ensure you are using the appropriate denominator.\n\nFor instance, if you want to capture the relationship between cumulative COVID-19 cases and overcrowding, share of elderly population and nonwhite minorities, use the following variables\n\nsdf <- sdf %>% mutate(\n  crowded_hou = Crowded_housing / Households, # share of crowded housing\n  elderly = (Age_85plus) / Residents, # share of population aged 65+\n  ethnic = (Mixed + Indian + Pakistani + Bangladeshi + Chinese + Other_Asian + Black + Other_ethnicity) / Residents, # share of nonwhite population\n)\n\nADVICE: Create a new spatial data frame including only the variables you will analyse. For example:\n\nnsdf <- sdf  %>%  dplyr::select(objectid, \n                         ctyua19cd, \n                         ctyua19nm, \n                         Region, \n                         covid19_r, \n                         crowded_hou, \n                         elderly, \n                         ethnic, \n                         Residents)\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nMultilevel Modelling, Centre for. n.d. “Introduction to Multilevel Modelling.” n.d. http://www.bristol.ac.uk/cmm/learning/online-course/course-topics.html#m04."
  },
  {
    "objectID": "08-multilevel-02.html#dependencies",
    "href": "08-multilevel-02.html#dependencies",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.1 Dependencies",
    "text": "8.1 Dependencies\nThis chapter uses the following libraries which are listed in the [Dependency list] Section of Chapter 1:\n\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis) # nice colour schemes\n# Fitting multilevel models\nlibrary(lme4)\n# Tools for extracting information generated by lme4\nlibrary(merTools)\n# Exportable regression tables\nlibrary(jtools)\nlibrary(stargazer)\nlibrary(sjPlot)"
  },
  {
    "objectID": "08-multilevel-02.html#data",
    "href": "08-multilevel-02.html#data",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.2 Data",
    "text": "8.2 Data\nFor this chapter, we will data for Liverpool from England’s 2011 Census. The original source is the Office of National Statistics and the dataset comprises a number of selected variables capturing demographic, health and socio-economic of the local resident population at four geographic levels: Output Area (OA), Lower Super Output Area (LSOA), Middle Super Output Area (MSOA) and Local Authority District (LAD). The variables include population counts and percentages. For a description of the variables, see the readme file in the mlm data folder.1\nLet us read the data:\n\n# clean workspace\nrm(list=ls())\n# read data\noa_shp <- st_read(\"data/mlm/OA.shp\")"
  },
  {
    "objectID": "08-multilevel-02.html#conceptual-overview",
    "href": "08-multilevel-02.html#conceptual-overview",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.3 Conceptual Overview",
    "text": "8.3 Conceptual Overview\nSo far, we have estimated varying-intercept models; that is, when the intercept (\\(\\beta_{0}\\)) is allowed to vary by group (eg. geographical area) - as shown in Fig. 1(a). The strength of the relationship between \\(y\\) (i.e. unemployment rate) and \\(x\\) (long-term illness) has been assumed to be the same across groups (i.e. MSOAs), as captured by the regression slope (\\(\\beta_{1}\\)). Yet it can also vary by group as shown in Fig. 1(b), or we can observe group variability for both intercepts and slopes as represented in Fig. 1(c).\n\n\n\nFig. 1. Linear regression model with (a) varying intercepts, (b) varying slopes, and (c) both. Source: Gelman and Hill (2006) p.238.\n\n\n\n8.3.1 Exploratory Analysis: Varying Slopes\nLet’s then explore if there is variation in the relationship between unemployment rate and the share of population in long-term illness. We do this by selecting the 8 MSOAs containing OAs with the highest unemployment rates in Liverpool.\n\n# Sort data \noa_shp <- oa_shp %>% arrange(-unemp)\noa_shp[1:9, c(\"msoa_cd\", \"unemp\")]\n\nSimple feature collection with 9 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335032 ymin: 387777 xmax: 338576.1 ymax: 395022.4\nProjected CRS: Transverse_Mercator\n    msoa_cd     unemp                       geometry\n1 E02001354 0.5000000 MULTIPOLYGON (((337491.2 39...\n2 E02001369 0.4960630 MULTIPOLYGON (((335272.3 39...\n3 E02001366 0.4461538 MULTIPOLYGON (((338198.1 39...\n4 E02001365 0.4352941 MULTIPOLYGON (((336572.2 39...\n5 E02001370 0.4024390 MULTIPOLYGON (((336328.3 39...\n6 E02001390 0.3801653 MULTIPOLYGON (((335833.6 38...\n7 E02001354 0.3750000 MULTIPOLYGON (((337403 3949...\n8 E02001385 0.3707865 MULTIPOLYGON (((336251.6 38...\n9 E02001368 0.3648649 MULTIPOLYGON (((335209.3 39...\n\n# Select MSOAs\ns_t8 <- oa_shp %>% dplyr::filter(\n    as.character(msoa_cd) %in% c(\n      \"E02001354\", \n      \"E02001369\", \n      \"E02001366\", \n      \"E02001365\", \n      \"E02001370\", \n      \"E02001390\", \n      \"E02001368\", \n      \"E02001385\")\n    )\n\nAnd then we generate a set of scatter plots and draw regression lines for each MSOA.\n\nggplot(s_t8, aes(x = lt_ill, y = unemp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  facet_wrap(~ msoa_cd, nrow = 2) +\n  ylab(\"Unemployment rate\") + \n  xlab(\"Long-term Illness (%)\") +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can observe great variability in the relationship between unemployment rates and the percentage of population in long-term illness. A strong and positive relationship exists in MSOA E02001366 (Tuebrook and Stoneycroft), while it is negative in MSOA E02001370 (Everton) and neutral in MSOA E02001390 (Princes Park & Riverside). This visual inspection suggests that accounting for differences in the way unmployment rates relate to long-term illness is important. Contextual factors may differ across MSOAs in systematic ways."
  },
  {
    "objectID": "08-multilevel-02.html#estimating-varying-intercept-and-slopes-models",
    "href": "08-multilevel-02.html#estimating-varying-intercept-and-slopes-models",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.4 Estimating Varying Intercept and Slopes Models",
    "text": "8.4 Estimating Varying Intercept and Slopes Models\nA way to capture for these group differences in the relationship between unemployment rates and long-term illness is to allow the relevant slope to vary by group (i.e. MSOA). We can do this estimating the following model:\nOA-level:\n\\[y_{ij} = \\beta_{0j} + \\beta_{1j}x_{ij} + e_{ij}\\]\nMSOA-level:\n\\[\\beta_{0j} = \\beta_{0} + u_{0j}\\] \\[\\beta_{1j} = \\beta_{1} + u_{1j} \\] Replacing the first equation into the second generates:\n\\[y_{ij} = (\\beta_{0} + u_{0j}) + (\\beta_{1} + u_{1j})x_{ij} + e_{ij}\\] where, as in the previous Chapter, \\(y\\) the proportion of unemployed population in OA \\(i\\) within MSOA \\(j\\); \\(\\beta_{0}\\) is the fixed intercept (averaging over all MSOAs); \\(u_{0j}\\) represents the MSOA-level residuals, or random effects, of the intercept; \\(e_{ij}\\) is the individual-level residuals; and, \\(x_{ij}\\) represents the percentage of long-term illness population. But now we have a varying slope represented by \\(\\beta_{1}\\) and \\(u_{1j}\\): \\(\\beta_{1}\\) is estimated average slope - fixed part of the model; and, \\(u_{1j}\\) is the estimated group-level errors of the slope.\nTo estimate such model, we add lt_ill in the bracket with a + sign between 1 and | i.e. (1 + lt_ill | msoa_cd).\n\n# attach df\nattach(oa_shp)\n\n# change to proportion\noa_shp$lt_ill <- lt_ill/100\n\n# specify a model equation\neq6 <- unemp ~ lt_ill + (1 + lt_ill | msoa_cd)\nmodel6 <- lmer(eq6, data = oa_shp)\n\n# estimates\nsummary(model6)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ lt_ill + (1 + lt_ill | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4762.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6639 -0.5744 -0.0873  0.4565  5.4876 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n msoa_cd  (Intercept) 0.003428 0.05855       \n          lt_ill      0.029425 0.17154  -0.73\n Residual             0.002474 0.04974       \nNumber of obs: 1584, groups:  msoa_cd, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.047650   0.008635   5.519\nlt_ill      0.301259   0.028162  10.697\n\nCorrelation of Fixed Effects:\n       (Intr)\nlt_ill -0.786\n\n\nIn this model, the estimated standard deviation of the unexplained within-MSOA variation is 0.04974, and the estimated standard deviation of the MSOA intercepts is 0.05855. But, additionally, we also have estimates of standard deviation of the MSOA slopes (0.17154) and correlation between MSOA-level residuals for the intercept and slope (-0.73). While the former measures the extent of average deviation in the slopes across MSOAs, the latter indicates that the intercept and slope MSOA-level residuals are negatively associated; that is, MSOAs with large slopes have relatively smaller intercepts and vice versa. We will come back to this in Section Interpreting Correlations Between Group-level Intercepts and Slopes.\nSimilarly, the correlation of fixed effects indicates a negative relationship between the intercept and slope of the average regression model; that is, as the average model intercept tends to increase, the average strength of the relationship between unemployment rate and long-term illness decreases and vice versa.\nWe then explore the estimated average coefficients (fixed effects):\n\nfixef(model6)\n\n(Intercept)      lt_ill \n 0.04765009  0.30125875 \n\n\nyields an estimated regression line in an average LSOA: \\(y = 0.04764998 + 0.30125916x\\). The fixed intercept indicates that the average unemployment rate is 0.05 if the percentage of population with long-term illness is zero.The fixed slope indicates that the average relationship between unemployment rate and long-term illness is positive across MSOAs i.e. as the percentage of population with long-term illness increases by 1 percentage point, the unemployment rate increases by 0.3.\nWe look the estimated MSOA-level errors (random effects):\n\nranef_m6 <- ranef(model6)\nhead(ranef_m6$msoa_cd, 5)\n\n           (Intercept)      lt_ill\nE02001347 -0.026561345  0.02718102\nE02001348  0.001688245 -0.11533102\nE02001349 -0.036084817  0.05547075\nE02001350  0.032240842 -0.14298734\nE02001351  0.086214137 -0.28130162\n\n\nRecall these estimates indicate the extent of deviation of the MSOA-specific intercept and slope from the estimated model average captured by the fixed model component.\nWe can also regain the estimated intercept and slope for each county by adding the estimated MSOA-level errors to the estimated average coefficients; or by executing:\n\n#coef(model6)\n\nWe are normally more interested in identifying the extent of deviation and its significance. To this end, we create a caterpillar plot:\n\n# plot\nplotREsim(REsim(model6))\n\n\n\n\nThese plots reveal some interesting patterns. First, only one MSOA, containing wards such as Tuebrook and Stoneycroft, Anfield & Everton, seems to have a statistically significantly different intercept, or average unemployment rate. Confidence intervals overlap zero for all other 60 MSOAs. Despite this, note that when a slope is allowed to vary by group, it generally makes sense for the intercept to also vary. Second, significant variability exists in the association between unemployment rate and long-term illness across MSOAs. Ten MSOAs display a significant positive association, while 12 exhibit a significantly negative relationship. Third, these results reveal that geographical differences in the relationship between unemployment rate and long-term illness can explain the significant differences in average unemployment rates in the varying intercept only model.\nLet’s try to get a better understanding of the varying relationship between unemployment rate and long-term illness by mapping the relevant MSOA-level errors.\n\n# read data\nmsoa_shp <- st_read(\"data/mlm/MSOA.shp\")\n\nReading layer `MSOA' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/mlm/MSOA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1\nProjected CRS: Transverse_Mercator\n\n# create a dataframe for MSOA-level random effects\nre_msoa_m6 <- REsim(model6) %>% filter(groupFctr == \"msoa_cd\") %>%\n  filter(term == \"lt_ill\")\nstr(re_msoa_m6)\n\n'data.frame':   61 obs. of  6 variables:\n $ groupFctr: chr  \"msoa_cd\" \"msoa_cd\" \"msoa_cd\" \"msoa_cd\" ...\n $ groupID  : chr  \"E02001347\" \"E02001348\" \"E02001349\" \"E02001350\" ...\n $ term     : chr  \"lt_ill\" \"lt_ill\" \"lt_ill\" \"lt_ill\" ...\n $ mean     : num  0.0251 -0.1179 0.0529 -0.1425 -0.2825 ...\n $ median   : num  0.0285 -0.1205 0.0559 -0.141 -0.2848 ...\n $ sd       : num  0.0441 0.0757 0.0915 0.0372 0.0397 ...\n\n# merge data\nmsoa_shp <- merge(x = msoa_shp, y = re_msoa_m6, by.x = \"MSOA_CD\", by.y = \"groupID\")\n\n\n# ensure geometry is valid\nmsoa_shp = sf::st_make_valid(msoa_shp)\n\n# create a map\nlegend_title = expression(\"MSOA-level residuals\")\nmap_msoa = tm_shape(msoa_shp) +\n  tm_fill(col = \"median\", title = legend_title, palette = magma(256, begin = 0, end = 1), style = \"cont\") + \n  tm_borders(col = \"white\", lwd = .01)  + \n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 4) + \n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position =  c(\"center\", \"bottom\")) \nmap_msoa\n\n\n\n\nThe map indicates that the relationship between unemployment rate and long-term illness is tends to stronger and positive in northern MSOAs; that is, the percentage of population with long-term illness explains a greater share of the variation in unemployment rates in these locations. As expected, a greater share of population in long-term illness is associated with higher local unemployment. In contrast, the relationship between unemployment rate and long-term illness tends to operate in the reverse direction in north-east and middle-southern MSOAs. In these MSOAs, OAs tend to have a higher unemployment rate relative the share of population in long-term illness. You can confirm this examining the data for specific MSOA executing:\n\noa_shp %>% dplyr::select(msoa_cd, ward_nm, unemp, lt_ill) %>%\n    filter(as.character(msoa_cd) == \"E02001370\")\n\nSimple feature collection with 23 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335885 ymin: 391134.2 xmax: 337596.3 ymax: 392467\nProjected CRS: Transverse_Mercator\nFirst 10 features:\n     msoa_cd                  ward_nm     unemp    lt_ill\n1  E02001370                  Everton 0.4024390 0.2792793\n2  E02001370 Tuebrook and Stoneycroft 0.3561644 0.3391813\n3  E02001370                  Everton 0.3285714 0.3106383\n4  E02001370                  Everton 0.3209877 0.3283019\n5  E02001370                  Anfield 0.3082707 0.1785714\n6  E02001370                  Everton 0.3000000 0.4369501\n7  E02001370                  Everton 0.2886598 0.3657143\n8  E02001370                  Everton 0.2727273 0.3375000\n9  E02001370                  Everton 0.2705882 0.2534247\n10 E02001370 Tuebrook and Stoneycroft 0.2661290 0.2941176\n                         geometry\n1  MULTIPOLYGON (((336328.3 39...\n2  MULTIPOLYGON (((337481.5 39...\n3  MULTIPOLYGON (((336018.5 39...\n4  MULTIPOLYGON (((336475.7 39...\n5  MULTIPOLYGON (((337110.6 39...\n6  MULTIPOLYGON (((336516.3 39...\n7  MULTIPOLYGON (((336668.6 39...\n8  MULTIPOLYGON (((336173.8 39...\n9  MULTIPOLYGON (((336870 3917...\n10 MULTIPOLYGON (((337363.8 39...\n\n\nNow try adding a group-level predictor and an individual-level predictor to the model. Unsure, look at Section 7.4.5 and Section 7.4.6 in Chapter 7."
  },
  {
    "objectID": "08-multilevel-02.html#interpreting-correlations-between-group-level-intercepts-and-slopes",
    "href": "08-multilevel-02.html#interpreting-correlations-between-group-level-intercepts-and-slopes",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.5 Interpreting Correlations Between Group-level Intercepts and Slopes",
    "text": "8.5 Interpreting Correlations Between Group-level Intercepts and Slopes\nCorrelations of random effects are confusing to interpret. Key for their appropriate interpretation is to recall they refer to group-level residuals i.e. deviation of intercepts and slopes from the average model intercept and slope. A strong negative correlation indicates that groups with high intercepts have relatively low slopes, and vice versa. A strong positive correlation indicates that groups with high intercepts have relatively high slopes, and vice versa. A correlation close to zero indicate little or no systematic between intercepts and slopes. Note that a high correlation between intercepts and slopes is not a problem, but it makes the interpretation of the estimated intercepts more challenging. For this reason, a suggestion is to center predictors (\\(x's\\)); that is, substract their average value (\\(z = x - \\bar{x}\\)). For a more detailed discussion, see Multilevel Modelling (n.d.).\nTo illustrate this, let’s reestimate our model adding an individual-level predictor: the share of population with no educational qualification.\n\n# centering to the mean\noa_shp$z_no_qual <- no_qual/100 - mean(no_qual/100)\noa_shp$z_lt_ill <- lt_ill - mean(lt_ill)\n\n# specify a model equation\neq7 <- unemp ~ z_lt_ill + z_no_qual + (1 + z_lt_ill | msoa_cd)\nmodel7 <- lmer(eq7, data = oa_shp)\n\n# estimates\nsummary(model7)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: unemp ~ z_lt_ill + z_no_qual + (1 + z_lt_ill | msoa_cd)\n   Data: oa_shp\n\nREML criterion at convergence: -4940.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6830 -0.5949 -0.0868  0.4631  6.3556 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n msoa_cd  (Intercept) 8.200e-04 0.02864       \n          z_lt_ill    2.161e-06 0.00147  -0.04\n Residual             2.246e-03 0.04739       \nNumber of obs: 1584, groups:  msoa_cd, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)  0.1163682  0.0039201   29.68\nz_lt_ill    -0.0003130  0.0003404   -0.92\nz_no_qual    0.3245811  0.0221347   14.66\n\nCorrelation of Fixed Effects:\n          (Intr) z_lt_l\nz_lt_ill  -0.007       \nz_no_qual -0.015 -0.679\n\n\nHow do you interpret the random effect correlation?"
  },
  {
    "objectID": "08-multilevel-02.html#model-building",
    "href": "08-multilevel-02.html#model-building",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.6 Model building",
    "text": "8.6 Model building\nNow we know how to estimate multilevel regression models in R. The question that remains is: When does multilevel modeling make a difference? The short answer is: when there is little group-level variation. When there is very little group-level variation, the multilevel modelling reduces to classical linear regression estimates with no group indicators. Inversely, when group-level coefficients vary greatly (compared to their standard errors of estimation), multilevel modelling reduces to classical regression with group indicators Gelman and Hill (2006).\nHow do you go about building a model?\nWe generally start simple by fitting simple linear regressions and then work our way up to a full multilevel model - see Gelman and Hill (2006) p. 270.\nHow many groups are needed?\nAs an absolute minimum, more than two groups are required. With only one or two groups, a multilevel model reduces to a linear regression model.\nHow many observations per group?\nTwo observations per group is sufficient to fit a multilevel model.\n\n8.6.1 Model Comparison\nHow we assess different candidate models? We can use the function anova() and assess various statistics: The Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), Loglik and Deviance. Generally, we look for lower scores for all these indicators. We can also refer to the Chisq statistic below. It tests the hypothesis of whether additional predictors improve model fit. Particularly it tests the Null Hypothesis whether the coefficients of the additional predictors equal 0. It does so comparing the deviance statistic and determining if changes in the deviance are statistically significant. Note that a major limitation of the deviance test is that it is for nested models i.e. a model being compared must be nested in the other. Below we compare our two models. The results indicate that adding an individual-level predictor (i.e. the share of population with no qualification) provides a model with better.\n\nanova(model6, model7)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: oa_shp\nModels:\nmodel6: unemp ~ lt_ill + (1 + lt_ill | msoa_cd)\nmodel7: unemp ~ z_lt_ill + z_no_qual + (1 + z_lt_ill | msoa_cd)\n       npar     AIC     BIC logLik deviance  Chisq Df Pr(>Chisq)    \nmodel6    6 -4764.7 -4732.5 2388.3  -4776.7                         \nmodel7    7 -4956.5 -4918.9 2485.2  -4970.5 193.76  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "08-multilevel-02.html#questions",
    "href": "08-multilevel-02.html#questions",
    "title": "8  Multilevel Modelling - Part 2",
    "section": "8.7 Questions",
    "text": "8.7 Questions\nWe will continue to use the COVID-19 dataset. Please see Chapter 11 for details on the data.\n\nsdf <- st_read(\"data/assignment_2_covid/covid19_eng.gpkg\")\n\nReading layer `covid19_eng' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_2_covid/covid19_eng.gpkg' \n  using driver `GPKG'\nSimple feature collection with 149 features and 507 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n\nUsing these data, you are required to address the following challenges:\n\nFit a varying-slope model. Let one slope to vary by region. Think carefully your choice.\nFit a varying-intercept and varying-slope model.\nCompare the results for models fitted in 1 and 2. Which is better? Why?\n\nUse the same explanatory variables used for the Chapter 7 challenge, so you can compare the model results from this chapter.\nAnalyse and discuss:\n\nthe varying slope estimate(s) from your model(s) (to what extent does the relationship between your dependent and independent variables vary across groups / areas? are they statistically significantly different?).\ndifferences between your varying intercept and varying slope models.\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\nMultilevel Modelling, Centre for. n.d. “Introduction to Multilevel Modelling.” n.d. http://www.bristol.ac.uk/cmm/learning/online-course/course-topics.html#m04."
  },
  {
    "objectID": "09-gwr.html#dependencies",
    "href": "09-gwr.html#dependencies",
    "title": "9  Geographically Weighted Regression",
    "section": "9.1 Dependencies",
    "text": "9.1 Dependencies\nThis chapter uses the following libraries:\n\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis) # nice colour schemes\n# Fitting geographically weighted regression models\nlibrary(spgwr)\n# Obtain correlation coefficients\nlibrary(corrplot)\n# Exportable regression tables\nlibrary(jtools)\nlibrary(stargazer)\nlibrary(sjPlot)\n# Assess multicollinearity\nlibrary(car)"
  },
  {
    "objectID": "09-gwr.html#data",
    "href": "09-gwr.html#data",
    "title": "9  Geographically Weighted Regression",
    "section": "9.2 Data",
    "text": "9.2 Data\nFor this chapter, we will use data on:\n\ncumulative COVID-19 confirmed cases from 1st January, 2020 to 14th April, 2020 from Public Health England via the GOV.UK dashboard;\nresident population characteristics from the 2011 census, available from the Office of National Statistics; and,\n2019 Index of Multiple Deprivation (IMD) data from GOV.UK and published by the Ministry of Housing, Communities & Local Government.\n\nThe data used for this Chapter are organised at the ONS Upper Tier Local Authority (UTLA) level - also known as Counties and Unitary Authorities. They are the geographical units used to report COVID-19 data.\nIf you use the dataset utilised in this chapter, make sure cite this book. For a full list of the variables included in the data set used in this Chapter, see the readme file in the gwr data folder.1\nLet’s read the data:\n\n# clean workspace\nrm(list=ls())\n# read data\nutla_shp <- st_read(\"data/gwr/Covid19_total_cases_geo.shp\") %>%\n  select(objct, cty19c, ctyu19nm, long, lat, st_rs, st_ln, X2020.04.14, I.PL1, IMD20, IMD2., Rsdnt, Hshld, Dwlln, Hsh_S, E_16_, A_65_, Ag_85, Mixed, Indin, Pkstn, Bngld, Chins, Oth_A, Black, Othr_t, CB_U_, Crwd_, Lng__, Trn__, Adm__, Ac___, Pb___, Edctn, H____, geometry)\n\n# replace nas with 0s\nutla_shp[is.na(utla_shp)] <- 0\n# explore data\nstr(utla_shp)"
  },
  {
    "objectID": "09-gwr.html#recap-spatial-effects",
    "href": "09-gwr.html#recap-spatial-effects",
    "title": "9  Geographically Weighted Regression",
    "section": "9.3 Recap: Spatial Effects",
    "text": "9.3 Recap: Spatial Effects\nTo this point, we have implicitly discussed three distinctive spatial effects:\n\nSpatial heterogeneity refers to the uneven distribution of a variable’s values across space\nSpatial dependence refers to the spatial relationship of a variable’s values for a pair of locations at a certain distance apart, so that they are more similar (or less similar) than expected for randomly associated pairs of observations\nSpatial nonstationarity refers to variations in the relationship between an outcome variable and a set of predictor variables across space\n\nIn previous sessions, we considered multilevel models to deal with spatial nonstationarity, recognising that the strength and direction of the relationship between an outcome \\(y\\) and a set of predictors \\(x\\) may vary over space. Here we consider a different approach, namely geographically weighted regression (GWR)."
  },
  {
    "objectID": "09-gwr.html#exploratory-analysis",
    "href": "09-gwr.html#exploratory-analysis",
    "title": "9  Geographically Weighted Regression",
    "section": "9.4 Exploratory Analysis",
    "text": "9.4 Exploratory Analysis\nWe will explore this technique through an empirical analysis considering the current global COVID-19 outbreak. Specifically we will seek to identify potential contextual factors that may be related to an increased risk of local infection. Population density, overcrowded housing, vulnerable individuals and critical workers have all been linked to a higher risk of COVID-19 infection.\nFirst, we will define and develop some basic understanding of our variable of interest. We define the risk of COVID-19 infection by the cumulative number of confirmed positive cases COVID-19 per 100,000 people:\n\n# risk of covid-19 infection\nutla_shp$covid19_r <- (utla_shp$X2020.04.14 / utla_shp$Rsdnt) * 100000\n\n# histogram\nggplot(data = utla_shp) +\ngeom_density(alpha=0.8, colour=\"black\", fill=\"lightblue\", aes(x = covid19_r)) +\n   theme_classic()\n\n\n\n# distribution in numbers\nsummary(utla_shp$covid19_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  32.11   92.81  140.15  146.66  190.96  341.56 \n\n\nThe results indicate a wide variation in the risk of infection across UTLAs in England, ranging from 31 to 342 confirmed positive cases of COVID-19 per 100,000 people with a median of 147. We map the cases to understand their spatial structure.\n\n# read region boundaries for a better looking map\nreg_shp <- st_read(\"data/gwr/Regions_December_2019_Boundaries_EN_BGC.shp\")\n\nReading layer `Regions_December_2019_Boundaries_EN_BGC' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/gwr/Regions_December_2019_Boundaries_EN_BGC.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 9 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 82672 ymin: 5342.7 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n# ensure geometry is valid\nutla_shp = sf::st_make_valid(utla_shp)\nreg_shp = sf::st_make_valid(reg_shp)\n\n# map\nlegend_title = expression(\"Cumulative cases per 100,000\")\nmap_utla = tm_shape(utla_shp) +\n  tm_fill(col = \"covid19_r\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_utla + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\n\n\n\nThe map shows that concentrations of high incidence of infections in the metropolitan areas of London, Liverpool, Newcastle, Sheffield, Middlesbrough and Birmingham. Below we list the UTLAs in these areas in descending order.\n\nhotspots <- utla_shp %>% select(ctyu19nm, covid19_r) %>%\n  filter(covid19_r > 190)\nhotspots[order(-hotspots$covid19_r),]\n\nSimple feature collection with 38 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 293941.4 ymin: 155850.8 xmax: 561956.7 ymax: 588517.4\nProjected CRS: Transverse_Mercator\nFirst 10 features:\n    ctyu19nm covid19_r                       geometry\n14     Brent  341.5645 POLYGON ((520113.1 190480.8...\n32 Southwark  337.1687 POLYGON ((532223 180545.4, ...\n27   Lambeth  305.5238 POLYGON ((531189.5 180531.3...\n22    Harrow  286.1254 POLYGON ((517363.8 194171.3...\n17   Croydon  276.8467 POLYGON ((531549.3 171045, ...\n12    Barnet  274.1410 POLYGON ((524645.2 198138.3...\n28  Lewisham  261.3408 POLYGON ((536691.6 178958.8...\n30    Newham  258.4550 POLYGON ((542600.7 186497.3...\n38   Cumbria  255.6726 MULTIPOLYGON (((321364.8 46...\n15   Bromley  253.7234 POLYGON ((542252.7 172828.7...\n\n\n\nChallenge 1: How does Liverpool ranked in this list?"
  },
  {
    "objectID": "09-gwr.html#global-regression",
    "href": "09-gwr.html#global-regression",
    "title": "9  Geographically Weighted Regression",
    "section": "9.5 Global Regression",
    "text": "9.5 Global Regression\nTo provide an intuitive understanding of GWR, a useful start is to explore the data using an ordinary least squares (OLS) linear regression model. The key issue here is to understand if high incidence of COVID-19 is linked to structural differences across UTLAs in England. As indicated above, confirmed positive cases of COVID-19 have been associated with overcrowded housing, vulnerable populations - including people in elderly age groups, economically disadvantaged groups and those suffering from chronic health conditions - ethnic minorities, critical workers in the health & social work, education, accommodation & food, transport, and administrative & support sectors. So, let’s create a set of variables to approximate these factors.\n\n# define predictors\nutla_shp <- utla_shp %>% mutate(\n  crowded_hou = Crwd_ / Hshld, # share of crowded housing\n  elderly = (A_65_ + Ag_85) / Rsdnt, # share of population aged 65+\n  lt_illness = Lng__ / Rsdnt, # share of population in long-term illness\n  ethnic = (Mixed + Indin + Pkstn + Bngld + Chins + Oth_A + Black + Othr_t) / Rsdnt, # share of nonwhite population\n  imd19_ext = IMD20, # proportion of a larger area’s population living in the most deprived LSOAs in the country\n  hlthsoc_sec = H____ / E_16_, # share of workforce in the human health & social work sector\n  educ_sec = Edctn / E_16_, # share of workforce in the education sector\n  trnsp_sec= Trn__ / E_16_, # share of workforce in the Transport & storage sector\n  accfood_sec = Ac___ / E_16_, # share of workforce in the accommodation & food service sector\n  admsupport_sec = Adm__ /  E_16_, # share of workforce in the administrative & support sector\n  pblic_sec = Pb___ / E_16_ # share of workforce in the public administration & defence sector\n)\n\nLet’s quickly examine how they correlate to our outcome variable i.e. incidence rate of COVID-19 using correlation coefficients and correlograms.\n\n# obtain a matrix of Pearson correlation coefficients\ndf_sel <- st_set_geometry(utla_shp[,37:48], NULL) # temporary data set removing geometries\ncormat <- cor(df_sel, use=\"complete.obs\", method=\"pearson\")\n\n# significance test\nsig1 <- corrplot::cor.mtest(df_sel, conf.level = .95)\n\n# creta a correlogram\ncorrplot::corrplot(cormat, type=\"lower\",\n                   method = \"circle\", \n                   order = \"original\", \n                   tl.cex = 0.7,\n                   p.mat = sig1$p, sig.level = .05, \n                   col = viridis::viridis(100, option = \"plasma\"),\n                   diag = FALSE)\n\n\n\n\nThe correlogram shows the strength and significance of the linear relationship between our set of variables. The size of the circle reflects the strength of the relationships as captured by the Pearson correlation coefficient, and crosses indicate statistically insignificant relationships at the 95% level of confidence. The colour indicate the direction of the relationship with dark (light) colours indicating a negative (positive) association.\nThe results indicate that the incidence of COVID-19 is significantly and positively related to the share of overcrowded housing, nonwhite ethnic minorities and administrative & support workers. Against expectations, the incidence of COVID-19 appears to be negatively correlated with the share of elderly population, of population suffering from long-term illness and of administrative & support workers, and displays no significant association with the share of the population living in deprived areas as well as the share of public administration & defence workers, and health & social workers. The latter probably reflects the effectiveness of the protective measures undertaken to prevent infection among these population groups, but it may also reflect the partial coverage of COVID-19 testing and underreporting. It may also reveal the descriptive limitations of correlation coefficients as they show the relationship between a pairs of variables, not controlling for others. Correlation coefficients can thus produce spurious relationships resulting from confounded variables. We will return to this point below.\nThe results also reveal high collinearity between particular pairs of variables, notably between the share of crowded housing and of nonwhite ethnic population, the share of crowded housing and of elderly population, the share of overcrowded housing and of administrative & support workers, the share of elderly population and of population suffering from long-term illness. A more refined analysis of multicollinearity is needed. Various diagnostics for multicollinearity in a regression framework exist, including matrix condition numbers (CNs), predictor variance inflation factors (VIFs) and variance decomposition factors (VDPs). Rules of thumb (CNs > 30, VIFs > 10 and VDPs > 0.5) to indicate worrying levels of collinearity can be found in Belsley, Kuh, and Welsch (2005). To avoid problems of multicollinearity, often a simple strategy is to remove highly correlated predictors. The difficultly is in deciding which predictor(s) to remove, especially when all are considered important. Keep this in mind when specifying your model.\n\nChallenge 2: Analyse the relationship of all the variables executing pairs(df_sel). How accurate would a linear regression be in capturing the relationships for our set of variables?\n\n\n9.5.1 Global Regression Results\nTo gain a better understanding of these relationships, we can regress the incidence rate of COVID-19 on a series of factors capturing differences across areas. To focus on the description of GWR, we keep our analysis simple and study the incidence rate of COVID-19 as a function of the share of nonwhite ethnic population and of population suffering from long-term illness by estimating the following OLS linear regression model:\n\n# attach data\nattach(utla_shp)\n\n# specify a model equation\neq1 <- covid19_r ~ ethnic + lt_illness\nmodel1 <- lm(formula = eq1, data = utla_shp)\n\n# estimates\nsummary(model1)\n\n\nCall:\nlm(formula = eq1, data = utla_shp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-109.234  -38.386   -4.879   29.284  143.786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    63.77      30.13   2.117    0.036 *  \nethnic        271.10      30.65   8.845 2.64e-15 ***\nlt_illness    216.20     151.88   1.424    0.157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51 on 147 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3844 \nF-statistic: 47.52 on 2 and 147 DF,  p-value: < 2.2e-16\n\n\nWe also compute the VIFs for the variables in the model:\n\nvif(model1)\n\n    ethnic lt_illness \n   1.43015    1.43015 \n\n\nThe regression results indicate a positive relationship exists between the share of nonwhite population and an increased risk of COVID-19 infection. A one percentage point increase in the share of nonwhite population returns a 271 rise in the cumulative count of COVID-19 infection per 100,000 people, everything else constant. The results also reveal a positive (albeit statistically insignificant) relationship between the share of population suffering from long-term illness and an increased risk of COVID-19 infection, after controlling for the share of nonwhite population, thereby confirming our suspicion about the limitations of correlation coefficients; that is, once differences in the share of nonwhite population are taken into account, the association between the share of population suffering from long-term illness and an increased risk of COVID-19 infection becomes positive. We also test for multicollinearity. The VIFs are below 10 indicating that multicollinearity is not highly problematic.\nThe \\(R^{2}\\) value for the OLS regression is 0.393 indicating that our model explains only 39% of the variance in the rate of COVID-19 infection. This leaves 71% of the variance unexplained. Some of this unexplained variance can be because we have only included two explanatory variables in our model, but also because the OLS regression model assumes that the relationships in the model are constant over space; that is, it assumes a stationary process. Hence, an OLS regression model is considered to capture global relationships. However, relationships may vary over space. Suppose, for instance, that there are intrinsic behavioural variations across England and that people have adhered more strictly to self-isolation and social distancing measures in some areas than in others, or that ethnic minorities are less exposed to contracting COVID-19 in certain parts of England. If such variations in associations exist over space, our estimated OLS model will be a misspecification of reality because it assumes these relationships to be constant.\nTo better understand this potential misspecification, we investigate the model residuals which show high variability (see below). The distribution is non-random displaying large positive residuals in the metropolitan areas of London, Liverpool, Newcastle (in light colours) and the Lake District and large negative residuals across much of England (in black). This conforms to the spatial pattern of confirmed COVID-19 cases with high concentration in a limited number of metropolitan areas (see above). While our residual map reveals that there is a problem with the OLS model, it does not indicate which, if any, of the parameters in the model might exhibit spatial nonstationarity. A simple way of examining if the relationships being modelled in our global OLS model are likely to be stationary over space would be to estimate separate OLS model for each UTLA in England. But this would require higher resolution i.e. data within UTLA, and we only have one data point per UTLA. -Fotheringham, Brunsdon, and Charlton (2002) (2002, p.40-44) discuss alternative approaches and their limitations.\n\nutla_shp$res_m1 <- residuals(model1)\n\n# map\nlegend_title = expression(\"OLS residuals\")\nmap_utla = tm_shape(utla_shp) +\n  tm_fill(col = \"res_m1\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_utla + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders"
  },
  {
    "objectID": "09-gwr.html#fitting-a-geographically-weighted-regression",
    "href": "09-gwr.html#fitting-a-geographically-weighted-regression",
    "title": "9  Geographically Weighted Regression",
    "section": "9.6 Fitting a Geographically Weighted Regression",
    "text": "9.6 Fitting a Geographically Weighted Regression\nGWR overcomes the limitation of the OLS regression model of generating a global set of estimates. The basic idea behind GWR is to examine the way in which the relationships between a dependent variable and a set of predictors might vary over space. GWR operates by moving a search window from one regression point to the next, working sequentially through all the existing regression points in the dataset. A set of regions is then defined around each regression point and within the search window. A regression model is then fitted to all data contained in each of the identified regions around a regression point, with data points closer to the sample point being weighted more heavily than are those farther away. This process is repeated for all samples points in the dataset. For a data set of 150 observations GWR will fit 150 weighted regression models. The resulting local estimates can then be mapped at the locations of the regression points to view possible variations in the relationships between variables.\nGraphically, GWR involves fitting a spatial kernel to the data as described in the Fig. 1. For a given regression point \\(X\\), the weight (\\(W\\)) of a data point is at a maximum at the location of the regression point. The weight decreases gradually as the distance between two points increases. A regression model is thus calibrated locally by moving the regression point across the area under study. For each location, the data are weighted differently so that the resulting estimates are unique to a particular location.\n\n\n\nFig. 1. GWR with fixed spatial kernel. Source: Fotheringham et al. (2002, 45).\n\n\n\n9.6.1 Fixed or Adaptive Kernel\nA key issue is to decide between two options of spatial kernels: a fixed kernel or an adaptive kernel. Intuitively, a fixed kernel involves using a fixed bandwidth to define a region around all regression points as displayed in Fig. 1. The extent of the kernel is determined by the distance to a given regression point, with the kernel being identical at any point in space. An adaptive kernel involves using varying bandwidth to define a region around regression points as displayed in Fig. 2. The extent of the kernel is determined by the number of nearest neighbours from a given regression point. The kernels have larger bandwidths where the data are sparse.\n\n\n\nFig. 2. GWR with adaptive spatial kernel. Source: Fotheringham et al. (2002, 47).\n\n\n\n\n9.6.2 Optimal Bandwidth\nA second issue is to define the extent of geographical area (i.e. optimal bandwidth) of the spatial kernel. The bandwidth is the distance beyond which a value of zero is assigned to weight observations. Larger bandwidths include a larger number of observations receiving a non-zero weight and more observations are used to fit a local regression.\nTo determine the optimal bandwidth, a cross-validation approach is applied; that is, for a location, a local regression is fitted based on a given bandwidth and used to predict the value of the dependent variable. The resulting predicted value is used to compute the residuals of the model. Residuals are compared using a series of bandwidth and the bandwidth returning the smallest local residuals are selected.\nVariance and Bias Trade off\nChoosing an optimal bandwidth involves a compromise between bias and precision. For example, a larger bandwidth will involve using a larger number of observations to fit a local regression, and hence result in reduced variance (or increased precision) but high bias of estimates. On the other hand, too small bandwidth involves using a very small number of observations resulting in increased variance but small bias. An optimal bandwidth offers a compromise between bias and variance.\n\n\n9.6.3 Shape of Spatial Kernel\nTwo general set of kernel functions can be distinguished: continuous kernels and kernels with compact support. Continuous kernels are used to weight all observations in the study area and includes uniform, Gaussian and Exponential kernel functions. Kernel with compact support are used to assign a nonzero weight to observations within a certain distance and a zero weight beyond it. The shape of the kernel has been reported to cause small changes to resulting estimates (Brunsdon, Fotheringham, and Charlton 1998).\n\n\n9.6.4 Selecting a Bandwidth\nLet’s now implement a GWR model. The first key step is to define the optimal bandwidth. We first illustrate the use of a fixed spatial kernel.\n\n9.6.4.1 Fixed Bandwidth\nCross-validation is used to search for the optimal bandwidth. Recall that this procedure compares the model residuals based on different bandwidths and chooses the optimal solution i.e. the bandwidth returning the smallest model residuals based on a given model specification. A key parameter here is the shape of the geographical weight function (gweight). We set it to be a Gaussian function which is the default. A bi-square function is recommended to reduce computational time. Since we have a simple model, a Gaussian function should not take that long. Note that we set the argument longlat to TRUE and use latitude and longitude for coordinates (coords). When longlat is set to TRUE, distances are measured in kilometres.\n\n# find optimal kernel bandwidth using cross validation\nfbw <- gwr.sel(eq1, \n               data = utla_shp, \n               coords=cbind( long, lat),\n               longlat = TRUE,\n               adapt=FALSE, \n               gweight = gwr.Gauss, \n               verbose = FALSE)\n\n# view selected bandwidth\nfbw\n\n[1] 29.30417\n\n\nThe result indicates that the optimal bandwidth is 39.79 kms. This means that neighbouring UTLAs within a fixed radius of 39.79 kms will be taken to estimate local regressions. To estimate a GWR, we execute the code below in which the optimal bandwidth above is used as an input in the argument bandwidth.\n\n# fit a gwr based on fixed bandwidth\nfb_gwr <- gwr(eq1, \n            data = utla_shp,\n            coords=cbind( long, lat),\n            longlat = TRUE,\n            bandwidth = fbw, \n            gweight = gwr.Gauss,\n            hatmatrix=TRUE, \n            se.fit=TRUE)\n\nfb_gwr\n\nCall:\ngwr(formula = eq1, data = utla_shp, coords = cbind(long, lat), \n    bandwidth = fbw, gweight = gwr.Gauss, hatmatrix = TRUE, longlat = TRUE, \n    se.fit = TRUE)\nKernel function: gwr.Gauss \nFixed bandwidth: 29.30417 \nSummary of GWR coefficient estimates at data points:\n                  Min.   1st Qu.    Median   3rd Qu.      Max.  Global\nX.Intercept.  -187.913   -42.890    93.702   211.685   792.989  63.768\nethnic        -785.938   104.813   194.609   254.717  1078.854 271.096\nlt_illness   -2599.119  -563.128   128.176   690.603  1507.024 216.198\nNumber of data points: 150 \nEffective number of parameters (residual: 2traceS - traceS'S): 57.11019 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 92.88981 \nSigma (residual: 2traceS - traceS'S): 38.34777 \nEffective number of parameters (model: traceS): 44.65744 \nEffective degrees of freedom (model: traceS): 105.3426 \nSigma (model: traceS): 36.00992 \nSigma (ML): 30.17717 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 1580.349 \nAIC (GWR p. 96, eq. 4.22): 1492.465 \nResidual sum of squares: 136599.2 \nQuasi-global R2: 0.7830537 \n\n\nWe will skip the interpretation of the results for now and consider them in the next section. Now, we want to focus on the overall model fit and will map the results of the \\(R^{2}\\) for the estimated local regressions. To do this, we extract the model results stored in a Spatial Data Frame (SDF) and add them to our spatial data frame utla_shp. Note that the Quasi-global \\(R^{2}\\) is very high (0.77) indicating a high in-sample prediction accuracy.\n\n# write gwr output into a data frame\nfb_gwr_out <- as.data.frame(fb_gwr$SDF)\n\nutla_shp$fmb_localR2 <- fb_gwr_out$localR2\n\n# map\n  # Local R2\nlegend_title = expression(\"Fixed: Local R2\")\nmap_fbgwr1 = tm_shape(utla_shp) +\n  tm_fill(col = \"fmb_localR2\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_fbgwr1 + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\n\n\n\nThe map shows very high in-sample model predictions of up to 80% in relatively large UTLAs (i.e. Cornwall, Devon and Cumbria) but poor predictions in Linconshire and small UTLAs in the North West and Yorkshire & The Humber Regions and the Greater London. The spatial distribution of this pattern may reflect a potential problem that arise in the application of GWR with fixed spatial kernels. The use of fixed kernels implies that local regressions for small spatial units may be calibrated on a large number of dissimilar areas, while local regressions for large areas may be calibrated on very few data points, giving rise to estimates with large standard errors. In extreme cases, generating estimates might not be possible due to insufficient variation in small samples. In practice, this issue is relatively common if the number of geographical areas in the dataset is small.\n\n\n9.6.4.2 Adaptive Bandwidth\nTo reduce these problems, adaptive spatial kernels can be used. These kernels adapt in size to variations in the density of the data so that the kernels have larger bandwidths where the data are sparse and have smaller bandwidths where the data are plentiful. As above, we first need to search for the optimal bandwidth before estimating a GWR.\n\n# find optimal kernel bandwidth using cross validation\nabw <- gwr.sel(eq1, \n               data = utla_shp, \n               coords=cbind( long, lat),\n               longlat = TRUE,\n               adapt = TRUE, \n               gweight = gwr.Gauss, \n               verbose = FALSE)\n\n# view selected bandwidth\nabw\n\n[1] 0.03126972\n\n\nThe optimal bandwidth is 0.03 indicating the proportion of observations (or k-nearest neighbours) to be included in the weighting scheme. In this example, the optimal bandwidth indicates that for a given UTLA, 3% of its nearest neighbours should be used to calibrate the relevant local regression; that is about 5 UTLAs. The search window will thus be variable in size depending on the extent of UTLAs. Note that here the optimal bandwidth is defined based on a data point’s k-nearest neighbours. It can also be defined by geographical distance as done above for the fixed spatial kernel. We next fit a GWR based on an adaptive bandwidth.\n\n# fit a gwr based on adaptive bandwidth\nab_gwr <- gwr(eq1, \n            data = utla_shp,\n            coords=cbind( long, lat),\n            longlat = TRUE,\n            adapt = abw, \n            gweight = gwr.Gauss,\n            hatmatrix=TRUE, \n            se.fit=TRUE)\n\nab_gwr\n\nCall:\ngwr(formula = eq1, data = utla_shp, coords = cbind(long, lat), \n    gweight = gwr.Gauss, adapt = abw, hatmatrix = TRUE, longlat = TRUE, \n    se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.03126972 (about 4 of 150 data points)\nSummary of GWR coefficient estimates at data points:\n                  Min.   1st Qu.    Median   3rd Qu.      Max.  Global\nX.Intercept.  -198.790   -28.398   113.961   226.437   346.510  63.768\nethnic        -121.872   106.822   229.591   283.739  1162.123 271.096\nlt_illness   -1907.098  -746.468  -125.855   798.875  1496.549 216.198\nNumber of data points: 150 \nEffective number of parameters (residual: 2traceS - traceS'S): 48.59361 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 101.4064 \nSigma (residual: 2traceS - traceS'S): 36.57493 \nEffective number of parameters (model: traceS): 36.04378 \nEffective degrees of freedom (model: traceS): 113.9562 \nSigma (model: traceS): 34.50222 \nSigma (ML): 30.07257 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 1546.029 \nAIC (GWR p. 96, eq. 4.22): 1482.809 \nResidual sum of squares: 135653.9 \nQuasi-global R2: 0.7845551 \n\n\n\n\n\n9.6.5 Model fit\nAssessing the global fit of the model, marginal improvements are observed. The \\(AIC\\) and Residual sum of squares experienced marginal reductions, while the \\(R^{2}\\) increased compared to the GRW based on a fixed kernel. To gain a better understanding of these changes, as above, we map the \\(R^{2}\\) values for the estimated local regressions.\n\n# write gwr output into a data frame\nab_gwr_out <- as.data.frame(ab_gwr$SDF)\n\nutla_shp$amb_ethnic <- ab_gwr_out$ethnic\nutla_shp$amb_lt_illness <- ab_gwr_out$lt_illness\nutla_shp$amb_localR2 <- ab_gwr_out$localR2\n\n# map\n  # Local R2\nlegend_title = expression(\"Adaptive: Local R2\")\nmap_abgwr1 = tm_shape(utla_shp) +\n  tm_fill(col = \"amb_localR2\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_abgwr1 + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\n\n\n\nThe map reveals notable improvements in local estimates for UTLAs within West and East Midlands, the South East, South West and East of England. Estimates are still poor in hot spot UTLAs concentrating confirmed cases of COVID-19, such as the Greater London, Liverpool and Newcastle areas.\n\n\n9.6.6 Interpretation\nThe key strength of GWR models is in identifying patterns of spatial variation in the associations between pairs of variables. The results reveal how these coefficients vary across the 150 UTLAs of England. To examine this variability, let’s first focus on the adaptive GWR output reported in Section 8.6.4.2. The output includes a summary of GWR coefficient estimates at various data points. The last column reports the global estimates which are the same as the coefficients from the OLS regression we fitted at the start of our analysis. For our variable nonwhite ethnic population, the GWR outputs reveals that local coefficients range from a minimum value of -148.41 to a maximum value of 1076.84, indicating that one percentage point increase in the share of nonwhite ethnic population is associated with a a reduction of 148.41 in the number of cumulative confirmed cases of COVID-19 per 100,000 people in some UTLAs and an increase of 1076.84 in others. For half of the UTLAs in the dataset, as the share of nonwhite ethnic population increases by one percentage point, the rate of COVID-19 will increase between 106.29 and 291.24 cases; that is, the inter-quartile range between the 1st Qu and the 3rd Qu. To analyse the spatial structure, we next map the estimated coefficients obtained from the adaptive kernel GWR.\n\n  # Ethnic\nlegend_title = expression(\"Ethnic\")\nmap_abgwr2 = tm_shape(utla_shp) +\n  tm_fill(col = \"amb_ethnic\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_abgwr2 = map_abgwr2 + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\n  # Long-term Illness\nlegend_title = expression(\"Long-term illness\")\nmap_abgwr3 = tm_shape(utla_shp) +\n  tm_fill(col = \"amb_lt_illness\", title = legend_title, palette = magma(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\") # change background colour\nmap_abgwr3 = map_abgwr3 + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\ntmap_arrange(map_abgwr2, map_abgwr3)\n\n\n\n\nAnalysing the map for long-term illness, a clear North-South divide can be identified. In the North we observed the expected positive relationship between COVID-19 and long-term illness i.e. as the share of the local population suffering from long-term illness rises, the cumulative number of positive COVID-19 cases is expected to increase. In the South, we observe the inverse pattern i.e. as the share of local population suffering from long-term illness rises, the cumulative number of positive COVID-19 cases is expected to drop. This pattern is counterintuitive but may be explained by the wider socio-economic disadvantages between the North and the South of England. The North is usually characterised by a persistent concentration of more disadvantaged neighbourhoods than the South where affluent households have tended to cluster for the last 40 years (Patias, Rowe, and Arribas-Bel 2021).\n\n\n9.6.7 Assessing statistical significance\nWhile the maps above offer valuable insights to understand the spatial pattering of relationships, they do not identify whether these associations are statistically significant. They may not be. Roughly, if a coefficient estimate has an absolute value of t greater than 1.96 and the sample is sufficiently large, then it is statistically significant. Our sample has only 150 observations, so we are more conservative and considered a coefficient to be statistically significant if it has an absolute value of t larger than 2. Note also that p-values could be computed - see Lu et al. (2014).\n\n# compute t statistic\nutla_shp$t_ethnic = ab_gwr_out$ethnic / ab_gwr_out$ethnic_se\n\n# categorise t values\nutla_shp$t_ethnic_cat <- cut(utla_shp$t_ethnic,\n                     breaks=c(min(utla_shp$t_ethnic), -2, 2, max(utla_shp$t_ethnic)),\n                     labels=c(\"sig\",\"nonsig\", \"sig\"))\n\n# map statistically significant coefs for ethnic\nlegend_title = expression(\"Ethnic: significant\")\nmap_sig = tm_shape(utla_shp) + \n  tm_fill(col = \"t_ethnic_cat\", title = legend_title, legend.hist = TRUE, midpoint = NA, textNA = \"\", colorNA = \"white\") +  # add fill\n  tm_borders(col = \"white\", lwd = .1)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 5) + # add compass\n  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c(\"center\", \"bottom\")) + # add scale bar\n  tm_layout(bg.color = \"white\", legend.outside = TRUE) # change background colour & place legend outside\n\nmap_sig + tm_shape(reg_shp) + # add region boundaries\n  tm_borders(col = \"white\", lwd = .5) # add borders\n\n\n\n# utla count\ntable(utla_shp$t_ethnic_cat)\n\n\n   sig nonsig \n   105     45 \n\n\nFor the share of nonwhite population, 67% of all local coefficients are statistically significant and these are largely in the South of England. Coefficients in the North tend to be insignificant. Through outliers exist in both regions. In the South, nonsignificant coefficients are observed in the metropolitan areas of London, Birmingham and Nottingham, while significant coefficients exist in the areas of Newcastle and Middlesbrough in the North.\n\nChallenge 3 Compute the t values for the intercept and estimated coefficient for long-term illness and create maps of their statistical significance. How many UTLAs report statistically significant coefficients?\n\n\n\n9.6.8 Collinearity in GWR\nAn important final note is: collinearity tends to be problematic in GWR models. It can be present in the data subsets to estimate local coefficients even when not observed globally Wheeler and Tiefelsdorf (2005). Collinearity can be highly problematic in the case of compositional, categorical and ordinal predictors, and may result in exact local collinearity making the search for an optimal bandwidth impossible. A recent paper suggests potential ways forward (Comber et al. 2022)."
  },
  {
    "objectID": "09-gwr.html#questions",
    "href": "09-gwr.html#questions",
    "title": "9  Geographically Weighted Regression",
    "section": "9.7 Questions",
    "text": "9.7 Questions\nWe will continue to use the COVID-19 dataset. Please see Chapter 11 for details on the data.\n\nsdf <- st_read(\"data/assignment_2_covid/covid19_eng.gpkg\")\n\nReading layer `covid19_eng' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_2_covid/covid19_eng.gpkg' \n  using driver `GPKG'\nSimple feature collection with 149 features and 507 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n\nUsing these data, you are required to address the following challenges:\n\nFit a GWR model using a fixed and an adaptive bandwidth.\nCreate a multiple map figure to analyse the spatial variation of coefficients.\n\nAnalyse and discuss:\n\nHow regression coefficients vary across space. Do they vary in size and statistical significance?\nWhat is the appropriate bandwidth for your GWR? Why?\n\n\n\n\n\nBelsley, David A, Edwin Kuh, and Roy E Welsch. 2005. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Vol. 571. John Wiley & Sons.\n\n\nBrunsdon, Chris, Stewart Fotheringham, and Martin Charlton. 1998. “Geographically Weighted Regression.” Journal of the Royal Statistical Society: Series D (The Statistician) 47 (3): 431–43.\n\n\nComber, Alexis, Christopher Brunsdon, Martin Charlton, Guanpeng Dong, Richard Harris, Binbin Lu, Yihe Lü, et al. 2022. “A Route Map for Successful Applications of Geographically Weighted Regression.” Geographical Analysis 55 (1): 155–78. https://doi.org/10.1111/gean.12316.\n\n\nFotheringham, Stewart, Chris Brunsdon, and Martin Charlton. 2002. Geographically Weighted Regression. John Wiley & Sons.\n\n\nLu, Binbin, Paul Harris, Martin Charlton, and Chris Brunsdon. 2014. “The GWmodel r Package: Further Topics for Exploring Spatial Heterogeneity Using Geographically Weighted Models.” Geo-Spatial Information Science 17 (2): 85–101.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021. “Trajectories of Neighbourhood Inequality in Britain: Unpacking Inter-Regional Socioeconomic Imbalances, 1971-2011.” The Geographical Journal 188 (2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nWheeler, David, and Michael Tiefelsdorf. 2005. “Multicollinearity and Correlation Among Local Regression Coefficients in Geographically Weighted Regression.” Journal of Geographical Systems 7 (2): 161–87."
  },
  {
    "objectID": "10-st_analysis.html#dependencies",
    "href": "10-st_analysis.html#dependencies",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.1 Dependencies",
    "text": "10.1 Dependencies\nThis chapter uses the following libraries:\n\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Nice colour schemes\nlibrary(viridis) \n# Obtain correlation coefficients\nlibrary(corrplot)\n# Highlight data on plots\nlibrary(gghighlight)\n# Analysing spatio-temporal data\n#library(STRbook)\nlibrary(spacetime)\n# Date parsing and manipulation\nlibrary(lubridate)\n# Applied statistics\nlibrary(MASS)\n# Statistical tests for linear regression models\nlibrary(lmtest)\n# Fit spatial random effects models\nlibrary(FRK)\n# Exportable regression tables\nlibrary(jtools)"
  },
  {
    "objectID": "10-st_analysis.html#data",
    "href": "10-st_analysis.html#data",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.2 Data",
    "text": "10.2 Data\nFor this chapter, we will use data on:\n\nCOVID-19 confirmed cases from 30th January, 2020 to 21st April, 2020 from Public Health England via the GOV.UK dashboard;\nresident population characteristics from the 2011 census, available from the Office of National Statistics; and,\n2019 Index of Multiple Deprivation (IMD) data from GOV.UK and published by the Ministry of Housing, Communities & Local Government. The data are at the ONS Upper Tier Local Authority (UTLA) level - also known as Counties and Unitary Authorities.\n\nFor a full list of the variables included in the data sets used in this chapter, see the readme file in the sta data folder.1. Before we get our hands on the data, there are some important concepts that need to be introduced. They provide a useful framework to understand the complex structure of spatio-temporal data. Let’s start by first highlighling the importance of spatio-temporal analysis."
  },
  {
    "objectID": "10-st_analysis.html#why-spatio-temporal-analysis",
    "href": "10-st_analysis.html#why-spatio-temporal-analysis",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.3 Why Spatio-Temporal Analysis?",
    "text": "10.3 Why Spatio-Temporal Analysis?\nInvestigating the spatial patterns of human processes as we have done so far in this book only offers a partial incomplete representation of these processes. It does not allow understanding of the temporal evolution of these processes. Human processes evolve in space and time. Human mobility is a inherent geographical process which changes over the course of the day, with peaks at rush hours and high concentration towards employment, education and retail centres. Exposure to air pollution changes with local climatic conditions, and emission and concentration of atmospheric pollutants which fluctuate over time. The rate of disease spread varies over space and may significantly change over time as we have seen during the current outbreak, with flattened or rapid declining trends in Australia, New Zealand and South Korea but fast proliferation in the United Kingdom and the United States. Only by considering time and space together we can address how geographic entities change over time and why they change. A large part of how and why of such change occurs is due to interactions across space and time, and multiple processes. It is essential to understand the past to inform our understanding of the present and make predictions about the future.\n\n10.3.1 Spatio-temporal Data Structures\nA first key element is to understand the structure of spatio-temporal data. Spatio-temporal data incorporate two dimensions. At one end, we have the temporal dimension. In quantitative analysis, time-series data are used to capture geographical processes at regular or irregular intervals; that is, in a continuous (daily) or discrete (only when a event occurs) temporal scale. At another end, we have the spatial dimension. We often use spatial data as temporal aggregations or temporally frozen states (or ‘snapshots’) of a geographical process - this is what we have done so far. Recall that spatial data can be capture in different geographical units, such as areal or lattice, points, flows or trajectories - refer to the introductory lecture in Week 1. Relatively few ways exist to formally integrate temporal and spatial data in consistent analytical framework. Two notable exceptions in R are the packages TraMiner (Gabadinho et al. 2009) and spacetime (Pebesma et al. 2012). We use the class definitions defined in the R package spacetime. These classes extend those used for spatial data in sp and time-series data in xts. Next a brief introduction to concepts that facilitate thinking about spatio-temporal data structures.\n\n10.3.1.1 Type of Table\nSpatio-temporal data can be conceptualised as three main different types of tables:\n\ntime-wide: a table in which columns correspond to different time points\nspace-wide: a table in which columns correspond to different spatial location\nlong formats: a table in which each row-column pair corresponds to a specific time and spatial location (or space coordinate)\n\n\nNote that data in long format are space inefficient because spatial coordinates and time attributes are required for each data point. Yet, data in this format are relatively easy to manipulate via packages such as dplyr and tidyr, and visualise using ggplot2. These packages are designed to work with data in long format.\n\n\n\n10.3.1.2 Type of Spatio-Temporal Object\nTo integrate spatio-temporal data, spatio-temporal objects are needed. We consider four different spatio-temporal frames (STFs) or objects which can be defined via the package spacetime:\n\nFull grid (STF): an object containing data on all possible locations in all time points in a sequence of data;\nSparse grid (STS): an object similar to STF but only containing non-missing space-time data combinations;\nIrregular (STI): an object with an irregular space-time data structure, where each point is allocated a spatial coordinate and a time stamp;\nSimple Trajectories (STT): an object containig a sequence of space-time points that form trajectories.\n\nMore details on these spatio-temporal structures, construction and manipulation, see Pebesma et al. (2012). Enough theory, let’s code!"
  },
  {
    "objectID": "10-st_analysis.html#data-wrangling",
    "href": "10-st_analysis.html#data-wrangling",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.4 Data Wrangling",
    "text": "10.4 Data Wrangling\nThis section illustrates the complexities of handling spatio-temporal data. It discusses good practices in data manipulation and construction of a Space Time Irregular Data Frame (STIDF) object. Three key requirements to define a STFDF object are:\n\nHave a data frame in long format i.e. a location-time pair data frame\nDefine a time stamp\nConstruct the spatio-temporal object of class STIDF by indicating the spatial and temporal coordinates\n\nLet’s now read all the required data. While we can have all data in a single data frame, you will find helpful to have separate data objects to identify:\n\nspatial locations\ntemporal units\ndata\n\nThese data objects correspond to locs, time, and covid19 and censusimd below. Throughout the chapter you will notice that we switch between the various data frames when convinient, depending on the operation.\n\n# clear workspace\nrm(list=ls())\n\n# read ONS UTLA shapefile\nutla_shp <- st_read(\"data/sta/ons_utla.shp\") \n\nReading layer `ons_utla' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/sta/ons_utla.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 150 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: Transverse_Mercator\n\n# create table of locations\nlocs <- utla_shp %>% as.data.frame() %>%\n  dplyr::select(objct, cty19c, ctyu19nm, long, lat, st_rs) \n\n# read time data frame\ntime <- read_csv(\"data/sta/reporting_dates.csv\")\n\n# read COVID-19 data in long format\ncovid19 <- read_csv(\"data/sta/covid19_cases.csv\")\n\n# read census and IMD data\ncensusimd <- read_csv(\"data/sta/2011census_2019imd_utla.csv\")\n\nIf we explore the structure of the data via head and str, we can see we have data on daily and cumulative new COVID-19 cases for 150 spatial units (i.e. UTLAs) over 71 time points from January 30th to April 21st. We also have census and IMD data for a range of attributes.\n\nhead(covid19, 3)\n\n# A tibble: 3 × 6\n  Area.name            Area.code Area.type            date       Daily…¹ Cumul…²\n  <chr>                <chr>     <chr>                <date>       <dbl>   <dbl>\n1 Barking and Dagenham E09000002 Upper tier local au… 2020-01-30       0       0\n2 Barnet               E09000003 Upper tier local au… 2020-01-30       0       0\n3 Barnsley             E08000016 Upper tier local au… 2020-01-30       0       0\n# … with abbreviated variable names ¹​Daily.lab.confirmed.cases,\n#   ²​Cumulative.lab.confirmed.cases\n\n\nOnce we have understood the structure of the data, we first need to confirm if the covid19 data are in wide or long format. Luckily they are in long format; otherwise, we would have needed to transform the data from wide to long format. Useful functions to achieve this include pivot_longer (pivot_longer) which has superseded gather (spread) in the tidyr package. Note that the covid19 data frame has 10,650 observations (i.e. rows); that is, 150 UTLAs * 71 daily observations.\nWe then define a regular time stamp for our temporal data. We use the lubridate package to do this. A key advantage of lubridate is that it automatically recognises the common separators used when recording dates (“-”, “/”, “.”, and ““). As a result, you only need to focus on specifying the order of the date elements to determine the parsing function applied. Below we check the structure of our time data, define a time stamp and create separate variables for days, weeks, months and year.\n\nNote that working with dates can be a complex task. A good discussion of these complexities is provided here.\n\n\n# check the time structure used for reporting covid cases\nhead(covid19$date, 5)\n\n[1] \"2020-01-30\" \"2020-01-30\" \"2020-01-30\" \"2020-01-30\" \"2020-01-30\"\n\n# parsing data into a time stamp\ncovid19$date <- ymd(covid19$date)\nclass(covid19$date)\n\n[1] \"Date\"\n\n# separate date variable into day,week, month and year variables\ncovid19$day <- day(covid19$date)\ncovid19$week <- week(covid19$date) # week of the year\ncovid19$month <- month(covid19$date)\ncovid19$year <- year(covid19$date)\n\nOnce defined the time stamp, we need to add the spatial information contained in our shapefile to create a spatio-temporal data frame.\n\n# join dfs\ncovid19_spt <- left_join(utla_shp, covid19, by = c(\"ctyu19nm\" = \"Area.name\"))\n\nWe now have all the components to build a spatio-temporal object of class STIDF using STIDF from the spacetime package:\n\n# identifying spatial fields\nspat_part <- as(dplyr::select(covid19_spt, -c(bng_e, bng_n, Area.code, Area.type, Daily.lab.confirmed.cases, Cumulative.lab.confirmed.cases, date, day, week, month, year)), Class = \"Spatial\")\n\n# identifying temporal fields\ntemp_part <- covid19_spt$date\n\n# identifying data\ncovid19_data <- covid19_spt %>% dplyr::select(c(Area.code, Area.type, date, Daily.lab.confirmed.cases, Cumulative.lab.confirmed.cases, day, week, month, year)) %>%\n  as.data.frame()\n\n# construct STIDF object\ncovid19_stobj <- STIDF(sp = spat_part, # spatial fields\n                time = temp_part, # time fields\n                data = covid19_data) # data\n                \nclass(covid19_stobj)\n\n[1] \"STIDF\"\nattr(,\"package\")\n[1] \"spacetime\"\n\n\nWe now add census and IMD variables. For the purposes of this Chapter, we only add total population and long-term sick or disabled population counts. You can add more variables by adding their names in the select function.\n\n# select pop data\npop <- censusimd %>% dplyr::select(\"UTLA19NM\", \"Residents\", \"Longterm_sick_or_disabled\")\n# join dfs\ncovid19_spt <- left_join(covid19_spt, pop,\n                         by = c(\"ctyu19nm\" = \"UTLA19NM\"))\ncovid19 <- left_join(covid19, pop, by = c(\"Area.name\" = \"UTLA19NM\"))"
  },
  {
    "objectID": "10-st_analysis.html#exploring-spatio-temporal-data",
    "href": "10-st_analysis.html#exploring-spatio-temporal-data",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.5 Exploring Spatio-Temporal Data",
    "text": "10.5 Exploring Spatio-Temporal Data\nWe now have all the required data in place. In this section various methods of data visualisation are illustrated before key dimensions of the data are explored. Both of these types of exploration can be challenging as one or more dimensions in space and one in time need to be interrogated.\n\n10.5.1 Visualisation\nIn the context spatio-temporal data, a first challenge is data visualization. Visualising more than two dimensions of spatio-temporal data, so it is helpful to slice or aggregate the data over a dimension, use color, or build animations through time. Before exploring the data, we need to define our key variable of interest; that is, the number of confirmed COVID-19 cases per 100,000 people. We also compute the cumulative number of confirmed COVID-19 cases per 100,000 people as it may be handy in some analyses.\nFisrt create variable to be analysed:\n\n# rate of new covid-19 infection\ncovid19_spt$n_covid19_r <- round( (covid19_spt$Daily.lab.confirmed.cases / covid19_spt$Residents) * 100000)\ncovid19$n_covid19_r <- round( (covid19$Daily.lab.confirmed.cases / covid19$Residents) * 100000 )\n\n# risk of cumulative covid-19 infection\ncovid19_spt$c_covid19_r <- round( (covid19_spt$Cumulative.lab.confirmed.cases / covid19_spt$Residents) * 100000)\ncovid19$c_covid19_r <- round( (covid19$Cumulative.lab.confirmed.cases / covid19$Residents) * 100000)\n\n\n10.5.1.1 Spatial Plots\nOne way to visualise the data is using spatial plots; that is, snapshots of a geographic process for a given time period. Data can be mapped in different ways using clorepleth, countour or surface plots. The key aim of these maps is to understand how the overall extent of spatial variation and local patterns of spatial concentration change over time. Below we visualise the weekly number of confirmed COVID-19 cases per 100,000 people.\n\nNote that Weeks range from 5 to 16 as they refer to calendar weeks. Calendar week 5 is when the first COVID-19 case in England was reported.\n\n\n# create data frame for new cases by week\ndaycases_week <- covid19_spt %>% \n  group_by(week, ctyu19nm, \n           as.character(cty19c), \n           Residents) %>%\n  summarise(n_daycases = sum(Daily.lab.confirmed.cases)) \n\n# weekly rate of new covid-19 infection\ndaycases_week$wn_covid19_r <- (daycases_week$n_daycases / daycases_week$Residents) * 100000\n\n# map\nlegend_title = expression(\"Cumulative Cases per 100,000 Population\")\ntm_shape(daycases_week) +\n  tm_fill(\"wn_covid19_r\", title = legend_title, palette = magma(256), style =\"cont\", legend.hist=FALSE, legend.is.portrait=FALSE) +\n  tm_facets(by = \"week\", ncol = 4) +\n  tm_borders(col = \"white\", lwd = .1)  + # add borders +\n  tm_layout(bg.color = \"white\", # change background colour\n            legend.outside = TRUE, # legend outside\n            legend.outside.position = \"bottom\",\n            legend.stack = \"horizontal\",\n            legend.title.size = 2,\n            legend.width = 1,\n            legend.height = 1,\n            panel.label.size = 3,\n            main.title = \"New COVID-19 Cases by Calendar Week, UTLA, England\") \n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend\n\n\n\n\n\nThe series of maps reveal a stable pattern of low reported cases from calendar weeks 5 to 11. From week 12 a number of hot spots emerged, notably in London, Birmingham, Cumbria and subsequently around Liverpool. The intensity of new cases seem to have started to decline from week 15; yet, it is important to note that week 16 display reported cases for only two days.\n\n\n10.5.1.2 Time-Series Plots\nTime-series plots can be used to capture a different dimension of the process in analysis. They can be used to better understand changes in an observation location, an aggregation of observations, or multiple locations simultaneously over time. We plot the cumulative number of COVID-19 cases per 100,000 people for UTLAs reporting over 310 cases. The plots identify the UTLAs in London, Newcastle and Sheffield reporting the largest numbers of COVID-19 cases. The plots also reveal that there has been a steady increase in the number of cases, with some differences. While cases have steadily increase in Brent and Southwark since mid March, the rise has been more sudden in Sunderland. The plots also reveal a possible case of misreporting in Sutton towards the end of the series.\n\ntsp <- ggplot(data = covid19_spt,\n            mapping = aes(x = date, y = c_covid19_r,\n                          group = ctyu19nm))\ntsp + geom_line(color = \"blue\") + \n    gghighlight(max(c_covid19_r) > 310, use_direct_label = FALSE) +\n    labs(title= paste(\" \"), x=\"Date\", y=\"Cumulative Cases per 100,000\") +\n    theme_classic() +\n    theme(plot.title=element_text(size = 20)) +\n    theme(axis.text=element_text(size=16)) +\n    theme(axis.title.y = element_text(size = 18)) +\n    theme(axis.title.x = element_text(size = 18)) +\n    theme(plot.subtitle=element_text(size = 16)) +\n    theme(axis.title=element_text(size=20, face=\"plain\")) +\n    facet_wrap(~ ctyu19nm)\n\n\n\n\n\n\n10.5.1.3 Hovmöller Plots\nAn alternative visualisation is a Hovmöller plot - sometimes known as heatmap. It is a two-dimensional space-time representation in which space is collapsed onto one dimension against time. Hovmöller plots can easily be generated if the data are arranged on a space-time grid; however, this is rarely the case. Luckily we have ggplot! which can do magic rearranging the data as needed. Below we produce a Hovmöller plot for UTLAs with resident populations over 260,000. The plot makes clear that the critical period of COVID-19 spread has been during April despite the implementation of a series of social distancing measures by the government.\n\nggplot(data = dplyr::filter(covid19_spt, Residents > 260000), \n           mapping = aes(x= date, y= reorder(ctyu19nm, c_covid19_r), fill= c_covid19_r)) +\n  geom_tile() +\n  scale_fill_viridis(name=\"New Cases per 100,000\", option =\"plasma\", begin = 0, end = 1, direction = 1) +\n  theme_minimal() + \n  labs(title= paste(\" \"), x=\"Date\", y=\"Upper Tier Authority Area\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_text(size=15)) +\n  theme(axis.text.y = element_text(size=10)) +\n  theme(axis.text.x = element_text(size=15)) +\n  theme(axis.title=element_text(size=20, face=\"plain\")) +\n  theme(legend.key.width = unit(5, \"cm\"), legend.key.height = unit(2, \"cm\"))\n\n\n\n\n\n\n10.5.1.4 Interactive Plots\nInteractive visualisations comprise very effective ways to understand spatio-temporal data and they are now fairly accessible. Interactive visualisations allow for a more data-immersive experience, and enable exploration of the data without having to resort to scripting. Here is when the use of tmap shines as it does not only enables easily creating nice static maps but also interactive maps! Below an interactive map for a time snapshot of the data (i.e. 2020-04-14) is produced, but with a bit of work layers can be added to display multiple temporal slices of the data.\n\n# map\nlegend_title = expression(\"Cumulative Cases per 100,000 Population\")\nimap = tm_shape(dplyr::filter(covid19_spt[,c(\"ctyu19nm\", \"date\", \"c_covid19_r\")], as.character(date) == \"2020-04-14\"), labels = \"Area.name\") +\n  tm_fill(\"c_covid19_r\", title = legend_title, palette = magma(256), style =\"cont\", legend.is.portrait=FALSE, alpha = 0.7) +\n  tm_borders(col = \"white\") +\n  #tm_text(\"ctyu19nm\", size = .4) +\n  tm_layout(bg.color = \"white\", # change background colour\n            legend.outside = TRUE, # legend outside\n            legend.title.size = 1,\n            legend.width = 1) \n\nTo view the map on your local machines, execute the code chunk below removing the # sign.\n\n#tmap_mode(\"view\")\n#imap\n\nAlternative data visualisation tools are animations, telliscope and shiny. Animations can be constructed by plotting spatial data frame-by-frame, and then stringing them together in sequence. A useful R packages gganimate and tmap! See Lovelace, Nowosad, and Muenchow (2019). Note that the creation of animations may require external dependencies; hence, they have been included here. Both telliscope and shiny are useful ways for visualising large spatio-temporal data sets in an interactive ways. Some effort is required to deploy these tools.\n\n\n\n10.5.2 Exploratory Analysis\nIn addition to visualising data, we often want to obtain numerical summaries of the data. Again, innovative ways to reduce the inherent dimensionality of the data and examine dependence structures and potential relationships in time and space are needed. We consider visualisations of empirical spatial and temporal means, dependence structures and some basic time-series analysis.\n\n10.5.2.1 Means\nEmpirical Spatial Mean\nThe empirical spatial mean for a data set can be obtained by averaging over time points for one location. In our case, we can compute the empirical spatial mean by averaging the daily rate of new COVID-19 cases for UTLAs between January 30th and April 21st. It reveals that Brent, Southwark and Sunderland report an average daily infection rate of over 5 new cases per 100,000 people, whereas Rutland and Isle of Wight display an average of less than 1.\n\n# compute empirical spatial mean\nsp_av <- covid19_spt %>% group_by(ctyu19nm) %>% # group by spatial unit\n  summarise(sp_mu_emp = mean(n_covid19_r))\n\n# plot empirical spatial mean\nggplot(data=sp_av) +\n  geom_col( aes( y = reorder(ctyu19nm, sp_mu_emp), x = sp_mu_emp) , fill = \"grey50\") +\n  theme_classic() +\n  labs(title= paste(\" \"), x=\"Average New Cases per 100,000\", y=\"Upper Tier Authority Area\") +\n  theme(legend.position = \"bottom\") +\n  theme(axis.text.y = element_text(size=7)) +\n  theme(axis.text.x = element_text(size=12)) +\n  theme(axis.title=element_text(size=20, face=\"plain\"))\n\n\n\n\nEmpirical Temporal Mean\nThe empirical temporal mean for a data set can be obtained by averaging across spatial locations for a time point. In our case, we can compute the empirical temporal mean by averaging the rate of new COVID-19 cases over UTLAs by day. The empirical temporal mean is plotted below revealing a peak of 8.32 number of new cases per 100,000 people the 7th of April, steadily decreasing to 0.35 for the last reporting observation in our data; that is, April 21st.\n\nNote the empirical temporal mean is smoothed via local polynomial regression fitting; hence below zero values are reported between February and March.\n\n\n# compute temporal mean\ntm_av <- covid19 %>% group_by(date) %>%\n  summarise(tm_mu_emp = mean(n_covid19_r))\n\n# plot temporal mean + trends for all spatial units\nggplot() +\n  geom_line(data = covid19, mapping = aes(x =date, y = n_covid19_r,\n                          group = Area.name), color = \"gray80\") +\n   theme_classic() +\n  geom_smooth(data = tm_av, mapping = aes(x =date, y = tm_mu_emp), \n              alpha = 0.5,\n              se = FALSE) +\n    labs(title= paste(\" \"), x=\"Date\", y=\"Cumulative Cases per 100,000\") +\n    theme_classic() +\n    theme(plot.title=element_text(size = 18)) +\n    theme(axis.text=element_text(size=14)) +\n    theme(axis.title.y = element_text(size = 16)) +\n    theme(axis.title.x = element_text(size = 16)) +\n    theme(plot.subtitle=element_text(size = 16)) +\n    theme(axis.title=element_text(size=18, face=\"plain\"))\n\n\n\n\n\n\n10.5.2.2 Dependence\nSpatial Dependence\nAs we know spatial dependence refers to the spatial relationship of a variable’s values for a pairs of locations at a certain distance apart, so that are more similar (or less similar) than expected for randomly associated pairs of observations. Patterns of spatial dependence may change over time. In the case of a disease outbreak patterns of spatial dependence can change very quickly as new cases emerge and social distancing measures are implemented. Chapter 6 illustrates how to measure spatial dependence in the context of spatial data.\n\nChallenge 1: Measure how spatial dependence change over time. Hint: compute the Moran’s I on the rate of new COVID-19 cases (i.e. n_covid19_r in the covid19 data frame) at multiple time points.\n\n\nNote: recall that the problem of ignoring the dependence in the errors when doing OLS regression is that the resulting standard errors and prediction standard errors are inappropriate. In the case of positive dependence, which is the most common case in spatio-temporal data (recall Tobler’s law), the standard errors and prediction standard errors are underestimated. This is if dependence is ignored, resulting in a false sense of how good the estimates and predictions really are.\n\nTemporal Dependence\nAs for spatial data, dependence can also exists in temporal data. Temporal dependence or temporal autocorrelation exists when a variable’s value at time \\(t\\) is dependent on its value(s) at \\(t-1\\). More recent observations are often expected to have a greater influence on present observations. A key difference between temporal and spatial dependence is that temporal dependence is unidirectional (i.e. past observations can only affect present or future observations but not inversely), while spatial dependence is multidirectional (i.e. an observation in a spatial unit can influence and be influenced by observations in multiple spatial units).\nBefore measuring the temporal dependence is our time-series, a time-series object needs to be created with a time stamp and given cycle frequency. A cycle frequency refers to when a seasonal pattern is repeated. We consider a time series of the total number of new COVID-19 cases per 100,000 (i.e. we sum cases over UTLAs by day) and the frequency set to 7 to reflect weekly cycles. So we end up with a data frame of length 71.\n\n# create a time series object\ntotal_cnt <- covid19 %>% group_by(date) %>%\n  summarise(new_cases = sum(n_covid19_r)) \ntotal_cases_ts <- ts(total_cnt$new_cases, \n                     start = 1,\n                     frequency =7)\n\nThere are various ways to test for temporal autocorrelation. An easy way is to compute the correlation coefficient between a time series measured at time \\(t\\) and its lag measured at time \\(t-1\\). Below we measure the temporal autocorrelation in the rate of new COVID-19 cases per 100,000 people. A correlation of 0.97 is returned indicating high positive autocorrelation; that is, high (low) past numbers of new COVID-19 cases per 100,000 people tend to correlate with higher (lower) present numbers of new COVID-19 cases. The Durbin-Watson test is often used to test for autocorrelation in regression models.\n\n# create lag term t-1\nlag_new_cases <- total_cnt$new_cases[-1]\ntotal_cnt <- cbind(total_cnt[1:70,], lag_new_cases)\ncor(total_cnt[,2:3])\n\n              new_cases lag_new_cases\nnew_cases      1.000000      0.974284\nlag_new_cases  0.974284      1.000000\n\n\nTime Series Components\nIn addition to temporal autocorrelation, critical to the analysis of time-series are its constituent components. A time-series is generally defined by three key components:\n\nTrend: A trend exists when there is a long-term increase or decrease in the data.\nSeasonal: A seasonal pattern exists when a time series is affected by seasonal factors and is of a fixed and known frequency. Seasonal cycles can occur at various time intervals such as the time of the day or the time of the year.\nCyclic (random): A cycle exists when the data exhibit rises and falls that are not of a fixed frequency.\n\nTo understand and model a time series, these components need to be identified and appropriately incorporated into a regression model. We illustrate these components by decomposing our time series for total COVID-19 cases below. The top plot shows the observed data. Subsequent plots display the trend, seasonal and random components of the total number of COVID-19 cases on a weekly periodicity. They reveal a clear inverted U-shape trend and seasonal pattern. This idea that we can decompose data to extract information and understand temporal processes is key to understand the concept of basis functions to model spatio-temporal data, which will be introduced in the next section.\n\n# decompose time series\ndec_ts <- decompose(total_cases_ts)\n# plot time series components\nplot(dec_ts)\n\n\n\n\nFor a good introduction to time-series analysis in R, refer to Hyndman and Athanasopoulos (2018) and DataCamp."
  },
  {
    "objectID": "10-st_analysis.html#spatio-temporal-data-modelling",
    "href": "10-st_analysis.html#spatio-temporal-data-modelling",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.6 Spatio-Temporal Data Modelling",
    "text": "10.6 Spatio-Temporal Data Modelling\nHaving some understanding of the spatio-temporal patterns of COVID-19 spread through data exploration, we are ready to start further examining structural relationships between the rate of new infections and local contextual factors via regression modelling across UTLAs. Specifically we consider the number of new cases per 100,000 people to capture the rate of new infections and only one contextual factor; that is, the share of population suffering from long-term sickness or disabled. We will consider some basic statistical models, of the form of linear regression and generalized linear models, to account for spatio-temporal dependencies in the data. Note that we do not consider more complex structures based on hierarchical models or spatio-temporal weighted regression models which would be the natural step moving forward.\nAs any modelling approach, spatio-temporal statistical modelling has three principal goals:\n\npredicting values of a given outcome variable at some location in space within the time span of the observations and offering information about the uncertainty of those predictions;\nperforming statistical inference about the influence of predictors on an outcome variable in the presence of spatio-temporal dependence; and,\nforecasting future values of an outcome variable at some location, offering information about the uncertainty of the forecast.\n\n\n10.6.1 Intuition\nThe key idea on what follows is to use a basic statistical regression model to understand the relationship between the share of new COVID-19 infections and the share of population suffering from long-term illness, accounting for spatio-temporal dependencies. We will consider what is known as a trend-surface regression model which assumes that spatio-temporal dependencies can be accounted for by “trend” components and incorporate as predictors in the model. Formally we consider the regression model below which seeks to account for spatial and temporal trends.\n\\[y(s_{i}, t_{j}) = \\beta_{0} + \\beta_{k}x(s_{i}, t_{j}) + e(s_{i}, t_{j})\\]\nwhere \\(\\beta_{0}\\) is the intercept and \\(\\beta_{k}\\) represents a set of regression coefficients associated with \\(x(s_{i}, t_{j})\\); the \\(k\\) indicates the number of covariates at spatial location \\(s_{i}\\) and time \\(t_{j}\\); \\(e\\) represents the regression errors which are assumed to follow a normal distribution. The key difference to aproaches considered in previous chapters is the incorporation of space and time together. As we learnt from the previous section, this has implications are we now have two sources of dependence: spatial and temporal autocorrelation, as well as seasonal and trend components. This has implications for modelling as we now need to account for all of these components if we are to establish any relationship between \\(y\\) and \\(x\\).\nA key implication is how we consider the set of covariates represented by \\(x\\). Three key types can be identified:\n\nspatial-variant, temporal-invariant covariates: these are attributes which may vary across space but be temporally invariant, such as geographical distances;\nspatial-invariant, temporal-variant covariates: these are attributes which do not vary across space but change over time; and,\nspatial-variant, temporal-variant covariates: these are attributes which vary over both space and time;\n\n\nNote that what is variant or invariant will depend on the spatial and temporal scale of the analysis.\n\nWe can also consider spatio-temporal “basis functions”. Note that this is an important concept for the rest of the Chapter. What are basis functions then? If you think that spatio-temporal data represent a complex set of curves or surfaces in space, basis functions represent the components into which this set of curves can be decomposed. In this sense, basis functions operate in a similar fashion as the decomposition of time series considered above i.e. time series data can be decomposed into a trend, seasonal and random components and their sum can be used to represent the observed temporal trajectory. Basis functions offer an effective way to incorporate spatio-temporal dependencies. Thus, basis functions have the key goal of accounting for spatio-temporal dependencies as spatial weight matrices or temporal lags help accounting spatial autocorrelation in spatial models and temporal autocorrelation in time series analysis.\nAs standard regression coefficients, basis functions are related to \\(y\\) via coefficients (or weights). The difference is that we typically assume that basis functions are known while coefficients are random. Examples of basis functions include polynomials, splines, wavelets, sines and cosines so various linear combinations that can be used to infer potential spatio-temporal dependencies in the data. This is similar to deep learning models in which cases you provide, for example, an image and the model provides a classification of pixels. But you normally do not know what the classification represents (hence they are known as black boxes!) so further analysis on the classification is needed to understand what the model has attempted to capture. Basically basis functions are smoother functions to represent the observed data, and their objective to capture the spatial and temporal variability in the data as well as their dependence.\nFor our application, we start by considering a basic OLS regression model with the following basis functions to account spatial-temporal structures:\n\noverall mean;\nlinear in lon-coordinate;\nlinear in lat-coordinate;\nlinear time daily trend;\nadditional spatio-temporal basis functions which are presented below; and,\n\nThese basis functions are incorporated as independent variables in the regression model. Additionally, we also include the share of population suffering from long-term illness as we know it is highly correlated to the cumulative number of COVID-19 cases. The share of population suffering long-term illness is incorporated as a spatial-variant, temporal-invariant covariates given that rely in 2011 census data.\n\n\n10.6.2 Fitting Spatio-Temporal Models\nAs indicated at the start of this Chapter, we use the FRK framework developed by Cressie and Johannesson (2008). It provides a scalable, relies on the use a spatial random effects model (with which we have some familiarity) and can be easily implemented in R by the use of the FRK package (Zammit-Mangion and Cressie 2017). In this framework, a spatially correlated errors can be decomposed using a linear combination of spatial basis functions, effectively addressing issues of spatial-temporal dependence and nonstationarity. The specification of spatio-temporal basis functions is a key component of the model and they can be generated automatically or by the user via the FRK package. We will use the automatically generated functions. While as we will notice they are difficult to interpret, user generated functions require greater understanding of the spatio-temporal structure of COVID-19 which is beyond the scope of this Chapter.\nPrepare Data\nThe first step to create a data frame with the variables that we will consider for the analysis. We first remove the geometries to convert covid19_spt from a simple feature object to a data frame and then compute the share of long-term illness population.\n\n# remove geometries\nst_geometry(covid19_spt) <- NULL\n\n# share of population in long-term illness \ncovid19_spt <- covid19_spt %>% mutate(\n lt_illness = Longterm_sick_or_disabled / Residents\n)\n\nConstruct Basis Functions\nWe now build the set of basis functions. The can be constructed by using the function auto_basis from the FRK package. The function takes as arguments: data, nres (which is the number of “resolutions” or aggregation to construct); and type of basis function to use. We consider a single resolution of the default Gaussian radial basis function.\n\n# build basis functions\nG <- auto_basis(data = covid19_spt[,c(\"long\",\"lat\")] %>%\n                       SpatialPoints(),           # To sp obj\n                nres = 1,                         # One resolution\n                type = \"Gaussian\")                # Gaussian BFs\n# basis functions evaluated at data locations are then the covariates\nS <- eval_basis(basis = G,                       # basis functions\n                s = covid19_spt[,c(\"long\",\"lat\")] %>%\n                     as.matrix()) %>%            # conv. to matrix\n     as.matrix()                                 # conv. to matrix\ncolnames(S) <- paste0(\"B\", 1:ncol(S)) # assign column names\n\nAdd Basis Functions to Data Frame\nWe then prepare a data frame for the regression model, adding the weights extracted from the basis functions. These weights enter as covariates in our model. Note that the resulting number of basis functions is nine. Explore by executing colnames(S). Below we select only relevant variables for our model.\n\n# selecting variables\nreg_df <- cbind(covid19_spt, S) %>%\n  dplyr::select(ctyu19nm, n_covid19_r, long, lat, date, lt_illness, B1:B9)\n\nFit Linear Regression\nWe now fit a linear model using lm including as covariates longitude, latitude, day, share of long-term ill population and the nine basis functions.\n\nRecall that latitude refers to north/south from the equator and longitude refers to west/east from Greenwich. Further up north means a higher latitude score. Further west means higher longitude score. Scores for Liverpool (53.4084° N, 2.9916° W) are thus higher than for London (51.5074° N, 0.1278° W). This will be helpful for interpretation.\n\n\neq1 <- n_covid19_r ~ long + lat + date + lt_illness + .\nlm_m <- lm(formula = eq1, \n           data = dplyr::select(reg_df, -ctyu19nm))\nlm_m %>% summary()\n\n\nCall:\nlm(formula = eq1, data = dplyr::select(reg_df, -ctyu19nm))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9726 -1.6679 -0.4867  1.1702 22.5346 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.082e+03  2.839e+01 -73.354  < 2e-16 ***\nlong        -1.932e+00  3.620e-01  -5.336  9.7e-08 ***\nlat          3.390e+00  3.266e-01  10.380  < 2e-16 ***\ndate         1.033e-01  1.245e-03  82.958  < 2e-16 ***\nlt_illness   3.276e+01  3.541e+00   9.250  < 2e-16 ***\nB1           7.556e+00  3.157e+00   2.393   0.0167 *  \nB2           1.898e+00  1.409e+00   1.347   0.1780    \nB3           1.750e+01  1.978e+00   8.847  < 2e-16 ***\nB4          -2.022e+00  2.742e+00  -0.737   0.4609    \nB5           2.207e+00  2.233e+00   0.989   0.3229    \nB6          -9.814e-01  2.498e+00  -0.393   0.6945    \nB7          -2.031e-01  3.687e+00  -0.055   0.9561    \nB8          -2.234e+00  2.801e+00  -0.797   0.4252    \nB9           1.877e+00  2.543e+00   0.738   0.4604    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.842 on 10636 degrees of freedom\nMultiple R-squared:  0.4169,    Adjusted R-squared:  0.4162 \nF-statistic:   585 on 13 and 10636 DF,  p-value: < 2.2e-16\n\n\nCoefficients for explicitly specified spatial and temporal variables and the share of long-term ill population are all statistically significant. The interpretation of the regression coefficients is as usual; that is, one unit increase in a covariate relates to one unit increase in the dependent variable. For instance, the coefficient for long-term illness population indicates that UTLAs with a larger share of long-term ill population in one percentage point tend to have 328 more new COVID-19 cases per 100,000 people! on average. The coefficient for date reveals a strong positive temporal dependence with an average increase in the number of new cases per 100,000 people over time. The coefficient for latitude indicates as we move north the number of new COVID-19 cases per 100,000 people tends to be higher but lower if we move west.\nWhile overall the model provides some understanding of the spatio-temporal structure of the spread of COVID-19, the overall fit of the model is relatively poor. The \\(R^{2}\\) reveals that the model explains only 4.2% of the variability of the spread of COVID-19 cases, reflecting the complexity of modelling spatio-temporal processes. Also, except for two, the coefficients associated to the basis functions are statistically insignificant. A key issue that we have ignored so far is the fact that our dependent variable is a count and is highly skewed - refer back to Section [8.4 Exploratory Analysis].\n\nChallenge 2: Explore a model with only spatial components (i.e. long and lat) or only temporal components (day). What model returns the largest \\(R^{2}\\)?\n\nPoisson Regression\nA Poisson regression model provides a more appropriate framework to address these issues. We do this fitting a general linear model (or GLM) specifying the family function to be a Poisson.\n\n# estimate a poisson model\npoisson_m1 <- glm(eq1,\n                family = poisson(\"log\"), # Poisson + log link\n                data = dplyr::select(reg_df, -ctyu19nm))\npoisson_m1 %>% summary()\n\n\nCall:\nglm(formula = eq1, family = poisson(\"log\"), data = dplyr::select(reg_df, \n    -ctyu19nm))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.7454  -1.2043  -0.6993   0.3764   7.8981  \n\nCoefficients:\n              Estimate Std. Error  z value Pr(>|z|)    \n(Intercept) -1.027e+03  8.168e+00 -125.699  < 2e-16 ***\nlong        -8.534e-01  9.080e-02   -9.399  < 2e-16 ***\nlat          1.456e+00  7.617e-02   19.115  < 2e-16 ***\ndate         5.153e-02  3.871e-04  133.121  < 2e-16 ***\nlt_illness   1.166e+01  7.701e-01   15.139  < 2e-16 ***\nB1           3.418e+00  8.005e-01    4.270 1.96e-05 ***\nB2           4.414e-01  3.249e-01    1.359  0.17421    \nB3           8.531e+00  5.480e-01   15.568  < 2e-16 ***\nB4          -7.129e-01  5.865e-01   -1.215  0.22418    \nB5           1.639e+00  5.048e-01    3.246  0.00117 ** \nB6          -1.282e+00  5.618e-01   -2.283  0.02245 *  \nB7          -3.572e-01  8.411e-01   -0.425  0.67102    \nB8          -1.003e+00  6.262e-01   -1.602  0.10917    \nB9           1.655e+00  6.268e-01    2.640  0.00829 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 51245  on 10649  degrees of freedom\nResidual deviance: 24458  on 10636  degrees of freedom\nAIC: 42364\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe Poisson model seems to provide a more appropriate functional forms to identify the strength of the relationship between new COVID-19 cases and spatio-temporal dependencies as captured by a greater number (relative to the linear model) of basis functions coefficients becoming statistically significant. Yet, the Poisson model assumes that the mean and variance of the COVID-19 cases is the same. But, given the distribution of our dependent variable, its variance is likely to be greater than the mean. That means the data exhibit “overdispersion”. How do we know this? An estimate of the dispersion is given by the ratio of the deviance to the total degrees of freedom (the number of data points minus the number of covariates). In this case the dispersion estimate is:\n\npoisson_m1$deviance / poisson_m1$df.residual\n\n[1] 2.29953\n\n\nwhich is clearly greater than 1! i.e. the data are overdispersed.\nQuasipoisson Regression\nAn approach to account for overdispersion is to use quasipoisson when calling glm. The quasi-Poisson model assumes that the variance is proportional to the mean, and that the constant of the proportionality is the over-dispersion parameter. This model corrects the standard error of the estimated coefficients. So only p-values and t values are affected. No changes are recorded in terms of residual deviance (24458) and median of deviance residuals (-0.6993), compared to the standard Poisson model.\n\n# estimate a quasipoisson model\nqpoisson_m1 <- glm(eq1,\n                family = quasipoisson(\"log\"), # QuasiPoisson + log link\n                data = dplyr::select(reg_df, -ctyu19nm))\nqpoisson_m1 %>% summary()\n\n\nCall:\nglm(formula = eq1, family = quasipoisson(\"log\"), data = dplyr::select(reg_df, \n    -ctyu19nm))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.7454  -1.2043  -0.6993   0.3764   7.8981  \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.027e+03  1.215e+01 -84.490  < 2e-16 ***\nlong        -8.534e-01  1.351e-01  -6.318 2.76e-10 ***\nlat          1.456e+00  1.133e-01  12.848  < 2e-16 ***\ndate         5.153e-02  5.759e-04  89.478  < 2e-16 ***\nlt_illness   1.166e+01  1.146e+00  10.176  < 2e-16 ***\nB1           3.418e+00  1.191e+00   2.870  0.00411 ** \nB2           4.414e-01  4.833e-01   0.913  0.36109    \nB3           8.531e+00  8.153e-01  10.464  < 2e-16 ***\nB4          -7.129e-01  8.726e-01  -0.817  0.41395    \nB5           1.639e+00  7.510e-01   2.182  0.02915 *  \nB6          -1.282e+00  8.358e-01  -1.534  0.12499    \nB7          -3.572e-01  1.251e+00  -0.286  0.77526    \nB8          -1.003e+00  9.316e-01  -1.077  0.28162    \nB9           1.655e+00  9.325e-01   1.774  0.07603 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 2.213379)\n\n    Null deviance: 51245  on 10649  degrees of freedom\nResidual deviance: 24458  on 10636  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nNegative Binomial Regression\nAn alternative approach to deal with overdispersion is the Negative Binomial Model (NBM). This models relaxes the assumption of equality between the mean and variance. We estimate a NBM by using the function glm.nb from the MASS package.\n\n# estimate a negative binomial model\nnb_m1 <- glm.nb(eq1, \n       data = dplyr::select(reg_df, -ctyu19nm))\nnb_m1 %>% summary()\n\n\nCall:\nglm.nb(formula = eq1, data = dplyr::select(reg_df, -ctyu19nm), \n    init.theta = 1.493089258, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0655  -0.8110  -0.4119   0.1861   3.1676  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.540e+03  1.596e+01 -96.459  < 2e-16 ***\nlong        -8.402e-01  1.650e-01  -5.090 3.57e-07 ***\nlat          1.604e+00  1.456e-01  11.021  < 2e-16 ***\ndate         7.901e-02  7.522e-04 105.030  < 2e-16 ***\nlt_illness   1.440e+01  1.534e+00   9.387  < 2e-16 ***\nB1           5.121e+00  1.460e+00   3.508 0.000452 ***\nB2           1.622e-01  6.177e-01   0.263 0.792897    \nB3           9.502e+00  9.469e-01  10.035  < 2e-16 ***\nB4          -2.054e+00  1.183e+00  -1.736 0.082590 .  \nB5           2.461e+00  9.802e-01   2.510 0.012059 *  \nB6          -1.095e+00  1.089e+00  -1.005 0.314895    \nB7           6.486e-01  1.642e+00   0.395 0.692877    \nB8          -1.143e+00  1.225e+00  -0.933 0.350789    \nB9           1.068e+00  1.169e+00   0.914 0.360917    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.4931) family taken to be 1)\n\n    Null deviance: 22092.1  on 10649  degrees of freedom\nResidual deviance:  8374.3  on 10636  degrees of freedom\nAIC: 34057\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.4931 \n          Std. Err.:  0.0361 \n\n 2 x log-likelihood:  -34027.2980 \n\n\nThe NBM leads to a significant reduction in residual deviance from 24,458 returned by the Poisson model to only 8,374. It points to a major improvement in explaining the spatio-temporal variability in the spread of COVID-19\nIncluding Interactions\nYet, we may further refine our model based on a different strategy. Let’s try running a NBM including interaction terms between spatial and temporal terms (i.e. longitude, latitude and date). We can do this by estimating the following model c_covid19_r ~ (long + lat + date)^2 + lt_illness + .\n\n# new model specification\neq2 <- n_covid19_r ~ (long + lat + date)^2 + lt_illness + .\n# estimate a negative binomial model\nnb_m2 <- glm.nb(eq2, \n       data = dplyr::select(reg_df, -ctyu19nm))\nnb_m2 %>% summary()\n\n\nCall:\nglm.nb(formula = eq2, data = dplyr::select(reg_df, -ctyu19nm), \n    init.theta = 1.56089592, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.1506  -0.8099  -0.4039   0.2036   3.4084  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  4.797e+03  6.955e+02   6.897 5.32e-12 ***\nlong        -4.509e+01  1.559e+01  -2.892  0.00382 ** \nlat         -1.185e+02  1.342e+01  -8.827  < 2e-16 ***\ndate        -2.754e-01  3.788e-02  -7.270 3.61e-13 ***\nlt_illness   1.329e+01  1.522e+00   8.734  < 2e-16 ***\nB1           1.155e+01  1.571e+00   7.354 1.92e-13 ***\nB2          -4.181e-01  6.212e-01  -0.673  0.50095    \nB3           2.062e+01  1.408e+00  14.644  < 2e-16 ***\nB4          -6.669e+00  1.256e+00  -5.311 1.09e-07 ***\nB5           9.446e+00  1.170e+00   8.071 6.96e-16 ***\nB6          -1.050e+01  1.398e+00  -7.509 5.96e-14 ***\nB7           2.309e+01  2.626e+00   8.793  < 2e-16 ***\nB8          -5.111e+00  1.279e+00  -3.995 6.48e-05 ***\nB9           1.139e+00  1.159e+00   0.983  0.32575    \nlong:lat     1.574e+00  1.462e-01  10.763  < 2e-16 ***\nlong:date   -1.937e-03  7.525e-04  -2.574  0.01005 *  \nlat:date     6.713e-03  7.309e-04   9.185  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.5609) family taken to be 1)\n\n    Null deviance: 22557.0  on 10649  degrees of freedom\nResidual deviance:  8352.3  on 10633  degrees of freedom\nAIC: 33849\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.5609 \n          Std. Err.:  0.0383 \n\n 2 x log-likelihood:  -33813.3960 \n\n\nThis model leads to a slightly better model by returning a small reduction in the residual deviance and AIC score. Interestingly it also returns highly statistically significant coefficients for all three interaction terms between longitude and latitude (long:lat), longitude and date (long:date), and latitude and date (lat:date). The first indicates that as we move one degree north and west, the number of new cases tend to increase in 2 cases. The second indicates that UTLAs located further north of England tend to record a smaller number of cases over time. The third indicates that UTLAs on the west of England tend to report a much higher number of cases as time passes.\nYou can report the output for all models estimated above by executing (after removing #):\n\n# export_summs(lm_m, poisson_m, qpoisson_m1, nb_m1, nb_m2)\n\nNote that you may need to install the huxtable package.\n\n10.6.2.1 Model Comparision\nTo compare regression models based on different specifications and assumptions, like those reported above, you may want to consider the cross-validation approach used in Chapter 5. An alternative approach if you would like to get a quick sense of model fit is to explore the correlation between observed and predicted values of the dependent variable. For our models, we can achieve this by executing:\n\n# computing predictions for all models\nlm_cnt <- predict(lm_m)\npoisson_cnt <- predict(poisson_m1)\nnb1_cnt <- predict(nb_m1)\nnb2_cnt <- predict(nb_m2)\nreg_df <- cbind(reg_df, lm_cnt, poisson_cnt, nb1_cnt, nb2_cnt)\n\n# computing correlation coefficients\ncormat <- cor(reg_df[, c(\"n_covid19_r\", \"lm_cnt\", \"poisson_cnt\", \"nb1_cnt\", \"nb2_cnt\")], \n              use=\"complete.obs\", \n              method=\"pearson\")\n\n# significance test\nsig1 <- corrplot::cor.mtest(reg_df[, c(\"n_covid19_r\", \"lm_cnt\", \"poisson_cnt\", \"nb1_cnt\", \"nb2_cnt\")],\n                            conf.level = .95)\n\n# create a correlogram\ncorrplot::corrplot.mixed(cormat,\n                         number.cex = 1,\n                         tl.pos = \"d\",\n                         tl.cex = 0.9)\n\n\n\n\nAll the models do a relatively job at predicting the observed count of new COVID-19 cases. They display correlation coefficients of 0.64/5 and high degree of correlation between them. These models will provide a good starting point for the assignment. There are a potentially few easy ways to make some considerable improvement.\n\nOption 1 Remove all zeros from the dependent variable n_covid19_r. They are likely to be affecting the ability of the model to predict positive values which are of main interest if we want to understand the spatio-temporal patterns of the outbreak.\nOption 2 Remove all zeros from the dependent variable and additionally use its log for the regression model.\nOption 3 Include more explanatory variables. Look for factors which can explain the spatial-temporal variability of the current COVID-19 outbreak. Look for hypotheses / anecdotal evidence from the newspapers and social media.\nOption 4 Check for collinearity. Collinearity is likely to be an issue given the way basis functions are created. Checking for collinearity of course will not improve the fit of the existing model but it is important to remove collinear terms if statistical inference is a key goal - which in this case is. Over to you now!"
  },
  {
    "objectID": "10-st_analysis.html#questions",
    "href": "10-st_analysis.html#questions",
    "title": "10  Spatio-Temporal Analysis",
    "section": "10.7 Questions",
    "text": "10.7 Questions\nWe will continue to use the COVID-19 dataset. Please see Chapter 11 for details on the data.\n\nsdf <- st_read(\"data/assignment_2_covid/covid19_eng.gpkg\")\n\nReading layer `covid19_eng' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_2_covid/covid19_eng.gpkg' \n  using driver `GPKG'\nSimple feature collection with 149 features and 507 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n\nUsing these data, you are required to address the following challenges:\n\nCreate a spatio-temporal visualisation.\nFit a ST model to assess changes over space and time.\n\nAnalyse and discuss:\n\nDo the results for your GWR and ST results differ: How do regression coefficients vary across space? Is this variation statistically significant?\nDoes the ST model indicate significant variations over time? How?\n\n\n\n\n\nCressie, Noel, and Gardar Johannesson. 2008. “Fixed Rank Kriging for Very Large Spatial Data Sets.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70 (1): 209–26.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S Müller. 2009. “Mining Sequence Data in r with the TraMineR Package: A User’s Guide.” Geneva: Department of Econometrics and Laboratory of Demography, University of Geneva.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. Chapman; Hall/CRC. https://doi.org/10.1201/9780203730058.\n\n\nPebesma, Edzer et al. 2012. “Spacetime: Spatio-Temporal Data in r.” Journal of Statistical Software 51 (7): 1–30.\n\n\nWikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019. Spatio-Temporal Statistics with r. CRC Press.\n\n\nZammit-Mangion, Andrew, and Noel Cressie. 2017. “FRK: An r Package for Spatial and Spatio-Temporal Prediction with Large Datasets.” arXiv Preprint arXiv:1705.08105."
  },
  {
    "objectID": "11-datasets.html#madrid-airbnb",
    "href": "11-datasets.html#madrid-airbnb",
    "title": "11  Data Sets",
    "section": "11.1 Madrid AirBnb",
    "text": "11.1 Madrid AirBnb\nThis dataset contains a pre-processed set of properties advertised on the AirBnb website within the region of Madrid (Spain), together with house characteristics.\n\nAvailability\nThe dataset is stored on a Geopackage that can be found, within the structure of this project, under:\n\npath <- \"data/assignment_1_madrid/madrid_abb.gpkg\"\n\n\ndb <- st_read(path)\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\n\n\nVariables\nFor each of the 17 properties, the following characteristics are available:\n\nprice: [string] Price with currency\nprice_usd: [int] Price expressed in USD\nlog1pm_price_usd: [float] Log of the price\naccommodates: [integer] Number of people the property accommodates\nbathrooms: [integer] Number of bathrooms\nbedrooms: [integer] Number of bedrooms\nbeds: [integer] Number of beds\nneighbourhood: [string] Name of the neighbourhood the property is located in\nroom_type: [string] Type of room offered (shared, private, entire home, hotel room)\nproperty_type: [string] Type of property advertised (apartment, house, hut, etc.)\nWiFi: [binary] Takes 1 if the property has WiFi, 0 otherwise\nCoffee: [binary] Takes 1 if the property has a coffee maker, 0 otherwise\nGym: [binary] Takes 1 if the property has access to a gym, 0 otherwise\nParking: [binary] Takes 1 if the property offers parking, 0 otherwise\nkm_to_retiro: [float] Euclidean distance from the property to the El Retiro park\ngeom: [geometry] Point geometry\n\n\n\nProjection\nThe location of each property is stored as point geometries and expressed in longitude and latitude coordinates:\n\nst_crs(db)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nSource & Pre-processing\nThe data are sourced from Inside Airbnb. A Jupyter notebook in Python (available at data/assignment_1_madrid/clean_data.ipynb) details the process from the original file available from source to the data in madrid_abb.gpkg."
  },
  {
    "objectID": "11-datasets.html#england-covid-19",
    "href": "11-datasets.html#england-covid-19",
    "title": "11  Data Sets",
    "section": "11.2 England COVID-19",
    "text": "11.2 England COVID-19\nThis dataset contains:\n\ndaily COVID-19 confirmed cases from 1st January, 2020 to 2nd February, 2021 from the GOV.UK dashboard;\nresident population characteristics from the 2011 census, available from the Office of National Statistics; and,\n2019 Index of Multiple Deprivation (IMD) data from GOV.UK and published by the Ministry of Housing, Communities & Local Government.\n\nThe data are at the Upper Tier Local Authority District (UTLAD) level - also known as Counties and Unitary Authorities.\n\nAvailability\nThe dataset is stored on a Geopackage:\n\nsdf <- st_read(\"data/assignment_2_covid/covid19_eng.gpkg\")\n\nReading layer `covid19_eng' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs453/202223/san/data/assignment_2_covid/covid19_eng.gpkg' \n  using driver `GPKG'\nSimple feature collection with 149 features and 507 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 657536\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\nVariables\nThe data set contains 508 variables:\n\nobjectid: [integer] unit identifier\nctyua19cd: [integer] Upper Tier Local Authority District (or Counties and Unitary Authorities) identifier\nctyua19nm: [character] Upper Tier Local Authority District (or Counties and Unitary Authorities) name\nRegion: [character] Region name\nlong: [numeric] longitude\nlat: [numeric] latitude\nst_areasha: [numeric] area in hectare\nX2020.01.31 to X2021.02.05: [numeric] Daily COVID-19 cases from 31st January, 2020 to 5th February, 2021\nIMD...Average.score - IMD.2019...Local.concentration: [numeric] IMD indicators - for details see File 11: upper-tier local authority summaries.\nResidents: [numeric] Total resident population\nHouseholds: [numeric] Total households\nDwellings: [numeric] Total dwellings\nHousehold_Spaces: [numeric] Total household spaces\nAged_16plus to Other_industry: [numeric] comprise 114 variables relating to various population and household attributes of the resident population. A description of all these variables can be found here\ngeom: [geometry] Point geometry\n\n\n\nProjection\nDetails of the coordinate reference system:\n\nst_crs(sdf)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc. 1988. Spatial Econometrics: Methods and Models.\nVol. 4. Springer Science & Business Media.\n\n\n———. 2003. “Spatial Externalities, Spatial Multipliers, and\nSpatial Econometrics.” International Regional Science\nReview 26 (2): 153–66.\n\n\n———. 2007. “Spatial Regression Analysis in r–a Workbook.”\nCenter for Spatially Integrated Social Science. http://csiss.org/GISPopSci/workshops/2011/PSU/readings/W15_Anselin2007.pdf.\n\n\nAnselin, Luc, and Sergio J. Rey. 2014. Modern Spatial Econometrics\nin Practice: A Guide to GeoDa, GeoDaSpace and PySAL. GeoDa Press\nLLC.\n\n\nArribas-Bel, Dani. 2014. “Spatial Data, Analysis, and Regression-a\nMini Course.” REGION 1 (1): R1. http://darribas.org/sdar_mini.\n\n\n———. 2019. “A Course on Geographic Data Science.” The\nJournal of Open Source Education 2 (14). https://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nArribas-Bel, Daniel, M.-À. Garcia-López, and Elisabet Viladecans-Marsal.\n2021. “Building(s and) Cities: Delineating Urban Areas with a\nMachine Learning Algorithm.” Journal of Urban Economics\n125 (September): 103217. https://doi.org/10.1016/j.jue.2019.103217.\n\n\nBanerjee, Sudipto, Bradley P Carlin, and Alan E Gelfand. 2014.\nHierarchical Modeling and Analysis for Spatial Data. Crc Press.\n\n\nBelsley, David A, Edwin Kuh, and Roy E Welsch. 2005. Regression\nDiagnostics: Identifying Influential Data and Sources of\nCollinearity. Vol. 571. John Wiley & Sons.\n\n\nBivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013.\nApplied Spatial Data Analysis with r. Springer New York. https://doi.org/10.1007/978-1-4614-7618-4.\n\n\nBrunsdon, Chris, and Lex Comber. 2015. An Introduction to r for\nSpatial Analysis & Mapping. Sage.\n\n\nBrunsdon, Chris, Stewart Fotheringham, and Martin Charlton. 1998.\n“Geographically Weighted Regression.” Journal of the\nRoyal Statistical Society: Series D (The Statistician) 47 (3):\n431–43.\n\n\nCasado-Díaz, José Manuel, Lucas Martínez-Bernabéu, and Francisco Rowe.\n2017. “An Evolutionary Approach to the Delimitation of Labour\nMarket Areas: An Empirical Application for Chile.” Spatial\nEconomic Analysis 12 (4): 379–403. https://doi.org/10.1080/17421772.2017.1273541.\n\n\nComber, Alexis, Christopher Brunsdon, Martin Charlton, Guanpeng Dong,\nRichard Harris, Binbin Lu, Yihe Lü, et al. 2022. “A Route Map for\nSuccessful Applications of Geographically Weighted Regression.”\nGeographical Analysis 55 (1): 155–78. https://doi.org/10.1111/gean.12316.\n\n\nCressie, Noel. 2015. Statistics for Spatial Data. John Wiley\n& Sons.\n\n\nCressie, Noel, and Gardar Johannesson. 2008. “Fixed Rank Kriging\nfor Very Large Spatial Data Sets.” Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 70 (1):\n209–26.\n\n\nFotheringham, A Stewart, and Morton E O’Kelly. 1989. Spatial\nInteraction Models: Formulations and Applications. Vol. 1. Kluwer\nAcademic Publishers Dordrecht.\n\n\nFotheringham, A Stewart, and David WS Wong. 1991. “The Modifiable\nAreal Unit Problem in Multivariate Statistical Analysis.”\nEnvironment and Planning A 23 (7): 1025–44.\n\n\nFotheringham, Stewart, Chris Brunsdon, and Martin Charlton. 2002.\nGeographically Weighted Regression. John Wiley & Sons.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S\nMüller. 2009. “Mining Sequence Data in r with the TraMineR\nPackage: A User’s Guide.” Geneva: Department of Econometrics\nand Laboratory of Demography, University of Geneva.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge University\nPress.\n\n\nGibbons, Stephen, Henry G Overman, and Eleonora Patacchini. 2014.\n“Spatial Methods.”\n\n\nGrolemund, Garrett, and Hadley Wickham. 2019. R for Data\nScience. O’Reilly, US. https://r4ds.had.co.nz.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting:\nPrinciples and Practice. OTexts.\n\n\nKong, Xiangjie, Menglin Li, Kai Ma, Kaiqi Tian, Mengyuan Wang, Zhaolong\nNing, and Feng Xia. 2018. “Big Trajectory Data: A Survey of\nApplications and Services.” IEEE Access 6: 58295–306. https://doi.org/10.1109/access.2018.2873779.\n\n\nKwan, Mei-Po, and Jiyeong Lee. 2004. “Geovisualization of Human\nActivity Patterns Using 3D GIS: A Time-Geographic Approach.”\nSpatially Integrated Social Science 27: 721–44.\n\n\nLoidl, Martin, Gudrun Wallentin, Robin Wendel, and Bernhard Zagel. 2016.\n“Mapping Bicycle Crash Risk Patterns on the Local Scale.”\nSafety 2 (3): 17. https://doi.org/10.3390/safety2030017.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with r. Chapman; Hall/CRC. https://doi.org/10.1201/9780203730058.\n\n\nLu, Binbin, Paul Harris, Martin Charlton, and Chris Brunsdon. 2014.\n“The GWmodel r Package: Further Topics for Exploring Spatial\nHeterogeneity Using Geographically Weighted Models.”\nGeo-Spatial Information Science 17 (2): 85–101.\n\n\nMultilevel Modelling, Centre for. n.d. “Introduction to Multilevel\nModelling.” n.d. http://www.bristol.ac.uk/cmm/learning/online-course/course-topics.html#m04.\n\n\nNiedomysl, Thomas. 2011. “How Migration Motives Change over\nMigration Distance: Evidence on Variation Across Socio-Economic and\nDemographic Groups.” Regional Studies 45 (6): 843–55. https://doi.org/10.1080/00343401003614266.\n\n\nÖnnerfors, Åsa, Mariana Kotzeva, Teodóra Brandmüller, et al. 2019.\n“Eurostat Regional Yearbook 2019 Edition.”\n\n\nOpenshaw, Stan. 1981. “The Modifiable Areal Unit Problem.”\nQuantitative Geography: A British View, 60–69.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021.\n“Trajectories of Neighbourhood Inequality in Britain: Unpacking\nInter-Regional Socioeconomic Imbalances,\n1971-2011.” The Geographical Journal 188\n(2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, and Stefano Cavazzi. 2019. “A\nScalable Analytical Framework for Spatio-Temporal Analysis of\nNeighborhood Change: A Sequence Analysis Approach.” In, 223–41.\nSpringer International Publishing. https://doi.org/10.1007/978-3-030-14745-7_13.\n\n\nPebesma, Edzer et al. 2012. “Spacetime: Spatio-Temporal Data in\nr.” Journal of Statistical Software 51 (7): 1–30.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming.\nGeographic Data Science with PySAL and the PyData Stack. CRC\npress.\n\n\nRobinson, WS. 1950. “Ecological Correlations and Individual\nBehavior.” American Sociological Review 15 (195):\n351–57.\n\n\nRowe, Francisco. 2022. “Introduction to Geographic Data\nScience.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/VHY2P.\n\n\nRowe, Francisco, Robin Lovelace, and Adam Dennett. 2022. “Spatial\nInteraction Modelling: A Manifesto.” http://dx.doi.org/10.31219/osf.io/xcdms.\n\n\nRowe, Francisco, and Nikos Patias. 2020. “Mapping the Spatial\nPatterns of Internal Migration in Europe.” Regional Studies,\nRegional Science 7 (1): 390–93. https://doi.org/10.1080/21681376.2020.1811139.\n\n\nSingleton, Alex. 2017. “Geographic Data Science for Urban\nAnalytics.” http://www.alex-singleton.com/GDS_UA_2017/.\n\n\nSingleton, Alexander D., and Seth E. Spielman. 2013. “The Past,\nPresent, and Future of Geodemographic Research in the United States and\nUnited Kingdom.” The Professional Geographer 66 (4):\n558–67. https://doi.org/10.1080/00330124.2013.848764.\n\n\nStillwell, John, Konstantinos Daras, and Martin Bell. 2018.\n“Spatial Aggregation Methods for Investigating the MAUP Effects in\nMigration Analysis.” Applied Spatial Analysis and Policy\n11 (4): 693–711. https://doi.org/10.1007/s12061-018-9274-6.\n\n\nTao, Sui, Jonathan Corcoran, Francisco Rowe, and Mark Hickman. 2018.\n“To Travel or Not to Travel: ‘Weather’\nIs the Question. Modelling the Effect of Local Weather Conditions on Bus\nRidership.” Transportation Research Part C: Emerging\nTechnologies 86 (January): 147–67. https://doi.org/10.1016/j.trc.2017.11.005.\n\n\nWheeler, David, and Michael Tiefelsdorf. 2005. “Multicollinearity\nand Correlation Among Local Regression Coefficients in Geographically\nWeighted Regression.” Journal of Geographical Systems 7\n(2): 161–87.\n\n\nWikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019.\nSpatio-Temporal Statistics with r. CRC Press.\n\n\nWilliamson, Paul. 2018. “Survey Analysis.”\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones,\nDavid Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2020.\n“Quantitative Geography III: Future Challenges and Challenging\nFutures.” Progress in Human Geography 45 (3): 596–608.\nhttps://doi.org/10.1177/0309132520924722.\n\n\nXie, Yihui, JJ Allaire, and Garrett Grolemund. 2019. R Markdown: The\nDefinitive Guide. CRC Press, Taylor & Francis, Chapman &\nHall Book. https://bookdown.org/yihui/rmarkdown/.\n\n\nZammit-Mangion, Andrew, and Noel Cressie. 2017. “FRK: An r Package\nfor Spatial and Spatio-Temporal Prediction with Large Datasets.”\narXiv Preprint arXiv:1705.08105."
  }
]