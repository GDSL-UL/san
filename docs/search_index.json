[
["index.html", "Spatial Analysis Notes Chapter 1 Spatial Analysis Notes", " Spatial Analysis Notes Francisco Rowe &amp; Dani Arribas-Bel 2020-02-06 Chapter 1 Spatial Analysis Notes This book contains computational illustrations on spatial analytical approaches using R. "],
["intro.html", "Chapter 2 Introduction 2.1 Dependencies 2.2 Introducing R 2.3 Setting the working directory 2.4 R Scripts and Notebooks 2.5 Getting Help 2.6 Variables and objects 2.7 Data Frames 2.8 Read Data 2.9 Manipulation Data 2.10 Using Spatial Data Frames 2.11 Useful Functions", " Chapter 2 Introduction This session1 introduces R Notebooks, basic functions and data types. These are all important concepts that we will use during the module. If you are already familiar with R, R notebooks and data types, you may want to jump to Section Read Data and start from there. This section describes how to read and manipulate data using sf and tidyverse functions, including mutate(), %&gt;% (known as pipe operator), select(), filter() and specific packages and functions how to manipulate spatial data. The content of this session is based on the following references: Grolemund and Wickham (2019), this book illustrates key libraries, including tidyverse, and functions for data manipulation in R Xie, Allaire, and Grolemund (2019), excellent introduction to R markdown! Williamson (2018), some examples from the first lecture of ENVS450 are used to explain the various types of random variables. Lovelace, Nowosad, and Muenchow (2020), a really good book on handling spatial data and historical background of the evolution of R packages for spatial data analysis. 2.1 Dependencies This tutorial uses the libraries below. Ensure they are installed on your machine2 before loading them executing the following code chunk: # Data manipulation, transformation and visualisation library(tidyverse) # Nice tables library(kableExtra) # Simple features (a standardised way to encode vector data ie. points, lines, polygons) library(sf) # Spatial objects conversion library(sp) # Thematic maps library(tmap) # Colour palettes library(RColorBrewer) # More colour palettes library(viridis) # nice colour schemes 2.2 Introducing R R is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques. It has gained widespread use in academia and industry. R offers a wider array of functionality than a traditional statistics package, such as SPSS and is composed of core (base) functionality, and is expandable through libraries hosted on CRAN. CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R. Commands are sent to R using either the terminal / command line or the R Console which is installed with R on either Windows or OS X. On Linux, there is no equivalent of the console, however, third party solutions exist. On your own machine, R can be installed from here. Normally RStudio is used to implement R coding. RStudio is an integrated development environment (IDE) for R and provides a more user-friendly front-end to R than the front-end provided with R. To run R or RStudio, just double click on the R or RStudio icon. Throughout this module, we will be using RStudio: Fig. 1. RStudio features. If you would like to know more about the various features of RStudio, watch this video 2.3 Setting the working directory Before we start any analysis, ensure to set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the data folder lives. #setwd(&#39;../data/sar.csv&#39;) #setwd(&#39;.&#39;) Note: It is good practice to not include spaces when naming folders and files. Use underscores or dots. You can check your current working directory by typing: getwd() ## [1] &quot;/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san&quot; 2.4 R Scripts and Notebooks An R script is a series of commands that you can execute at one time and help you save time. So you don’t repeat the same steps every time you want to execute the same process with different datasets. An R script is just a plain text file with R commands in it. To create an R script in RStudio, you need to Open a new script file: File &gt; New File &gt; R Script Write some code on your new script window by typing eg. mtcars Run the script. Click anywhere on the line of code, then hit Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run the command or select the code chunk and click run on the right-top corner of your script window. If do that, you should get: mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Save the script: File &gt; Save As, select your required destination folder, and enter any filename that you like, provided that it ends with the file extension .R An R Notebook is an R Markdown document with descriptive text and code chunks that can be executed independently and interactively, with output visible immediately beneath a code chunk - see Xie, Allaire, and Grolemund (2019). To create an R Notebook, you need to: Open a new script file: File &gt; New File &gt; R Notebook Fig. 2. YAML metadata for notebooks. Insert code chunks, either: use the Insert command on the editor toolbar; use the keyboard shortcut Ctrl + Alt + I or Cmd + Option + I (Mac); or, type the chunk delimiters ```{r} and ``` In a chunk code you can produce text output, tables, graphics and write code! You can control these outputs via chunk options which are provided inside the curly brackets eg. Fig. 3. Code chunk example. Details on the various options: https://rmarkdown.rstudio.com/lesson-3.html Execute code: hit “Run Current Chunk”, Ctrl + Shift + Enter or Cmd + Shift + Enter (Mac) Save an R notebook: File &gt; Save As. A notebook has a *.Rmd extension and when it is saved a *.nb.html file is automatically created. The latter is a self-contained HTML file which contains both a rendered copy of the notebook with all current chunk outputs and a copy of the *.Rmd file itself. Rstudio also offers a Preview option on the toolbar which can be used to create pdf, html and word versions of the notebook. To do this, choose from the drop-down list menu knit to ... 2.5 Getting Help You can use help or ? to ask for details for a specific function: help(sqrt) #or ?sqrt And using example provides examples for said function: example(sqrt) ## ## sqrt&gt; require(stats) # for spline ## ## sqrt&gt; require(graphics) ## ## sqrt&gt; xx &lt;- -9:9 ## ## sqrt&gt; plot(xx, sqrt(abs(xx)), col = &quot;red&quot;) Figure 2.1: Example sqrt ## ## sqrt&gt; lines(spline(xx, sqrt(abs(xx)), n=101), col = &quot;pink&quot;) 2.6 Variables and objects An object is a data structure having attributes and methods. In fact, everything in R is an object! A variable is a type of data object. Data objects also include list, vector, matrices and text. Creating a data object In R a variable can be created by using the symbol &lt;- to assign a value to a variable name. The variable name is entered on the left &lt;- and the value on the right. Note: Data objects can be given any name, provided that they start with a letter of the alphabet, and include only letters of the alphabet, numbers and the characters . and _. Hence AgeGroup, Age_Group and Age.Group are all valid names for an R data object. Note also that R is case-sensitive, to agegroup and AgeGroup would be treated as different data objects. To save the value 28 to a variable (data object) labelled age, run the code: age &lt;- 28 Inspecting a data object To inspect the contents of the data object age run the following line of code: age ## [1] 28 Find out what kind (class) of data object age is using: class(age) ## [1] &quot;numeric&quot; Inspect the structure of the age data object: str(age) ## num 28 The vector data object What if we have more than one response? We can use the c( ) function to combine multiple values into one data vector object: age &lt;- c(28, 36, 25, 24, 32) age ## [1] 28 36 25 24 32 class(age) #Still numeric.. ## [1] &quot;numeric&quot; str(age) #..but now a vector (set) of 5 separate values ## num [1:5] 28 36 25 24 32 Note that on each line in the code above any text following the # character is ignored by R when executing the code. Instead, text following a # can be used to add comments to the code to make clear what the code is doing. Two marks of good code are a clear layout and clear commentary on the code. 2.6.1 Basic Data Types There are a number of data types. Four are the most common. In R, numeric is the default type for numbers. It stores all numbers as floating-point numbers (numbers with decimals). This is because most statistical calculations deal with numbers with up to two decimals. Numeric num &lt;- 4.5 # Decimal values class(num) ## [1] &quot;numeric&quot; Integer int &lt;- as.integer(4) # Natural numbers. Note integers are also numerics. class(int) ## [1] &quot;integer&quot; Character cha &lt;- &quot;are you enjoying this?&quot; # text or string. You can also type `as.character(&quot;are you enjoying this?&quot;)` class(cha) ## [1] &quot;character&quot; Logical log &lt;- 2 &lt; 1 # assigns TRUE or FALSE. In this case, FALSE as 2 is greater than 1 log ## [1] FALSE class(log) ## [1] &quot;logical&quot; 2.6.2 Random Variables In statistics, we differentiate between data to capture: Qualitative attributes categorise objects eg.gender, marital status. To measure these attributes, we use Categorical data which can be divided into: Nominal data in categories that have no inherent order eg. gender Ordinal data in categories that have an inherent order eg. income bands Quantitative attributes: Discrete data: count objects of a certain category eg. number of kids, cars Continuous data: precise numeric measures eg. weight, income, length. In R these three types of random variables are represented by the following types of R data object: variables objects nominal factor ordinal ordered factor discrete numeric continuous numeric We have already encountered the R data type numeric. The next section introduces the factor data type. 2.6.2.1 Factor What is a factor? A factor variable assigns a numeric code to each possible category (level) in a variable. Behind the scenes, R stores the variable using these numeric codes to save space and speed up computing. For example, compare the size of a list of 10,000 males and females to a list of 10,000 1s and 0s. At the same time R also saves the category names associated with each numeric code (level). These are used for display purposes. For example, the variable gender, converted to a factor, would be stored as a series of 1s and 2s, where 1 = female and 2 = male; but would be displayed in all outputs using their category labels of female and male. Creating a factor To convert a numeric or character vector into a factor use the factor( ) function. For instance: gender &lt;- c(&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) # create a gender variable gender &lt;- factor(gender) # replace character vector with a factor version gender ## [1] female male male female female ## Levels: female male class(gender) ## [1] &quot;factor&quot; str(gender) ## Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 1 1 Now gender is a factor and is stored as a series of 1s and 2s, with 1s representing females and 2s representing males. The function levels( ) lists the levels (categories) associated with a given factor variable: levels(gender) ## [1] &quot;female&quot; &quot;male&quot; The categories are reported in the order that they have been numbered (starting from 1). Hence from the output we can infer that females are coded as 1, and males as 2. 2.7 Data Frames R stores different types of data using different types of data structure. Data are normally stored as a data.frame. A data frames contain one row per observation (e.g. wards) and one column per attribute (eg. population and health). We create three variables wards, population (pop) and people with good health (ghealth). We use 2011 census data counts for total population and good health for wards in Liverpool. wards &lt;- c(&quot;Allerton and Hunts Cross&quot;,&quot;Anfield&quot;,&quot;Belle Vale&quot;,&quot;Central&quot;,&quot;Childwall&quot;,&quot;Church&quot;,&quot;Clubmoor&quot;,&quot;County&quot;,&quot;Cressington&quot;,&quot;Croxteth&quot;,&quot;Everton&quot;,&quot;Fazakerley&quot;,&quot;Greenbank&quot;,&quot;Kensington and Fairfield&quot;,&quot;Kirkdale&quot;,&quot;Knotty Ash&quot;,&quot;Mossley Hill&quot;,&quot;Norris Green&quot;,&quot;Old Swan&quot;,&quot;Picton&quot;,&quot;Princes Park&quot;,&quot;Riverside&quot;,&quot;St Michael&#39;s&quot;,&quot;Speke-Garston&quot;,&quot;Tuebrook and Stoneycroft&quot;,&quot;Warbreck&quot;,&quot;Wavertree&quot;,&quot;West Derby&quot;,&quot;Woolton&quot;,&quot;Yew Tree&quot;) pop &lt;- c(14853,14510,15004,20340,13908,13974,15272,14045,14503, 14561,14782,16786,16132,15377,16115,13312,13816,15047, 16461,17009,17104,18422,12991,20300,16489,16481,14772, 14382,12921,16746) ghealth &lt;- c(7274,6124,6129,11925,7219,7461,6403,5930,7094,6992, 5517,7879,8990,6495,6662,5981,7322,6529,7192,7953, 7636,9001,6450,8973,7302,7521,7268,7013,6025,7717) Note that pop and ghealth and wards contains characters. 2.7.1 Creating a data frame We can create a data frame and examine its structure: df &lt;- data.frame(wards, pop, ghealth) df # or use view(data) ## wards pop ghealth ## 1 Allerton and Hunts Cross 14853 7274 ## 2 Anfield 14510 6124 ## 3 Belle Vale 15004 6129 ## 4 Central 20340 11925 ## 5 Childwall 13908 7219 ## 6 Church 13974 7461 ## 7 Clubmoor 15272 6403 ## 8 County 14045 5930 ## 9 Cressington 14503 7094 ## 10 Croxteth 14561 6992 ## 11 Everton 14782 5517 ## 12 Fazakerley 16786 7879 ## 13 Greenbank 16132 8990 ## 14 Kensington and Fairfield 15377 6495 ## 15 Kirkdale 16115 6662 ## 16 Knotty Ash 13312 5981 ## 17 Mossley Hill 13816 7322 ## 18 Norris Green 15047 6529 ## 19 Old Swan 16461 7192 ## 20 Picton 17009 7953 ## 21 Princes Park 17104 7636 ## 22 Riverside 18422 9001 ## 23 St Michael&#39;s 12991 6450 ## 24 Speke-Garston 20300 8973 ## 25 Tuebrook and Stoneycroft 16489 7302 ## 26 Warbreck 16481 7521 ## 27 Wavertree 14772 7268 ## 28 West Derby 14382 7013 ## 29 Woolton 12921 6025 ## 30 Yew Tree 16746 7717 str(df) # or use glimpse(data) ## &#39;data.frame&#39;: 30 obs. of 3 variables: ## $ wards : Factor w/ 30 levels &quot;Allerton and Hunts Cross&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ pop : num 14853 14510 15004 20340 13908 ... ## $ ghealth: num 7274 6124 6129 11925 7219 ... 2.7.2 Referencing data frame Throughout this module, you will need to refer to particular parts of a dataframe - perhaps a particular column (an area attribute); or a particular subset of respondents. Hence it is worth spending some time now mastering this particular skill. The relevant R function, [ ], has the format [row,col] or, more generally, [set of rows, set of cols]. Run the following commands to get a feel of how to extract different slices of the data: df # whole data.frame df[1, 1] # contents of first row and column df[2, 2:3] # contents of the second row, second and third columns df[1, ] # first row, ALL columns [the default if no columns specified] df[ ,1:2] # ALL rows; first and second columns df[c(1,3,5), ] # rows 1,3,5; ALL columns df[ , 2] # ALL rows; second column (by default results containing only #one column are converted back into a vector) df[ , 2, drop=FALSE] # ALL rows; second column (returned as a data.frame) In the above, note that we have used two other R functions: 1:3 The colon operator tells R to produce a list of numbers including the named start and end points. c(1,3,5) Tells R to combine the contents within the brackets into one list of objects Run both of these fuctions on their own to get a better understanding of what they do. Three other methods for referencing the contents of a data.frame make direct use of the variable names within the data.frame, which tends to make for easier to read/understand code: df[,&quot;pop&quot;] # variable name in quotes inside the square brackets df$pop # variable name prefixed with $ and appended to the data.frame name # or you can use attach attach(df) pop # but be careful if you already have an age variable in your local workspace Want to check the variables available, use the names( ): names(df) ## [1] &quot;wards&quot; &quot;pop&quot; &quot;ghealth&quot; 2.8 Read Data Ensure your memory is clear rm(list=ls()) # rm for targeted deletion / ls for listing all existing objects There are many commands to read / load data onto R. The command to use will depend upon the format they have been saved. Normally they are saved in csv format from Excel or other software packages. So we use either: df &lt;- read.table(&quot;path/file_name.csv&quot;, header = FALSE, sep =&quot;,&quot;) df &lt;- read(&quot;path/file_name.csv&quot;, header = FALSE) df &lt;- read.csv2(&quot;path/file_name.csv&quot;, header = FALSE) To read files in other formats, refer to this useful DataCamp tutorial census &lt;- read.csv(&quot;../san/data/census/census_data.csv&quot;) head(census) ## code ward pop16_74 higher_managerial pop ## 1 E05000886 Allerton and Hunts Cross 10930 1103 14853 ## 2 E05000887 Anfield 10712 312 14510 ## 3 E05000888 Belle Vale 10987 432 15004 ## 4 E05000889 Central 19174 1346 20340 ## 5 E05000890 Childwall 10410 1123 13908 ## 6 E05000891 Church 10569 1843 13974 ## ghealth ## 1 7274 ## 2 6124 ## 3 6129 ## 4 11925 ## 5 7219 ## 6 7461 # NOTE: always ensure your are setting the correct directory leading to the data. # It may differ from your existing working directory 2.8.1 Quickly inspect the data What class? What R data types? What data types? # 1 class(census) # 2 &amp; 3 str(census) Just interested in the variable names: names(census) ## [1] &quot;code&quot; &quot;ward&quot; &quot;pop16_74&quot; ## [4] &quot;higher_managerial&quot; &quot;pop&quot; &quot;ghealth&quot; or want to view the data: View(census) 2.9 Manipulation Data 2.9.1 Adding New Variables Usually you want to add / create new variables to your data frame using existing variables eg. computing percentages by dividing two variables. There are many ways in which you can do this i.e. referecing a data frame as we have done above, or using $ (e.g. census$pop). For this module, we’ll use tidyverse: census &lt;- census %&gt;% mutate(per_ghealth = ghealth / pop) Note we used a pipe operator %&gt;%, which helps make the code more efficient and readable - more details, see Grolemund and Wickham (2019). When using the pipe operator, recall to first indicate the data frame before %&gt;%. Note also the use a variable name before the = sign in brackets to indicate the name of the new variable after mutate. 2.9.2 Selecting Variables Usually you want to select a subset of variables for your analysis as storing to large data sets in your R memory can reduce the processing speed of your machine. A selection of data can be achieved by using the select function: ndf &lt;- census %&gt;% select(ward, pop16_74, per_ghealth) Again first indicate the data frame and then the variable you want to select to build a new data frame. Note the code chunk above has created a new data frame called ndf. Explore it. 2.9.3 Filtering Data You may also want to filter values based on defined conditions. You may want to filter observations greater than a certain threshold or only areas within a certain region. For example, you may want to select areas with a percentage of good health population over 50%: ndf2 &lt;- census %&gt;% filter(per_ghealth &lt; 0.5) You can use more than one variables to set conditions. Use “,” to add a condition. 2.9.4 Joining Data Drames When working with spatial data, we often need to join data. To this end, you need a common unique id variable. Let’s say, we want to add a data frame containing census data on households for Liverpool, and join the new attributes to one of the existing data frames in the workspace. First we will read the data frame we want to join (ie. census_data2.csv). # read data census2 &lt;- read.csv(&quot;../san/data/census/census_data2.csv&quot;) # visualise data structure str(census2) ## &#39;data.frame&#39;: 30 obs. of 3 variables: ## $ geo_code : Factor w/ 30 levels &quot;E05000886&quot;,&quot;E05000887&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ households : int 6359 6622 6622 7139 5391 5884 6576 6745 6317 6024 ... ## $ socialrented_households: int 827 1508 2818 1311 374 178 2859 1564 1023 1558 ... The variable geo_code in this data frame corresponds to the code in the existing data frame and they are unique so they can be automatically matched by using the merge() function. The merge() function uses two arguments: x and y. The former refers to data frame 1 and the latter to data frame 2. Both of these two data frames must have a id variable containing the same information. Note they can have different names. Another key argument to include is all.x=TRUE which tells the function to keep all the records in x, but only those in y that match in case there are discrepancies in the id variable. # join data frames join_dfs &lt;- merge(census, census2, by.x=&quot;code&quot;, by.y=&quot;geo_code&quot;, all.x = TRUE) # check data head(join_dfs) ## code ward pop16_74 higher_managerial pop ## 1 E05000886 Allerton and Hunts Cross 10930 1103 14853 ## 2 E05000887 Anfield 10712 312 14510 ## 3 E05000888 Belle Vale 10987 432 15004 ## 4 E05000889 Central 19174 1346 20340 ## 5 E05000890 Childwall 10410 1123 13908 ## 6 E05000891 Church 10569 1843 13974 ## ghealth per_ghealth households socialrented_households ## 1 7274 0.4897327 6359 827 ## 2 6124 0.4220538 6622 1508 ## 3 6129 0.4084911 6622 2818 ## 4 11925 0.5862832 7139 1311 ## 5 7219 0.5190538 5391 374 ## 6 7461 0.5339201 5884 178 2.9.5 Saving Data It may also be convinient to save your R projects. They contains all the objects that you have created in your workspace by using the save.image( ) function: save.image(&quot;week1_envs453.RData&quot;) This creates a file labelled “week1_envs453.RData” in your working directory. You can load this at a later stage using the load( ) function. load(&quot;week1_envs453.RData&quot;) Alternatively you can save / export your data into a csv file. The first argument in the function is the object name, and the second: the name of the csv we want to create. write.csv(join_dfs, &quot;join_censusdfs.csv&quot;) 2.10 Using Spatial Data Frames A core area of this module is learning to work with spatial data in R. R has various purposedly designed packages for manipulation of spatial data and spatial analysis techniques. Various R packages exist in CRAN eg. spatial, sgeostat, splancs, maptools, tmap, rgdal, spand and more recent development of sf - see Lovelace, Nowosad, and Muenchow (2020) for a great description and historical context for some of these packages. During this session, we will use sf. We first need to import our spatial data. We will use a shapefile containing data at Output Area (OA) level for Liverpool. These data illustrates the hierarchical structure of spatial data. 2.10.1 Read Spatial Data oa_shp &lt;- st_read(&quot;../san/data/census/Liverpool_OA.shp&quot;) ## Reading layer `Liverpool_OA&#39; from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/census/Liverpool_OA.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1584 features and 18 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1 ## epsg (SRID): NA ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs Examine the input data. A spatial data frame stores a range of attributes derived from a shapefile including the geometry of features (e.g. polygon shape and location), attributes for each feature (stored in the .dbf), projection and coordinates of the shapefile’s bounding box - for details, execute: ?st_read You can employ the usual functions to visualise the content of the created data frame: # visualise variable names names(oa_shp) ## [1] &quot;OA_CD&quot; &quot;LSOA_CD&quot; &quot;MSOA_CD&quot; &quot;LAD_CD&quot; &quot;pop&quot; &quot;H_Vbad&quot; ## [7] &quot;H_bad&quot; &quot;H_fair&quot; &quot;H_good&quot; &quot;H_Vgood&quot; &quot;age_men&quot; &quot;age_med&quot; ## [13] &quot;age_60&quot; &quot;S_Rent&quot; &quot;Ethnic&quot; &quot;illness&quot; &quot;unemp&quot; &quot;males&quot; ## [19] &quot;geometry&quot; # data structure str(oa_shp) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 1584 obs. of 19 variables: ## $ OA_CD : Factor w/ 1584 levels &quot;E00032987&quot;,&quot;E00032988&quot;,..: 1547 485 143 1567 980 1186 1122 889 863 433 ... ## $ LSOA_CD : Factor w/ 298 levels &quot;E01006512&quot;,&quot;E01006513&quot;,..: 291 96 33 124 187 231 220 175 173 83 ... ## $ MSOA_CD : Factor w/ 61 levels &quot;E02001347&quot;,&quot;E02001348&quot;,..: 59 12 19 23 29 20 31 14 30 10 ... ## $ LAD_CD : Factor w/ 1 level &quot;E08000012&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ pop : int 185 281 208 200 321 187 395 320 316 214 ... ## $ H_Vbad : int 1 2 3 7 4 4 5 9 5 4 ... ## $ H_bad : int 2 20 10 8 10 25 19 22 25 17 ... ## $ H_fair : int 9 47 22 17 32 70 42 53 55 39 ... ## $ H_good : int 53 111 71 52 112 57 131 104 104 53 ... ## $ H_Vgood : int 120 101 102 116 163 31 198 132 127 101 ... ## $ age_men : num 27.9 37.7 37.1 33.7 34.2 ... ## $ age_med : num 25 36 32 29 34 53 23 30 34 29 ... ## $ age_60 : num 0.0108 0.1637 0.1971 0.1 0.1402 ... ## $ S_Rent : num 0.0526 0.176 0.0235 0.2222 0.0222 ... ## $ Ethnic : num 0.3514 0.0463 0.0192 0.215 0.0779 ... ## $ illness : int 185 281 208 200 321 187 395 320 316 214 ... ## $ unemp : num 0.0438 0.121 0.1121 0.036 0.0743 ... ## $ males : int 122 128 95 120 158 123 207 164 157 94 ... ## $ geometry:sfc_MULTIPOLYGON of length 1584; first list element: List of 1 ## ..$ :List of 1 ## .. ..$ : num [1:14, 1:2] 335106 335130 335164 335173 335185 ... ## ..- attr(*, &quot;class&quot;)= chr &quot;XY&quot; &quot;MULTIPOLYGON&quot; &quot;sfg&quot; ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;names&quot;)= chr &quot;OA_CD&quot; &quot;LSOA_CD&quot; &quot;MSOA_CD&quot; &quot;LAD_CD&quot; ... # see first few observations head(oa_shp) ## Simple feature collection with 6 features and 18 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479 ## epsg (SRID): NA ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs ## OA_CD LSOA_CD MSOA_CD LAD_CD pop H_Vbad H_bad H_fair H_good ## 1 E00176737 E01033761 E02006932 E08000012 185 1 2 9 53 ## 2 E00033515 E01006614 E02001358 E08000012 281 2 20 47 111 ## 3 E00033141 E01006546 E02001365 E08000012 208 3 10 22 71 ## 4 E00176757 E01006646 E02001369 E08000012 200 7 8 17 52 ## 5 E00034050 E01006712 E02001375 E08000012 321 4 10 32 112 ## 6 E00034280 E01006761 E02001366 E08000012 187 4 25 70 57 ## H_Vgood age_men age_med age_60 S_Rent Ethnic illness ## 1 120 27.94054 25 0.01081081 0.05263158 0.35135135 185 ## 2 101 37.71174 36 0.16370107 0.17600000 0.04626335 281 ## 3 102 37.08173 32 0.19711538 0.02352941 0.01923077 208 ## 4 116 33.73000 29 0.10000000 0.22222222 0.21500000 200 ## 5 163 34.19003 34 0.14018692 0.02222222 0.07788162 321 ## 6 31 56.09091 53 0.44919786 0.88524590 0.11764706 187 ## unemp males geometry ## 1 0.04379562 122 MULTIPOLYGON (((335106.3 38... ## 2 0.12101911 128 MULTIPOLYGON (((335810.5 39... ## 3 0.11214953 95 MULTIPOLYGON (((336738 3931... ## 4 0.03597122 120 MULTIPOLYGON (((335914.5 39... ## 5 0.07428571 158 MULTIPOLYGON (((339325 3914... ## 6 0.44615385 123 MULTIPOLYGON (((338198.1 39... TASK: What are the geographical hierarchy in these data? What is the smallest geography? What is the largest geography? 2.10.2 Basic Mapping Again, many functions exist in CRAN for creating maps: plot to create static maps tmap to create static and interactive maps leaflet to create interactive maps mapview to create interactive maps ggplot2 to create data visualisations, including static maps shiny to create web applications, including maps Here this notebook demonstrates the use of plot and tmap. First plot is used to map the spatial distribution of non-British-born population in Liverpool. First we only map the geometries on the right, 2.10.2.1 Using plot # mapping geometry plot(st_geometry(oa_shp)) Figure 2.2: OAs of Livepool and then: # map attributes, adding intervals plot(oa_shp[&quot;Ethnic&quot;], key.pos = 4, axes = TRUE, key.width = lcm(1.3), key.length = 1., breaks = &quot;jenks&quot;, lwd = 0.1, border = &#39;grey&#39;) Figure 2.3: Spatial distribution of ethnic groups, Liverpool TASK: What is the key pattern emerging from this map? 2.10.2.2 Using tmap Similar to ggplot2, tmap is based on the idea of a ‘grammar of graphics’ which involves a separation between the input data and aesthetics (i.e. the way data are visualised). Each data set can be mapped in various different ways, including location as defined by its geometry, colour and other features. The basic building block is tm_shape() (which defines input data), followed by one or more layer elements such as tm_fill() and tm_dots(). # ensure geometry is valid oa_shp = lwgeom::st_make_valid(oa_shp) # map legend_title = expression(&quot;% ethnic pop.&quot;) map_oa = tm_shape(oa_shp) + tm_fill(col = &quot;Ethnic&quot;, title = legend_title, palette = magma(256), style = &quot;cont&quot;) + # add fill tm_borders(col = &quot;white&quot;, lwd = .01) + # add borders tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;top&quot;) , size = 4) + # add compass tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position = c(&quot;center&quot;, &quot;bottom&quot;)) # add scale bar map_oa Note that the operation + is used to add new layers. You can set style themes by tm_style. To visualise the existing styles use tmap_style_catalogue(), and you can also evaluate the code chunk below if you would like to create an interactive map. tmap_mode(&quot;view&quot;) map_oa TASK: Try mapping other variables in the spatial data frame. Where do population aged 60 and over concentrate? 2.10.3 Comparing geographies If you recall, one of the key issues of working with spatial data is the modifiable area unit problem (MAUP) - see lecture notes. To get a sense of the effects of MAUP, we analyse differences in the spatial patterns of the ethnic population in Liverpool between Middle Layer Super Output Areas (MSOAs) and OAs. So we map these geographies together. # read data at the msoa level msoa_shp &lt;- st_read(&quot;../san/data/census/Liverpool_MSOA.shp&quot;) ## Reading layer `Liverpool_MSOA&#39; from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/census/Liverpool_MSOA.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 61 features and 16 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1 ## epsg (SRID): NA ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs # ensure geometry is valid msoa_shp = lwgeom::st_make_valid(msoa_shp) # create a map map_msoa = tm_shape(msoa_shp) + tm_fill(col = &quot;Ethnic&quot;, title = legend_title, palette = magma(256), style = &quot;cont&quot;) + tm_borders(col = &quot;white&quot;, lwd = .01) + tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;top&quot;) , size = 4) + tm_scale_bar(breaks = c(0,1,2), text.size = 0.5, position = c(&quot;center&quot;, &quot;bottom&quot;)) # arrange maps tmap_arrange(map_msoa, map_oa) TASK: What differences do you see between OAs and MSOAs? Can you identify areas of spatial clustering? Where are they? 2.11 Useful Functions Function Description read.csv() read csv files into data frames str() inspect data structure mutate() create a new variable filter() filter observations based on variable values %&gt;% pipe operator - chain operations select() select variables merge() join dat frames st_read read spatial data (ie. shapefiles) plot() create a map based a spatial data set tm_shape(), tm_fill(), tm_borders() create a map using tmap functions tm_arrange display multiple maps in a single “metaplot” References "],
["points.html", "Chapter 3 Points 3.1 Dependencies 3.2 Data 3.3 KDE 3.4 Spatial Interpolation", " Chapter 3 Points This session3 is based on the following references, which are great follow-up’s on the topic: Lovelace and Cheshire (2014) is a great introduction. Chapter 6 of Brunsdon and Comber (2015), in particular subsections 6.3 and 6.7. Bivand, Pebesma, and Gómez-Rubio (2013) provides an in-depth treatment of spatial data in R. This tutorial is part of Spatial Analysis Notes, a compilation hosted as a GitHub repository that you can access it in a few ways: As a download of a .zip file that contains all the materials. As an html website. As a pdf document As a GitHub repository. 3.1 Dependencies This tutorial relies on the following libraries that you will need to have installed on your machine to be able to interactively follow along4. Once installed, load them up with the following commands: # Layout library(tufte) # For pretty table library(knitr) # Spatial Data management library(rgdal) ## Loading required package: sp ## rgdal: version: 1.4-6, (SVN revision 841) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28 ## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal ## GDAL binary built with GEOS: FALSE ## Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520] ## Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj ## Linking to sp version: 1.3-1 # Pretty graphics library(ggplot2) # Thematic maps library(tmap) # Pretty maps library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. # Various GIS utilities library(GISTools) ## Loading required package: maptools ## Checking rgeos availability: TRUE ## Loading required package: RColorBrewer ## Loading required package: MASS ## Loading required package: rgeos ## rgeos version: 0.5-1, (SVN revision 614) ## GEOS runtime version: 3.7.2-CAPI-1.11.2 ## Linking to sp version: 1.3-1 ## Polygon checking: TRUE # For all your interpolation needs library(gstat) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo # For data manipulation library(plyr) Before we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the house_transactions folder with the data lives. #setwd(&#39;/media/dani/baul/AAA/Documents/teaching/u-lvl/2016/envs453/code&#39;) setwd(&#39;.&#39;) 3.2 Data For this session, we will use a subset of residential property transaction data for the city of Liverpool. These are provided by the Land Registry (as part of their Price Paid Data) but have been cleaned and re-packaged by Dani Arribas-Bel. Let us start by reading the data, which comes in a shapefile: db &lt;- readOGR(dsn = &#39;data/house_transactions&#39;, layer = &#39;liv_house_trans&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/house_transactions&quot;, layer: &quot;liv_house_trans&quot; ## with 6324 features ## It has 18 fields ## Integer64 fields read as strings: price Before we forget, let us make sure price is considered a number, not a factor: db@data$price &lt;- as.numeric(as.character((db@data$price))) The dataset spans the year 2014: # Format dates dts &lt;- as.Date(db@data$trans_date) # Set up summary table tab &lt;- summary(dts) tab ## Min. 1st Qu. Median Mean 3rd Qu. ## &quot;2014-01-02&quot; &quot;2014-04-11&quot; &quot;2014-07-09&quot; &quot;2014-07-08&quot; &quot;2014-10-03&quot; ## Max. ## &quot;2014-12-30&quot; We can then examine the elements of the object with the summary method: summary(db) ## Object of class SpatialPointsDataFrame ## Coordinates: ## min max ## coords.x1 333536 345449 ## coords.x2 382684 397833 ## Is projected: TRUE ## proj4string : ## [+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 ## +y_0=-100000 +ellps=airy ## +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m ## +no_defs] ## Number of points: 6324 ## Data attributes: ## pcds id ## L1 6LS : 126 {00029226-80EF-4280-9809-109B8509656A}: 1 ## L8 5TE : 63 {00041BD2-4A07-4D41-A5AE-6459CD5FD37C}: 1 ## L1 5AQ : 34 {0005AE67-9150-41D4-8D56-6BFC868EECA3}: 1 ## L24 1WA: 31 {00183CD7-EE48-434B-8A1A-C94B30A93691}: 1 ## L17 6BT: 26 {003EA3A5-F804-458D-A66F-447E27569456}: 1 ## L3 1EE : 24 {00411304-DD5B-4F11-9748-93789D6A000E}: 1 ## (Other):6020 (Other) :6318 ## price trans_date type new duration ## Min. : 1000 2014-06-27 00:00: 109 D: 505 N:5495 F:3927 ## 1st Qu.: 70000 2014-12-19 00:00: 109 F:1371 Y: 829 L:2397 ## Median : 110000 2014-02-28 00:00: 105 O: 119 ## Mean : 144310 2014-10-31 00:00: 95 S:1478 ## 3rd Qu.: 160000 2014-03-28 00:00: 94 T:2851 ## Max. :26615720 2014-11-28 00:00: 94 ## (Other) :5718 ## paon saon street ## 3 : 203 FLAT 2 : 25 CROSSHALL STREET: 133 ## 11 : 151 FLAT 3 : 25 STANHOPE STREET : 63 ## 14 : 148 FLAT 1 : 24 PALL MALL : 47 ## 5 : 146 APARTMENT 4: 23 DUKE STREET : 41 ## 4 : 140 APARTMENT 2: 21 MANN ISLAND : 41 ## 8 : 128 (Other) : 893 OLD HALL STREET : 39 ## (Other):5408 NA&#39;s :5313 (Other) :5960 ## locality town district county ## WAVERTREE : 126 LIVERPOOL:6324 KNOWSLEY : 12 MERSEYSIDE:6324 ## MOSSLEY HILL: 102 LIVERPOOL:6311 ## WALTON : 88 WIRRAL : 1 ## WEST DERBY : 71 ## WOOLTON : 66 ## (Other) : 548 ## NA&#39;s :5323 ## ppd_cat status lsoa11 LSOA11CD ## A:5393 A:6324 E01033762: 144 E01033762: 144 ## B: 931 E01033756: 98 E01033756: 98 ## E01033752: 93 E01033752: 93 ## E01033750: 71 E01033750: 71 ## E01006518: 68 E01006518: 68 ## E01033755: 65 E01033755: 65 ## (Other) :5785 (Other) :5785 See how it contains several pieces, some relating to the spatial information, some relating to the tabular data attached to it. We can access each of the separately if we need it. For example, to pull out the names of the columns in the data.frame, we can use the @data appendix: colnames(db@data) ## [1] &quot;pcds&quot; &quot;id&quot; &quot;price&quot; &quot;trans_date&quot; &quot;type&quot; ## [6] &quot;new&quot; &quot;duration&quot; &quot;paon&quot; &quot;saon&quot; &quot;street&quot; ## [11] &quot;locality&quot; &quot;town&quot; &quot;district&quot; &quot;county&quot; &quot;ppd_cat&quot; ## [16] &quot;status&quot; &quot;lsoa11&quot; &quot;LSOA11CD&quot; The rest of this session will focus on two main elements of the shapefile: the spatial dimension (as stored in the point coordinates), and the house price values contained in the price column. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands: # Create the histogram hist &lt;- qplot(data=db@data,x=price) hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.1: Raw house prices in Liverpool This basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to shrink these differences is to take logarithms: # Create log and add it to the table logpr &lt;- log(as.numeric(db@data$price)) db@data[&#39;logpr&#39;] &lt;- logpr # Create the histogram hist &lt;- qplot(x=logpr) hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.2: Log of house price in Liverpool To obtain the spatial distribution of these houses, we need to turn away from the @data component of db. The easiest, quickest (and also dirtiest) way to get a sense of what the data look like over space is using plot: plot(db) Figure 3.3: Spatial distribution of house transactions in Liverpool 3.3 KDE Kernel Density Estimation (KDE) is a technique that creates a continuous representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions. 3.3.1 One-dimensional KDE KDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before: # Create the base base &lt;- ggplot(db@data, aes(x=logpr)) # Histogram hist &lt;- base + geom_histogram(bins=50, aes(y=..density..)) # Overlay density plot kde &lt;- hist + geom_density(fill=&quot;#FF6666&quot;, alpha=0.5, colour=&quot;#FF6666&quot;) kde Figure 3.4: Histogram and KDE of the log of house prices in Liverpool The key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts. 3.3.2 Two-dimensional (spatial) KDE Geography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates, X and Y (or latitude and longitude). Thanks to that, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a surface, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above. To create a spatial KDE in R, there are several ways. If you do not want to necessarily acknowledge the spatial nature of your data, or you they are not stored in a spatial format, you can plot them using ggplot2. Note we first need to convert the coordinates (stored in the spatial part of db) into columns of X and Y coordinates, then we can plot them: # Attach XY coordinates db@data[&#39;X&#39;] &lt;- db@coords[, 1] db@data[&#39;Y&#39;] &lt;- db@coords[, 2] # Set up base layer base &lt;- ggplot(data=db@data, aes(x=X, y=Y)) # Create the KDE surface kde &lt;- base + stat_density2d(aes(x = X, y = Y, alpha = ..level..), size = 0.01, bins = 16, geom = &#39;polygon&#39;) + scale_fill_gradient() kde Figure 3.5: KDE of house transactions in Liverpool Or, we can use a package such as the GISTools, which allows to pass a spatial object directly: # Compute the KDE kde &lt;- kde.points(db) # Plot the KDE level.plot(kde) Figure 3.6: KDE of house transactions in Liverpool Either of these approaches generate a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. To do it, we will leverage the library ggmap, which is designed to play nicely with the ggplot2 family (hence the seemingly counterintuitive example above). Before we can plot them with the online map, we need to reproject them though. # Reproject coordinates wgs84 &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;) db_wgs84 &lt;- spTransform(db, wgs84) db_wgs84@data[&#39;lon&#39;] &lt;- db_wgs84@coords[, 1] db_wgs84@data[&#39;lat&#39;] &lt;- db_wgs84@coords[, 2] xys &lt;- db_wgs84@data[c(&#39;lon&#39;, &#39;lat&#39;)] # Bounding box liv &lt;- c(left = min(xys$lon), bottom = min(xys$lat), right = max(xys$lon), top = max(xys$lat)) # Download map tiles basemap &lt;- get_stamenmap(liv, zoom = 12, maptype = &quot;toner-lite&quot;) ## Source : http://tile.stamen.com/toner-lite/12/2013/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2013/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2013/1327.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1327.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1327.png # Overlay KDE final &lt;- ggmap(basemap, extent = &quot;device&quot;, maprange=FALSE) + stat_density2d(data = db_wgs84@data, aes(x = lon, y = lat, alpha=..level.., fill = ..level..), size = 0.01, bins = 16, geom = &#39;polygon&#39;, show.legend = FALSE) + scale_fill_gradient2(&quot;Transaction\\nDensity&quot;, low = &quot;#fffff8&quot;, high = &quot;#8da0cb&quot;) final Figure 3.7: KDE of house transactions in Liverpool The plot above5 allows us to not only see the distribution of house transactions, but to relate it to what we know about Liverpool, allowing us to establish many more connections than we were previously able. Mainly, we can easily see that the area with a highest volume of houses being sold is the city centre, with a “hole” around it that displays very few to no transactions and then several pockets further away. 3.4 Spatial Interpolation The previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house transactions, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example). Many times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house price transactions, fits into this category as well: although where house are sold may be of relevance, more often we are interested in finding out what the “surface of price” looks like. Rather than where are most houses being sold? we usually want to know where the most expensive or most affordable houses are located. In cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is spatial interpolation, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g. temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible6. In this session, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, Banerjee, Carlin, and Gelfand (2014) or Cressie (2015) are hard but good overviews. 3.4.1 Inverse Distance Weight (IDW) interpolation The technique we will cover here is called Inverse Distance Weighting, or IDW for convenience. Brunsdon and Comber (2015) offer a good description: In the inverse distance weighting (IDW) approach to interpolation, to estimate the value of \\(z\\) at location \\(x\\) a weighted mean of nearby observations is taken […]. To accommodate the idea that observations of \\(z\\) at points closer to \\(x\\) should be given more importance in the interpolation, greater weight is given to these points […] — Page 204 The math7 is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. Essentially, the idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing. Enough chat, let’s code. From what we have just mentioned, there are a few steps to perform an IDW spatial interpolation: Create a regular grid over the area where we have house transactions. Obtain IDW estimates for each point in the grid, based on the values of \\(k\\) nearest neighbors. Plot a smoothed version of the grid, effectively representing the surface of house prices. Let us go in detail into each of them8. First, let us set up a grid: liv.grid &lt;- spsample(db, type=&#39;regular&#39;, n=25000) That’s it, we’re done! The function spsample hugely simplifies the task by taking a spatial object and returning the grid we need. Not a couple of additional arguments we pass: type allows us to get a set of points that are uniformly distributed over space, which is handy for the later smoothing; n controls how many points we want to create in that grid. On to the IDW. Again, this is hugely simplified by gstat: idw.hp &lt;- idw(price ~ 1, locations=db, newdata=liv.grid) ## [inverse distance weighted interpolation] Boom! We’ve got it. Let us pause for a second to see how we just did it. First, we pass price ~ 1. This specifies the formula we are using to model house prices. The name on the left of ~ represents the variable we want to explain, while everything to its right captures the explanatory variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply write 1. Then we specify the original locations for which we do have house prices (our original db object), and the points where we want to interpolate the house prices (the liv.grid object we just created above). One more note: by default, idw.hp uses all the available observations, weighted by distance, to provide an estimate for a given point. If you want to modify that and restrict the maximum number of neighbors to consider, you need to tweak the argument nmax, as we do above by using the 150 neares observations to each point9. The object we get from idw is another spatial table, just as db, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table: head(idw.hp@data) ## var1.pred var1.var ## 1 158115.1 NA ## 2 158226.4 NA ## 3 158340.5 NA ## 4 158457.6 NA ## 5 158577.7 NA ## 6 158701.0 NA The column we will pay attention to is var1.pred. And to see the locations for which those correspond: head(idw.hp@coords) ## x1 x2 ## [1,] 333610.0 382713 ## [2,] 333695.0 382713 ## [3,] 333779.9 382713 ## [4,] 333864.9 382713 ## [5,] 333949.8 382713 ## [6,] 334034.8 382713 So, for a hypothetical house sold at the location in the first row of idw.hp@coords (expressed in the OSGB coordinate system), the price we would guess it would cost, based on the price of houses sold nearby, is the first element of column var1.pred in idw.hp@data. 3.4.2 A surface of housing prices Once we have the IDW object computed, we can plot it to explore the distribution, not of house transactions in this case, but of house price over the geography of Liverpool. The easiest way to do this is by quickly calling the command spplot: spplot(idw.hp[&#39;var1.pred&#39;]) However, this is not entirely satisfactory for a number of reasons. Let us get an equivalen plot with the package tmap, which streamlines some of this and makes more aesthetically pleasant maps easier to build as it follows a “ggplot-y” approach. # Load up the layer liv.otl &lt;- readOGR(&#39;data/house_transactions&#39;, &#39;liv_outline&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/house_transactions&quot;, layer: &quot;liv_outline&quot; ## with 1 features ## It has 1 fields The shape we will overlay looks like this: qtm(liv.otl) Now let’s give it a first go! # p = tm_shape(liv.otl) + tm_fill(col=&#39;black&#39;, alpha=1) + tm_shape(idw.hp) + tm_symbols(col=&#39;var1.pred&#39;, size=0.1, alpha=0.25, border.lwd=0., palette=&#39;YlGn&#39;) p The last two plots, however, are not really a surface, but a representation of the points we have just estimated. To create a surface, we need to do an interim transformation to convert the spatial object idw.hp into a table that a “surface plotter” can understand. xyz &lt;- data.frame(x=coordinates(idw.hp)[, 1], y=coordinates(idw.hp)[, 2], z=idw.hp$var1.pred) Now we are ready to plot the surface as a contour: base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_contour(aes(z=z)) surface Figure 3.8: Contour of prices in Liverpool Which can also be shown as a filled contour: base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_raster(aes(fill=z)) surface The problem here, when compared to the KDE above for example, is that a few values are extremely large: qplot(data=xyz, x=z, geom=&#39;density&#39;) Figure 3.9: Skewness of prices in Liverpool Let us then take the logarithm before we plot the surface: xyz[&#39;lz&#39;] &lt;- log(xyz$z) base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_raster(aes(fill=lz), show.legend = F) surface Figure 3.10: Surface of log-prices in Liverpool Now this looks better. We can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because ggmap –the library we have been using to display tiles from cloud providers– does not play well with our own rasters (i.e. the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now. However, developments such as the sf project promise to make this easier in the future10. 3.4.3 “What should the next house’s price be?” The last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location. Let us say, the house is located on the coordinates x=340000, y=390000 as expressed in the GB National Grid coordinate system. In that case, we can do as follows: pt &lt;- SpatialPoints(cbind(x=340000, y=390000), proj4string = db@proj4string) idw.one &lt;- idw(price ~ 1, locations=db, newdata=pt) ## [inverse distance weighted interpolation] idw.one ## class : SpatialPointsDataFrame ## features : 1 ## extent : 340000, 340000, 390000, 390000 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## variables : 2 ## names : var1.pred, var1.var ## value : 157099.029513871, NA And, as show above, the estimated value is GBP157,09911. Using this predictive logic, and taking advantage of Google Maps and its geocoding capabilities, it is possible to devise a function that takes an arbitrary address in Liverpool and, based on the transactions occurred throughout 2014, provides an estimate of what the price for a property in that location could be. how.much.is &lt;- function(address, print.message=TRUE){ # Convert the address into Lon/Lat coordinates # NOTE: this now requires an API key # https://github.com/dkahle/ggmap#google-maps-and-credentials ll.pt &lt;- geocode(address) # Process as spatial table wgs84 &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;) ll.pt &lt;- SpatialPoints(cbind(x=ll.pt$lon, y=ll.pt$lat), proj4string = wgs84) # Transform Lon/Lat into OSGB pt &lt;- spTransform(ll.pt, db@proj4string) # Obtain prediction idw.one &lt;- idw(price ~ 1, locations=db, newdata=pt) price &lt;- idw.one@data$var1.pred # Return predicted price if(print.message==T){ writeLines(paste(&quot;\\n\\nBased on what surrounding properties were sold&quot;, &quot;for in 2014 a house located at&quot;, address, &quot;would&quot;, &quot;cost&quot;, paste(&quot;GBP&quot;, round(price), &quot;.&quot;, sep=&#39;&#39;), &quot;\\n\\n&quot;)) } return(price) } Ready to test! address &lt;- &quot;74 Bedford St S, Liverpool, L69 7ZT, UK&quot; #p &lt;- how.much.is(address) References "],
["flows.html", "Chapter 4 Flows", " Chapter 4 Flows DA-B to fill in "],
["spatial-econometrics.html", "Chapter 5 Spatial Econometrics", " Chapter 5 Spatial Econometrics DA-B to fill in "],
["multilevel-models-pt-i.html", "Chapter 6 Multilevel Models (Pt. I)", " Chapter 6 Multilevel Models (Pt. I) FR to fill in "],
["multilevel-models-pt-ii.html", "Chapter 7 Multilevel Models (Pt. II)", " Chapter 7 Multilevel Models (Pt. II) FR to fill in "],
["gwr.html", "Chapter 8 GWR", " Chapter 8 GWR FR "],
["space-time-analysis.html", "Chapter 9 Space-Time Analysis", " Chapter 9 Space-Time Analysis FR "],
["references.html", "References", " References "]
]
