[
["index.html", "Spatial Analysis Notes Chapter 1 Spatial Analysis Notes", " Spatial Analysis Notes Francisco Rowe-Gonzalez &amp; Dani Arribas-Bel 2020-02-03 Chapter 1 Spatial Analysis Notes [Introduction here] "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction FR-G to fill in "],
["points.html", "Chapter 3 Points 3.1 Dependencies 3.2 Data 3.3 KDE 3.4 Spatial Interpolation", " Chapter 3 Points This session1 is based on the following references, which are great follow-up’s on the topic: Lovelace and Cheshire (2014) is a great introduction. Chapter 6 of Brunsdon and Comber (2015), in particular subsections 6.3 and 6.7. Bivand, Pebesma, and Gómez-Rubio (2013) provides an in-depth treatment of spatial data in R. This tutorial is part of Spatial Analysis Notes, a compilation hosted as a GitHub repository that you can access it in a few ways: As a download of a .zip file that contains all the materials. As an html website. As a pdf document As a GitHub repository. 3.1 Dependencies This tutorial relies on the following libraries that you will need to have installed on your machine to be able to interactively follow along2. Once installed, load them up with the following commands: # Layout library(tufte) # For pretty table library(knitr) # Spatial Data management library(rgdal) ## Loading required package: sp ## rgdal: version: 1.4-4, (SVN revision 833) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20 ## Path to GDAL shared files: /usr/share/gdal/2.2 ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493] ## Path to PROJ.4 shared files: (autodetected) ## Linking to sp version: 1.3-1 # Pretty graphics library(ggplot2) # Thematic maps library(tmap) # Pretty maps library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. # Various GIS utilities library(GISTools) ## Loading required package: maptools ## Checking rgeos availability: TRUE ## Loading required package: RColorBrewer ## Loading required package: MASS ## Loading required package: rgeos ## rgeos version: 0.5-1, (SVN revision 614) ## GEOS runtime version: 3.6.2-CAPI-1.10.2 ## Linking to sp version: 1.3-1 ## Polygon checking: TRUE # For all your interpolation needs library(gstat) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo # For data manipulation library(plyr) Before we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the house_transactions folder with the data lives. #setwd(&#39;/media/dani/baul/AAA/Documents/teaching/u-lvl/2016/envs453/code&#39;) setwd(&#39;.&#39;) 3.2 Data For this session, we will use a subset of residential property transaction data for the city of Liverpool. These are provided by the Land Registry (as part of their Price Paid Data) but have been cleaned and re-packaged by Dani Arribas-Bel. Let us start by reading the data, which comes in a shapefile: db &lt;- readOGR(dsn = &#39;data/house_transactions&#39;, layer = &#39;liv_house_trans&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/jovyan/work/data/house_transactions&quot;, layer: &quot;liv_house_trans&quot; ## with 6324 features ## It has 18 fields ## Integer64 fields read as strings: price Before we forget, let us make sure price is considered a number, not a factor: db@data$price &lt;- as.numeric(as.character((db@data$price))) The dataset spans the year 2014: # Format dates dts &lt;- as.Date(db@data$trans_date) # Set up summary table tab &lt;- summary(dts) tab ## Min. 1st Qu. Median Mean 3rd Qu. ## &quot;2014-01-02&quot; &quot;2014-04-11&quot; &quot;2014-07-09&quot; &quot;2014-07-08&quot; &quot;2014-10-03&quot; ## Max. ## &quot;2014-12-30&quot; We can then examine the elements of the object with the summary method: summary(db) ## Object of class SpatialPointsDataFrame ## Coordinates: ## min max ## coords.x1 333536 345449 ## coords.x2 382684 397833 ## Is projected: TRUE ## proj4string : ## [+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 ## +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy ## +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894] ## Number of points: 6324 ## Data attributes: ## pcds id ## L1 6LS : 126 {00029226-80EF-4280-9809-109B8509656A}: 1 ## L8 5TE : 63 {00041BD2-4A07-4D41-A5AE-6459CD5FD37C}: 1 ## L1 5AQ : 34 {0005AE67-9150-41D4-8D56-6BFC868EECA3}: 1 ## L24 1WA: 31 {00183CD7-EE48-434B-8A1A-C94B30A93691}: 1 ## L17 6BT: 26 {003EA3A5-F804-458D-A66F-447E27569456}: 1 ## L3 1EE : 24 {00411304-DD5B-4F11-9748-93789D6A000E}: 1 ## (Other):6020 (Other) :6318 ## price trans_date type new duration ## Min. : 1000 2014-06-27 00:00: 109 D: 505 N:5495 F:3927 ## 1st Qu.: 70000 2014-12-19 00:00: 109 F:1371 Y: 829 L:2397 ## Median : 110000 2014-02-28 00:00: 105 O: 119 ## Mean : 144310 2014-10-31 00:00: 95 S:1478 ## 3rd Qu.: 160000 2014-03-28 00:00: 94 T:2851 ## Max. :26615720 2014-11-28 00:00: 94 ## (Other) :5718 ## paon saon street ## 3 : 203 FLAT 2 : 25 CROSSHALL STREET: 133 ## 11 : 151 FLAT 3 : 25 STANHOPE STREET : 63 ## 14 : 148 FLAT 1 : 24 PALL MALL : 47 ## 5 : 146 APARTMENT 4: 23 DUKE STREET : 41 ## 4 : 140 APARTMENT 2: 21 MANN ISLAND : 41 ## 8 : 128 (Other) : 893 OLD HALL STREET : 39 ## (Other):5408 NA&#39;s :5313 (Other) :5960 ## locality town district county ## WAVERTREE : 126 LIVERPOOL:6324 KNOWSLEY : 12 MERSEYSIDE:6324 ## MOSSLEY HILL: 102 LIVERPOOL:6311 ## WALTON : 88 WIRRAL : 1 ## WEST DERBY : 71 ## WOOLTON : 66 ## (Other) : 548 ## NA&#39;s :5323 ## ppd_cat status lsoa11 LSOA11CD ## A:5393 A:6324 E01033762: 144 E01033762: 144 ## B: 931 E01033756: 98 E01033756: 98 ## E01033752: 93 E01033752: 93 ## E01033750: 71 E01033750: 71 ## E01006518: 68 E01006518: 68 ## E01033755: 65 E01033755: 65 ## (Other) :5785 (Other) :5785 See how it contains several pieces, some relating to the spatial information, some relating to the tabular data attached to it. We can access each of the separately if we need it. For example, to pull out the names of the columns in the data.frame, we can use the @data appendix: colnames(db@data) ## [1] &quot;pcds&quot; &quot;id&quot; &quot;price&quot; &quot;trans_date&quot; &quot;type&quot; ## [6] &quot;new&quot; &quot;duration&quot; &quot;paon&quot; &quot;saon&quot; &quot;street&quot; ## [11] &quot;locality&quot; &quot;town&quot; &quot;district&quot; &quot;county&quot; &quot;ppd_cat&quot; ## [16] &quot;status&quot; &quot;lsoa11&quot; &quot;LSOA11CD&quot; The rest of this session will focus on two main elements of the shapefile: the spatial dimension (as stored in the point coordinates), and the house price values contained in the price column. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands: # Create the histogram hist &lt;- qplot(data=db@data,x=price) hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.1: Raw house prices in Liverpool This basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to shrink these differences is to take logarithms: # Create log and add it to the table logpr &lt;- log(as.numeric(db@data$price)) db@data[&#39;logpr&#39;] &lt;- logpr # Create the histogram hist &lt;- qplot(x=logpr) hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.2: Log of house price in Liverpool To obtain the spatial distribution of these houses, we need to turn away from the @data component of db. The easiest, quickest (and also dirtiest) way to get a sense of what the data look like over space is using plot: plot(db) Figure 3.3: Spatial distribution of house transactions in Liverpool 3.3 KDE Kernel Density Estimation (KDE) is a technique that creates a continuous representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions. 3.3.1 One-dimensional KDE KDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before: # Create the base base &lt;- ggplot(db@data, aes(x=logpr)) # Histogram hist &lt;- base + geom_histogram(bins=50, aes(y=..density..)) # Overlay density plot kde &lt;- hist + geom_density(fill=&quot;#FF6666&quot;, alpha=0.5, colour=&quot;#FF6666&quot;) kde Figure 3.4: Histogram and KDE of the log of house prices in Liverpool The key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts. 3.3.2 Two-dimensional (spatial) KDE Geography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates, X and Y (or latitude and longitude). Thanks to that, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a surface, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above. To create a spatial KDE in R, there are several ways. If you do not want to necessarily acknowledge the spatial nature of your data, or you they are not stored in a spatial format, you can plot them using ggplot2. Note we first need to convert the coordinates (stored in the spatial part of db) into columns of X and Y coordinates, then we can plot them: # Attach XY coordinates db@data[&#39;X&#39;] &lt;- db@coords[, 1] db@data[&#39;Y&#39;] &lt;- db@coords[, 2] # Set up base layer base &lt;- ggplot(data=db@data, aes(x=X, y=Y)) # Create the KDE surface kde &lt;- base + stat_density2d(aes(x = X, y = Y, alpha = ..level..), size = 0.01, bins = 16, geom = &#39;polygon&#39;) + scale_fill_gradient() kde Figure 3.5: KDE of house transactions in Liverpool Or, we can use a package such as the GISTools, which allows to pass a spatial object directly: # Compute the KDE kde &lt;- kde.points(db) # Plot the KDE level.plot(kde) Figure 3.6: KDE of house transactions in Liverpool Either of these approaches generate a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. To do it, we will leverage the library ggmap, which is designed to play nicely with the ggplot2 family (hence the seemingly counterintuitive example above). Before we can plot them with the online map, we need to reproject them though. # Reproject coordinates wgs84 &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;) db_wgs84 &lt;- spTransform(db, wgs84) db_wgs84@data[&#39;lon&#39;] &lt;- db_wgs84@coords[, 1] db_wgs84@data[&#39;lat&#39;] &lt;- db_wgs84@coords[, 2] xys &lt;- db_wgs84@data[c(&#39;lon&#39;, &#39;lat&#39;)] # Bounding box liv &lt;- c(left = min(xys$lon), bottom = min(xys$lat), right = max(xys$lon), top = max(xys$lat)) # Download map tiles basemap &lt;- get_stamenmap(liv, zoom = 12, maptype = &quot;toner-lite&quot;) ## Source : http://tile.stamen.com/toner-lite/12/2013/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1325.png ## Source : http://tile.stamen.com/toner-lite/12/2013/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1326.png ## Source : http://tile.stamen.com/toner-lite/12/2013/1327.png ## Source : http://tile.stamen.com/toner-lite/12/2014/1327.png ## Source : http://tile.stamen.com/toner-lite/12/2015/1327.png # Overlay KDE final &lt;- ggmap(basemap, extent = &quot;device&quot;, maprange=FALSE) + stat_density2d(data = db_wgs84@data, aes(x = lon, y = lat, alpha=..level.., fill = ..level..), size = 0.01, bins = 16, geom = &#39;polygon&#39;, show.legend = FALSE) + scale_fill_gradient2(&quot;Transaction\\nDensity&quot;, low = &quot;#fffff8&quot;, high = &quot;#8da0cb&quot;) final Figure 3.7: KDE of house transactions in Liverpool The plot above3 allows us to not only see the distribution of house transactions, but to relate it to what we know about Liverpool, allowing us to establish many more connections than we were previously able. Mainly, we can easily see that the area with a highest volume of houses being sold is the city centre, with a “hole” around it that displays very few to no transactions and then several pockets further away. 3.4 Spatial Interpolation The previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house transactions, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example). Many times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house price transactions, fits into this category as well: although where house are sold may be of relevance, more often we are interested in finding out what the “surface of price” looks like. Rather than where are most houses being sold? we usually want to know where the most expensive or most affordable houses are located. In cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is spatial interpolation, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g. temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible4. In this session, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, Banerjee, Carlin, and Gelfand (2014) or Cressie (2015) are hard but good overviews. 3.4.1 Inverse Distance Weight (IDW) interpolation The technique we will cover here is called Inverse Distance Weighting, or IDW for convenience. Brunsdon and Comber (2015) offer a good description: In the inverse distance weighting (IDW) approach to interpolation, to estimate the value of \\(z\\) at location \\(x\\) a weighted mean of nearby observations is taken […]. To accommodate the idea that observations of \\(z\\) at points closer to \\(x\\) should be given more importance in the interpolation, greater weight is given to these points […] — Page 204 The math5 is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. Essentially, the idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing. Enough chat, let’s code. From what we have just mentioned, there are a few steps to perform an IDW spatial interpolation: Create a regular grid over the area where we have house transactions. Obtain IDW estimates for each point in the grid, based on the values of \\(k\\) nearest neighbors. Plot a smoothed version of the grid, effectively representing the surface of house prices. Let us go in detail into each of them6. First, let us set up a grid: liv.grid &lt;- spsample(db, type=&#39;regular&#39;, n=25000) That’s it, we’re done! The function spsample hugely simplifies the task by taking a spatial object and returning the grid we need. Not a couple of additional arguments we pass: type allows us to get a set of points that are uniformly distributed over space, which is handy for the later smoothing; n controls how many points we want to create in that grid. On to the IDW. Again, this is hugely simplified by gstat: idw.hp &lt;- idw(price ~ 1, locations=db, newdata=liv.grid) ## [inverse distance weighted interpolation] Boom! We’ve got it. Let us pause for a second to see how we just did it. First, we pass price ~ 1. This specifies the formula we are using to model house prices. The name on the left of ~ represents the variable we want to explain, while everything to its right captures the explanatory variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply write 1. Then we specify the original locations for which we do have house prices (our original db object), and the points where we want to interpolate the house prices (the liv.grid object we just created above). One more note: by default, idw.hp uses all the available observations, weighted by distance, to provide an estimate for a given point. If you want to modify that and restrict the maximum number of neighbors to consider, you need to tweak the argument nmax, as we do above by using the 150 neares observations to each point7. The object we get from idw is another spatial table, just as db, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table: head(idw.hp@data) ## var1.pred var1.var ## 1 158122.0 NA ## 2 158233.4 NA ## 3 158347.7 NA ## 4 158465.0 NA ## 5 158585.5 NA ## 6 158709.2 NA The column we will pay attention to is var1.pred. And to see the locations for which those correspond: head(idw.hp@coords) ## x1 x2 ## [1,] 333608.5 382751.2 ## [2,] 333693.5 382751.2 ## [3,] 333778.4 382751.2 ## [4,] 333863.4 382751.2 ## [5,] 333948.3 382751.2 ## [6,] 334033.3 382751.2 So, for a hypothetical house sold at the location in the first row of idw.hp@coords (expressed in the OSGB coordinate system), the price we would guess it would cost, based on the price of houses sold nearby, is the first element of column var1.pred in idw.hp@data. 3.4.2 A surface of housing prices Once we have the IDW object computed, we can plot it to explore the distribution, not of house transactions in this case, but of house price over the geography of Liverpool. The easiest way to do this is by quickly calling the command spplot: spplot(idw.hp[&#39;var1.pred&#39;]) However, this is not entirely satisfactory for a number of reasons. Let us get an equivalen plot with the package tmap, which streamlines some of this and makes more aesthetically pleasant maps easier to build as it follows a “ggplot-y” approach. # Load up the layer liv.otl &lt;- readOGR(&#39;data/house_transactions&#39;, &#39;liv_outline&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/jovyan/work/data/house_transactions&quot;, layer: &quot;liv_outline&quot; ## with 1 features ## It has 1 fields The shape we will overlay looks like this: qtm(liv.otl) Now let’s give it a first go! # p = tm_shape(liv.otl) + tm_fill(col=&#39;black&#39;, alpha=1) + tm_shape(idw.hp) + tm_symbols(col=&#39;var1.pred&#39;, size=0.1, alpha=0.25, border.lwd=0., palette=&#39;YlGn&#39;) p The last two plots, however, are not really a surface, but a representation of the points we have just estimated. To create a surface, we need to do an interim transformation to convert the spatial object idw.hp into a table that a “surface plotter” can understand. xyz &lt;- data.frame(x=coordinates(idw.hp)[, 1], y=coordinates(idw.hp)[, 2], z=idw.hp$var1.pred) Now we are ready to plot the surface as a contour: base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_contour(aes(z=z)) surface Figure 3.8: Contour of prices in Liverpool Which can also be shown as a filled contour: base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_raster(aes(fill=z)) surface The problem here, when compared to the KDE above for example, is that a few values are extremely large: qplot(data=xyz, x=z, geom=&#39;density&#39;) Figure 3.9: Skewness of prices in Liverpool Let us then take the logarithm before we plot the surface: xyz[&#39;lz&#39;] &lt;- log(xyz$z) base &lt;- ggplot(data=xyz, aes(x=x, y=y)) surface &lt;- base + geom_raster(aes(fill=lz), show.legend = F) surface Figure 3.10: Surface of log-prices in Liverpool Now this looks better. We can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because ggmap –the library we have been using to display tiles from cloud providers– does not play well with our own rasters (i.e. the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now. However, developments such as the sf project promise to make this easier in the future8. 3.4.3 “What should the next house’s price be?” The last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location. Let us say, the house is located on the coordinates x=340000, y=390000 as expressed in the GB National Grid coordinate system. In that case, we can do as follows: pt &lt;- SpatialPoints(cbind(x=340000, y=390000), proj4string = db@proj4string) idw.one &lt;- idw(price ~ 1, locations=db, newdata=pt) ## [inverse distance weighted interpolation] idw.one ## class : SpatialPointsDataFrame ## features : 1 ## extent : 340000, 340000, 390000, 390000 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894 ## variables : 2 ## names : var1.pred, var1.var ## value : 157099.029513871, NA And, as show above, the estimated value is GBP157,0999. Using this predictive logic, and taking advantage of Google Maps and its geocoding capabilities, it is possible to devise a function that takes an arbitrary address in Liverpool and, based on the transactions occurred throughout 2014, provides an estimate of what the price for a property in that location could be. how.much.is &lt;- function(address, print.message=TRUE){ # Convert the address into Lon/Lat coordinates # NOTE: this now requires an API key # https://github.com/dkahle/ggmap#google-maps-and-credentials ll.pt &lt;- geocode(address) # Process as spatial table wgs84 &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;) ll.pt &lt;- SpatialPoints(cbind(x=ll.pt$lon, y=ll.pt$lat), proj4string = wgs84) # Transform Lon/Lat into OSGB pt &lt;- spTransform(ll.pt, db@proj4string) # Obtain prediction idw.one &lt;- idw(price ~ 1, locations=db, newdata=pt) price &lt;- idw.one@data$var1.pred # Return predicted price if(print.message==T){ writeLines(paste(&quot;\\n\\nBased on what surrounding properties were sold&quot;, &quot;for in 2014 a house located at&quot;, address, &quot;would&quot;, &quot;cost&quot;, paste(&quot;GBP&quot;, round(price), &quot;.&quot;, sep=&#39;&#39;), &quot;\\n\\n&quot;)) } return(price) } Ready to test! address &lt;- &quot;74 Bedford St S, Liverpool, L69 7ZT, UK&quot; #p &lt;- how.much.is(address) References "],
["flows.html", "Chapter 4 Flows", " Chapter 4 Flows DA-B to fill in "],
["spatial-econometrics.html", "Chapter 5 Spatial Econometrics", " Chapter 5 Spatial Econometrics DA-B to fill in "],
["multilevel-models-pt-i.html", "Chapter 6 Multilevel Models (Pt. I)", " Chapter 6 Multilevel Models (Pt. I) FR-G to fill in "],
["multilevel-models-pt-ii.html", "Chapter 7 Multilevel Models (Pt. II)", " Chapter 7 Multilevel Models (Pt. II) FR-G to fill in "],
["gwr.html", "Chapter 8 GWR", " Chapter 8 GWR FR-G "],
["space-time-analysis.html", "Chapter 9 Space-Time Analysis", " Chapter 9 Space-Time Analysis FR-G "],
["references.html", "References", " References "]
]
