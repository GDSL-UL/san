\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Spatial Analysis Notes},
            pdfauthor={Francisco Rowe \& Dani Arribas-Bel},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Spatial Analysis Notes}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Francisco Rowe \& Dani Arribas-Bel}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2020-02-13}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{spatial-analysis-notes}{%
\chapter{Spatial Analysis Notes}\label{spatial-analysis-notes}}

This book contains computational illustrations on spatial analytical approaches using R.

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

This session\footnote{This note is part of \href{index.html}{Spatial Analysis Notes} {Introduction -- R Notebooks + Basic Functions + Data Types} by Francisco Rowe is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.} introduces R Notebooks, basic functions and data types. These are all important concepts that we will use during the module.

If you are already familiar with R, R notebooks and data types, you may want to jump to Section \protect\hyperlink{sec_readdata}{Read Data} and start from there. This section describes how to read and manipulate data using \texttt{sf} and \texttt{tidyverse} functions, including \texttt{mutate()}, \texttt{\%\textgreater{}\%} (known as pipe operator), \texttt{select()}, \texttt{filter()} and specific packages and functions how to manipulate spatial data.

The content of this session is based on the following references:

\begin{itemize}
\item
  \citet{grolemund_wickham_2019_book}, this book illustrates key libraries, including tidyverse, and functions for data manipulation in R
\item
  \citet{Xie_et_al_2019_book}, excellent introduction to R markdown!
\item
  \citet{envs450_2018}, some examples from the first lecture of ENVS450 are used to explain the various types of random variables.
\item
  \citet{Lovelace_et_al_2020_book}, a really good book on handling spatial data and historical background of the evolution of R packages for spatial data analysis.
\end{itemize}

\hypertarget{dependencies}{%
\section{Dependencies}\label{dependencies}}

This tutorial uses the libraries below. Ensure they are installed on your machine\footnote{You can install package \texttt{mypackage} by running the command \texttt{install.packages("mypackage")} on the R prompt or through the \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in RStudio.} before loading them executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Data manipulation, transformation and visualisation}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{# Nice tables}
\KeywordTok{library}\NormalTok{(kableExtra)}
\CommentTok{# Simple features (a standardised way to encode vector data ie. points, lines, polygons)}
\KeywordTok{library}\NormalTok{(sf) }
\CommentTok{# Spatial objects conversion}
\KeywordTok{library}\NormalTok{(sp) }
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap) }
\CommentTok{# Colour palettes}
\KeywordTok{library}\NormalTok{(RColorBrewer) }
\CommentTok{# More colour palettes}
\KeywordTok{library}\NormalTok{(viridis) }\CommentTok{# nice colour schemes}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-r}{%
\section{Introducing R}\label{introducing-r}}

R is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques. It has gained widespread use in academia and industry. R offers a wider array of functionality than a traditional statistics package, such as SPSS and is composed of core (base) functionality, and is expandable through libraries hosted on \href{https://cran.r-project.org}{CRAN}. CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.

Commands are sent to R using either the terminal / command line or the R Console which is installed with R on either Windows or OS X. On Linux, there is no equivalent of the console, however, third party solutions exist. On your own machine, R can be installed from \href{https://www.r-project.org/}{here}.

Normally RStudio is used to implement R coding. RStudio is an integrated development environment (IDE) for R and provides a more user-friendly front-end to R than the front-end provided with R.

To run R or RStudio, just double click on the R or RStudio icon. Throughout this module, we will be using RStudio:

\begin{figure}
\centering
\includegraphics{figs/ch2/rstudio_features.png}
\caption{Fig. 1. RStudio features.}
\end{figure}

If you would like to know more about the various features of RStudio, watch this \href{https://rstudio.com/products/rstudio/}{video}

\hypertarget{setting-the-working-directory}{%
\section{Setting the working directory}\label{setting-the-working-directory}}

Before we start any analysis, ensure to set the path to the directory where we are working. We can easily do that with \texttt{setwd()}. Please replace in the following line the path to the folder where you have placed this file -and where the \texttt{data} folder lives.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd('../data/sar.csv')}
\CommentTok{#setwd('.')}
\end{Highlighting}
\end{Shaded}

Note: It is good practice to not include spaces when naming folders and files. Use \emph{underscores} or \emph{dots}.

You can check your current working directory by typing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "/home/jovyan/work"
\end{verbatim}

\hypertarget{r-scripts-and-notebooks}{%
\section{R Scripts and Notebooks}\label{r-scripts-and-notebooks}}

An \emph{R script} is a series of commands that you can execute at one time and help you save time. So you don't repeat the same steps every time you want to execute the same process with different datasets. An R script is just a plain text file with R commands in it.

To create an R script in RStudio, you need to

\begin{itemize}
\item
  Open a new script file: \emph{File} \textgreater{} \emph{New File} \textgreater{} \emph{R Script}
\item
  Write some code on your new script window by typing eg. \texttt{mtcars}
\item
  Run the script. Click anywhere on the line of code, then hit \emph{Ctrl + Enter} (Windows) or \emph{Cmd + Enter} (Mac) to run the command or select the code chunk and click \emph{run} on the right-top corner of your script window. If do that, you should get:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Save the script: \emph{File} \textgreater{} \emph{Save As}, select your required destination folder, and enter any filename that you like, provided that it ends with the file extension \emph{.R}
\end{itemize}

An \emph{R Notebook} is an R Markdown document with descriptive text and code chunks that can be executed independently and interactively, with output visible immediately beneath a code chunk - see \citet{Xie_et_al_2019_book}.

To create an R Notebook, you need to:

\begin{itemize}
\tightlist
\item
  Open a new script file: \emph{File} \textgreater{} \emph{New File} \textgreater{} \emph{R Notebook}
\end{itemize}

\begin{figure}
\centering
\includegraphics{figs/ch2/rnotebook_yaml.png}
\caption{Fig. 2. YAML metadata for notebooks.}
\end{figure}

\begin{itemize}
\tightlist
\item
  Insert code chunks, either:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  use the \emph{Insert} command on the editor toolbar;
\item
  use the keyboard shortcut \emph{Ctrl + Alt + I} or \emph{Cmd + Option + I} (Mac); or,
\item
  type the chunk delimiters \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}} and \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{enumerate}

In a chunk code you can produce text output, tables, graphics and write code! You can control these outputs via chunk options which are provided inside the curly brackets eg.

\begin{figure}
\centering
\includegraphics{figs/ch2/codechunk.png}
\caption{Fig. 3. Code chunk example. Details on the various options: \url{https://rmarkdown.rstudio.com/lesson-3.html}}
\end{figure}

\begin{itemize}
\item
  Execute code: hit \emph{``Run Current Chunk''}, \emph{Ctrl + Shift + Enter} or \emph{Cmd + Shift + Enter} (Mac)
\item
  Save an R notebook: \emph{File} \textgreater{} \emph{Save As}. A notebook has a \texttt{*.Rmd} extension and when it is saved a \texttt{*.nb.html} file is automatically created. The latter is a self-contained HTML file which contains both a rendered copy of the notebook with all current chunk outputs and a copy of the *.Rmd file itself.
\end{itemize}

Rstudio also offers a \emph{Preview} option on the toolbar which can be used to create pdf, html and word versions of the notebook. To do this, choose from the drop-down list menu \texttt{knit\ to\ ...}

\hypertarget{getting-help}{%
\section{Getting Help}\label{getting-help}}

You can use \texttt{help} or \texttt{?} to ask for details for a specific function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(sqrt) }\CommentTok{#or ?sqrt}
\end{Highlighting}
\end{Shaded}

And using \texttt{example} provides examples for said function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{example}\NormalTok{(sqrt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## sqrt> require(stats) # for spline
## 
## sqrt> require(graphics)
## 
## sqrt> xx <- -9:9
## 
## sqrt> plot(xx, sqrt(abs(xx)),  col = "red")
\end{verbatim}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-7-1.pdf}
\caption{\label{fig:unnamed-chunk-7}Example sqrt}
\end{figure}

\begin{verbatim}
## 
## sqrt> lines(spline(xx, sqrt(abs(xx)), n=101), col = "pink")
\end{verbatim}

\hypertarget{variables-and-objects}{%
\section{Variables and objects}\label{variables-and-objects}}

An \emph{object} is a data structure having attributes and methods. In fact, everything in R is an object!

A \emph{variable} is a type of data object. Data objects also include list, vector, matrices and text.

\begin{itemize}
\tightlist
\item
  Creating a data object
\end{itemize}

In R a variable can be created by using the symbol \texttt{\textless{}-} to assign a value to a variable name. The variable name is entered on the left \texttt{\textless{}-} and the value on the right. Note: Data objects can be given any name, provided that they start with a letter of the alphabet, and include only letters of the alphabet, numbers and the characters \texttt{.} and \texttt{\_}. Hence AgeGroup, Age\_Group and Age.Group are all valid names for an R data object. Note also that R is case-sensitive, to agegroup and AgeGroup would be treated as different data objects.

To save the value \emph{28} to a variable (data object) labelled \emph{age}, run the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\DecValTok{28}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Inspecting a data object
\end{itemize}

To inspect the contents of the data object \emph{age} run the following line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 28
\end{verbatim}

Find out what kind (class) of data object \emph{age} is using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(age) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

Inspect the structure of the \emph{age} data object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(age) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num 28
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The \emph{vector} data object
\end{itemize}

What if we have more than one response? We can use the \texttt{c(\ )} function to combine multiple values into one data vector object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{32}\NormalTok{)}
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 28 36 25 24 32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(age) }\CommentTok{#Still numeric..}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(age) }\CommentTok{#..but now a vector (set) of 5 separate values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:5] 28 36 25 24 32
\end{verbatim}

Note that on each line in the code above any text following the \texttt{\#} character is ignored by R when executing the code. Instead, text following a \texttt{\#} can be used to add comments to the code to make clear what the code is doing. Two marks of good code are a clear layout and clear commentary on the code.

\hypertarget{basic-data-types}{%
\subsection{Basic Data Types}\label{basic-data-types}}

There are a number of data types. Four are the most common. In R, \textbf{numeric} is the default type for numbers. It stores all numbers as floating-point numbers (numbers with decimals). This is because most statistical calculations deal with numbers with up to two decimals.

\begin{itemize}
\tightlist
\item
  Numeric
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num <-}\StringTok{ }\FloatTok{4.5} \CommentTok{# Decimal values}
\KeywordTok{class}\NormalTok{(num)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Integer
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{int <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\DecValTok{4}\NormalTok{) }\CommentTok{# Natural numbers. Note integers are also numerics.}
\KeywordTok{class}\NormalTok{(int)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Character
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cha <-}\StringTok{ "are you enjoying this?"} \CommentTok{# text or string. You can also type `as.character("are you enjoying this?")`}
\KeywordTok{class}\NormalTok{(cha)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Logical
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log <-}\StringTok{ }\DecValTok{2} \OperatorTok{<}\StringTok{ }\DecValTok{1} \CommentTok{# assigns TRUE or FALSE. In this case, FALSE as 2 is greater than 1}
\NormalTok{log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(log)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\hypertarget{random-variables}{%
\subsection{Random Variables}\label{random-variables}}

In statistics, we differentiate between data to capture:

\begin{itemize}
\item
  \emph{Qualitative attributes} categorise objects eg.gender, marital status. To measure these attributes, we use \emph{Categorical} data which can be divided into:

  \begin{itemize}
  \tightlist
  \item
    \emph{Nominal} data in categories that have no inherent order eg. gender
  \item
    \emph{Ordinal} data in categories that have an inherent order eg. income bands
  \end{itemize}
\item
  \emph{Quantitative attributes}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Discrete} data: count objects of a certain category eg. number of kids, cars
  \item
    \emph{Continuous} data: precise numeric measures eg. weight, income, length.
  \end{itemize}
\end{itemize}

In R these three types of random variables are represented by the following types of R data object:

\begin{tabular}{l|l}
\hline
variables & objects\\
\hline
nominal & factor\\
\hline
ordinal & ordered factor\\
\hline
discrete & numeric\\
\hline
continuous & numeric\\
\hline
\end{tabular}

We have already encountered the R data type \emph{numeric}. The next section introduces the \emph{factor} data type.

\hypertarget{factor}{%
\subsubsection{Factor}\label{factor}}

\textbf{What is a factor?}

A factor variable assigns a numeric code to each possible category (\emph{level}) in a variable. Behind the scenes, R stores the variable using these numeric codes to save space and speed up computing. For example, compare the size of a list of \texttt{10,000} \emph{males} and \emph{females} to a list of \texttt{10,000} \texttt{1s} and \texttt{0s}. At the same time R also saves the category names associated with each numeric code (level). These are used for display purposes.

For example, the variable \emph{gender}, converted to a factor, would be stored as a series of \texttt{1s} and \texttt{2s}, where \texttt{1\ =\ female} and \texttt{2\ =\ male}; but would be displayed in all outputs using their category labels of \emph{female} and \emph{male}.

\textbf{Creating a factor}

To convert a numeric or character vector into a factor use the \texttt{factor(\ )} function. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gender <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\NormalTok{,}\StringTok{"male"}\NormalTok{,}\StringTok{"female"}\NormalTok{,}\StringTok{"female"}\NormalTok{) }\CommentTok{# create a gender variable}
\NormalTok{gender <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(gender) }\CommentTok{# replace character vector with a factor version}
\NormalTok{gender}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] female male   male   female female
## Levels: female male
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Factor w/ 2 levels "female","male": 1 2 2 1 1
\end{verbatim}

Now \emph{gender} is a factor and is stored as a series of \texttt{1s} and \texttt{2s}, with \texttt{1s} representing \texttt{females} and \texttt{2s} representing \texttt{males}. The function \texttt{levels(\ )} lists the levels (categories) associated with a given factor variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "female" "male"
\end{verbatim}

The categories are reported in the order that they have been numbered (starting from \texttt{1}). Hence from the output we can infer that \texttt{females} are coded as \texttt{1}, and \texttt{males} as \texttt{2}.

\hypertarget{data-frames}{%
\section{Data Frames}\label{data-frames}}

R stores different types of data using different types of data structure. Data are normally stored as a \emph{data.frame}. A data frames contain one row per observation (e.g.~wards) and one column per attribute (eg. population and health).

We create three variables wards, population (\texttt{pop}) and people with good health (\texttt{ghealth}). We use 2011 census data counts for total population and good health for wards in Liverpool.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wards <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Allerton and Hunts Cross"}\NormalTok{,}\StringTok{"Anfield"}\NormalTok{,}\StringTok{"Belle Vale"}\NormalTok{,}\StringTok{"Central"}\NormalTok{,}\StringTok{"Childwall"}\NormalTok{,}\StringTok{"Church"}\NormalTok{,}\StringTok{"Clubmoor"}\NormalTok{,}\StringTok{"County"}\NormalTok{,}\StringTok{"Cressington"}\NormalTok{,}\StringTok{"Croxteth"}\NormalTok{,}\StringTok{"Everton"}\NormalTok{,}\StringTok{"Fazakerley"}\NormalTok{,}\StringTok{"Greenbank"}\NormalTok{,}\StringTok{"Kensington and Fairfield"}\NormalTok{,}\StringTok{"Kirkdale"}\NormalTok{,}\StringTok{"Knotty Ash"}\NormalTok{,}\StringTok{"Mossley Hill"}\NormalTok{,}\StringTok{"Norris Green"}\NormalTok{,}\StringTok{"Old Swan"}\NormalTok{,}\StringTok{"Picton"}\NormalTok{,}\StringTok{"Princes Park"}\NormalTok{,}\StringTok{"Riverside"}\NormalTok{,}\StringTok{"St Michael's"}\NormalTok{,}\StringTok{"Speke-Garston"}\NormalTok{,}\StringTok{"Tuebrook and Stoneycroft"}\NormalTok{,}\StringTok{"Warbreck"}\NormalTok{,}\StringTok{"Wavertree"}\NormalTok{,}\StringTok{"West Derby"}\NormalTok{,}\StringTok{"Woolton"}\NormalTok{,}\StringTok{"Yew Tree"}\NormalTok{)}

\NormalTok{pop <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{14853}\NormalTok{,}\DecValTok{14510}\NormalTok{,}\DecValTok{15004}\NormalTok{,}\DecValTok{20340}\NormalTok{,}\DecValTok{13908}\NormalTok{,}\DecValTok{13974}\NormalTok{,}\DecValTok{15272}\NormalTok{,}\DecValTok{14045}\NormalTok{,}\DecValTok{14503}\NormalTok{,}
                \DecValTok{14561}\NormalTok{,}\DecValTok{14782}\NormalTok{,}\DecValTok{16786}\NormalTok{,}\DecValTok{16132}\NormalTok{,}\DecValTok{15377}\NormalTok{,}\DecValTok{16115}\NormalTok{,}\DecValTok{13312}\NormalTok{,}\DecValTok{13816}\NormalTok{,}\DecValTok{15047}\NormalTok{,}
                \DecValTok{16461}\NormalTok{,}\DecValTok{17009}\NormalTok{,}\DecValTok{17104}\NormalTok{,}\DecValTok{18422}\NormalTok{,}\DecValTok{12991}\NormalTok{,}\DecValTok{20300}\NormalTok{,}\DecValTok{16489}\NormalTok{,}\DecValTok{16481}\NormalTok{,}\DecValTok{14772}\NormalTok{,}
                \DecValTok{14382}\NormalTok{,}\DecValTok{12921}\NormalTok{,}\DecValTok{16746}\NormalTok{)}

\NormalTok{ghealth <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{7274}\NormalTok{,}\DecValTok{6124}\NormalTok{,}\DecValTok{6129}\NormalTok{,}\DecValTok{11925}\NormalTok{,}\DecValTok{7219}\NormalTok{,}\DecValTok{7461}\NormalTok{,}\DecValTok{6403}\NormalTok{,}\DecValTok{5930}\NormalTok{,}\DecValTok{7094}\NormalTok{,}\DecValTok{6992}\NormalTok{,}
                 \DecValTok{5517}\NormalTok{,}\DecValTok{7879}\NormalTok{,}\DecValTok{8990}\NormalTok{,}\DecValTok{6495}\NormalTok{,}\DecValTok{6662}\NormalTok{,}\DecValTok{5981}\NormalTok{,}\DecValTok{7322}\NormalTok{,}\DecValTok{6529}\NormalTok{,}\DecValTok{7192}\NormalTok{,}\DecValTok{7953}\NormalTok{,}
                 \DecValTok{7636}\NormalTok{,}\DecValTok{9001}\NormalTok{,}\DecValTok{6450}\NormalTok{,}\DecValTok{8973}\NormalTok{,}\DecValTok{7302}\NormalTok{,}\DecValTok{7521}\NormalTok{,}\DecValTok{7268}\NormalTok{,}\DecValTok{7013}\NormalTok{,}\DecValTok{6025}\NormalTok{,}\DecValTok{7717}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that \texttt{pop} and \texttt{ghealth} and \texttt{wards} contains characters.

\hypertarget{creating-a-data-frame}{%
\subsection{Creating A Data Frame}\label{creating-a-data-frame}}

We can create a data frame and examine its structure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(wards, pop, ghealth)}
\NormalTok{df }\CommentTok{# or use view(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                       wards   pop ghealth
## 1  Allerton and Hunts Cross 14853    7274
## 2                   Anfield 14510    6124
## 3                Belle Vale 15004    6129
## 4                   Central 20340   11925
## 5                 Childwall 13908    7219
## 6                    Church 13974    7461
## 7                  Clubmoor 15272    6403
## 8                    County 14045    5930
## 9               Cressington 14503    7094
## 10                 Croxteth 14561    6992
## 11                  Everton 14782    5517
## 12               Fazakerley 16786    7879
## 13                Greenbank 16132    8990
## 14 Kensington and Fairfield 15377    6495
## 15                 Kirkdale 16115    6662
## 16               Knotty Ash 13312    5981
## 17             Mossley Hill 13816    7322
## 18             Norris Green 15047    6529
## 19                 Old Swan 16461    7192
## 20                   Picton 17009    7953
## 21             Princes Park 17104    7636
## 22                Riverside 18422    9001
## 23             St Michael's 12991    6450
## 24            Speke-Garston 20300    8973
## 25 Tuebrook and Stoneycroft 16489    7302
## 26                 Warbreck 16481    7521
## 27                Wavertree 14772    7268
## 28               West Derby 14382    7013
## 29                  Woolton 12921    6025
## 30                 Yew Tree 16746    7717
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(df) }\CommentTok{# or use glimpse(data) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    30 obs. of  3 variables:
##  $ wards  : Factor w/ 30 levels "Allerton and Hunts Cross",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ pop    : num  14853 14510 15004 20340 13908 ...
##  $ ghealth: num  7274 6124 6129 11925 7219 ...
\end{verbatim}

\hypertarget{referencing-data-frames}{%
\subsection{Referencing Data Frames}\label{referencing-data-frames}}

Throughout this module, you will need to refer to particular parts of a dataframe - perhaps a particular column (an area attribute); or a particular subset of respondents. Hence it is worth spending some time now mastering this particular skill.

The relevant R function, \texttt{{[}\ {]}}, has the format \texttt{{[}row,col{]}} or, more generally, \texttt{{[}set\ of\ rows,\ set\ of\ cols{]}}.

Run the following commands to get a feel of how to extract different slices of the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\CommentTok{# whole data.frame}
\NormalTok{df[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] }\CommentTok{# contents of first row and column}
\NormalTok{df[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{# contents of the second row, second and third columns}
\NormalTok{df[}\DecValTok{1}\NormalTok{, ] }\CommentTok{# first row, ALL columns [the default if no columns specified]}
\NormalTok{df[ ,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{] }\CommentTok{# ALL rows; first and second columns}
\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{), ] }\CommentTok{# rows 1,3,5; ALL columns}
\NormalTok{df[ , }\DecValTok{2}\NormalTok{] }\CommentTok{# ALL rows; second column (by default results containing only }
             \CommentTok{#one column are converted back into a vector)}
\NormalTok{df[ , }\DecValTok{2}\NormalTok{, drop=}\OtherTok{FALSE}\NormalTok{] }\CommentTok{# ALL rows; second column (returned as a data.frame)}
\end{Highlighting}
\end{Shaded}

In the above, note that we have used two other R functions:

\begin{itemize}
\item
  \texttt{1:3} The colon operator tells R to produce a list of numbers including the named start and end points.
\item
  \texttt{c(1,3,5)} Tells R to combine the contents within the brackets into one list of objects
\end{itemize}

Run both of these fuctions on their own to get a better understanding of what they do.

Three other methods for referencing the contents of a data.frame make direct use of the variable names within the data.frame, which tends to make for easier to read/understand code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[,}\StringTok{"pop"}\NormalTok{] }\CommentTok{# variable name in quotes inside the square brackets}
\NormalTok{df}\OperatorTok{$}\NormalTok{pop }\CommentTok{# variable name prefixed with $ and appended to the data.frame name}
\CommentTok{# or you can use attach}
\KeywordTok{attach}\NormalTok{(df)}
\NormalTok{pop }\CommentTok{# but be careful if you already have an age variable in your local workspace}
\end{Highlighting}
\end{Shaded}

Want to check the variables available, use the \texttt{names(\ )}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "wards"   "pop"     "ghealth"
\end{verbatim}

\hypertarget{sec_readdata}{%
\section{Read Data}\label{sec_readdata}}

Ensure your memory is clear

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{()) }\CommentTok{# rm for targeted deletion / ls for listing all existing objects}
\end{Highlighting}
\end{Shaded}

There are many commands to read / load data onto R. The command to use will depend upon the format they have been saved. Normally they are saved in \emph{csv} format from Excel or other software packages. So we use either:

\begin{itemize}
\tightlist
\item
  \texttt{df\ \textless{}-\ read.table("path/file\_name.csv",\ header\ =\ FALSE,\ sep\ =",")}
\item
  \texttt{df\ \textless{}-\ read("path/file\_name.csv",\ header\ =\ FALSE)}
\item
  \texttt{df\ \textless{}-\ read.csv2("path/file\_name.csv",\ header\ =\ FALSE)}
\end{itemize}

To read files in other formats, refer to this useful \href{https://www.datacamp.com/community/tutorials/r-data-import-tutorial?utm_source=adwords_ppc\&utm_campaignid=1655852085\&utm_adgroupid=61045434382\&utm_device=c\&utm_keyword=\%2Bread\%20\%2Bdata\%20\%2Br\&utm_matchtype=b\&utm_network=g\&utm_adpostion=1t1\&utm_creative=318880582308\&utm_targetid=kwd-309793905111\&utm_loc_interest_ms=\&utm_loc_physical_ms=9046551\&gclid=CjwKCAiA3uDwBRBFEiwA1VsajJO0QK0Jg7VipIt8_t82qQrnUliI0syAlh8CIxnE76Rb0kh3FbiehxoCzCgQAvD_BwE\#csv}{DataCamp tutorial}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/census/census_data.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        code                     ward pop16_74 higher_managerial   pop
## 1 E05000886 Allerton and Hunts Cross    10930              1103 14853
## 2 E05000887                  Anfield    10712               312 14510
## 3 E05000888               Belle Vale    10987               432 15004
## 4 E05000889                  Central    19174              1346 20340
## 5 E05000890                Childwall    10410              1123 13908
## 6 E05000891                   Church    10569              1843 13974
##   ghealth
## 1    7274
## 2    6124
## 3    6129
## 4   11925
## 5    7219
## 6    7461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }\AlertTok{NOTE}\CommentTok{: always ensure your are setting the correct directory leading to the data. }
\CommentTok{# It may differ from your existing working directory}
\end{Highlighting}
\end{Shaded}

\hypertarget{quickly-inspect-the-data}{%
\subsection{Quickly inspect the data}\label{quickly-inspect-the-data}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What class?
\item
  What R data types?
\item
  What data types?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1}
\KeywordTok{class}\NormalTok{(census)}
\CommentTok{# 2 & 3}
\KeywordTok{str}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

Just interested in the variable names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "code"              "ward"              "pop16_74"         
## [4] "higher_managerial" "pop"               "ghealth"
\end{verbatim}

or want to view the data:

\texttt{View(census)}

\hypertarget{manipulation-data}{%
\section{Manipulation Data}\label{manipulation-data}}

\hypertarget{adding-new-variables}{%
\subsection{Adding New Variables}\label{adding-new-variables}}

Usually you want to add / create new variables to your data frame using existing variables eg. computing percentages by dividing two variables. There are many ways in which you can do this i.e.~referecing a data frame as we have done above, or using \texttt{\$} (e.g.~\texttt{census\$pop}). For this module, we'll use \texttt{tidyverse}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{per_ghealth =}\NormalTok{ ghealth }\OperatorTok{/}\StringTok{ }\NormalTok{pop)}
\end{Highlighting}
\end{Shaded}

Note we used a \emph{pipe operator} \texttt{\%\textgreater{}\%}, which helps make the code more efficient and readable - more details, see \citet{grolemund_wickham_2019_book}. When using the pipe operator, recall to first indicate the data frame before \texttt{\%\textgreater{}\%}.

Note also the use a variable name before the \texttt{=} sign in brackets to indicate the name of the new variable after \texttt{mutate}.

\hypertarget{selecting-variables}{%
\subsection{Selecting Variables}\label{selecting-variables}}

Usually you want to select a subset of variables for your analysis as storing to large data sets in your R memory can reduce the processing speed of your machine. A selection of data can be achieved by using the \texttt{select} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndf <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(ward, pop16_}\DecValTok{74}\NormalTok{, per_ghealth)}
\end{Highlighting}
\end{Shaded}

Again first indicate the data frame and then the variable you want to select to build a new data frame. Note the code chunk above has created a new data frame called \texttt{ndf}. Explore it.

\hypertarget{filtering-data}{%
\subsection{Filtering Data}\label{filtering-data}}

You may also want to filter values based on defined conditions. You may want to filter observations greater than a certain threshold or only areas within a certain region. For example, you may want to select areas with a percentage of good health population over 50\%:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndf2 <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(per_ghealth }\OperatorTok{<}\StringTok{ }\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can use more than one variables to set conditions. Use ``\texttt{,}'' to add a condition.

\hypertarget{joining-data-drames}{%
\subsection{Joining Data Drames}\label{joining-data-drames}}

When working with spatial data, we often need to join data. To this end, you need a common unique \texttt{id\ variable}. Let's say, we want to add a data frame containing census data on households for Liverpool, and join the new attributes to one of the existing data frames in the workspace. First we will read the data frame we want to join (ie. \texttt{census\_data2.csv}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{census2 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/census/census_data2.csv"}\NormalTok{)}
\CommentTok{# visualise data structure}
\KeywordTok{str}\NormalTok{(census2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    30 obs. of  3 variables:
##  $ geo_code               : Factor w/ 30 levels "E05000886","E05000887",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ households             : int  6359 6622 6622 7139 5391 5884 6576 6745 6317 6024 ...
##  $ socialrented_households: int  827 1508 2818 1311 374 178 2859 1564 1023 1558 ...
\end{verbatim}

The variable \texttt{geo\_code} in this data frame corresponds to the \texttt{code} in the existing data frame and they are unique so they can be automatically matched by using the \texttt{merge()} function. The \texttt{merge()} function uses two arguments: \texttt{x} and \texttt{y}. The former refers to data frame 1 and the latter to data frame 2. Both of these two data frames must have a \texttt{id} variable containing the same information. Note they can have different names. Another key argument to include is \texttt{all.x=TRUE} which tells the function to keep all the records in \texttt{x}, but only those in \texttt{y} that match in case there are discrepancies in the \texttt{id} variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join data frames}
\NormalTok{join_dfs <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(census, census2, }\DataTypeTok{by.x=}\StringTok{"code"}\NormalTok{, }\DataTypeTok{by.y=}\StringTok{"geo_code"}\NormalTok{, }\DataTypeTok{all.x =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# check data}
\KeywordTok{head}\NormalTok{(join_dfs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        code                     ward pop16_74 higher_managerial   pop
## 1 E05000886 Allerton and Hunts Cross    10930              1103 14853
## 2 E05000887                  Anfield    10712               312 14510
## 3 E05000888               Belle Vale    10987               432 15004
## 4 E05000889                  Central    19174              1346 20340
## 5 E05000890                Childwall    10410              1123 13908
## 6 E05000891                   Church    10569              1843 13974
##   ghealth per_ghealth households socialrented_households
## 1    7274   0.4897327       6359                     827
## 2    6124   0.4220538       6622                    1508
## 3    6129   0.4084911       6622                    2818
## 4   11925   0.5862832       7139                    1311
## 5    7219   0.5190538       5391                     374
## 6    7461   0.5339201       5884                     178
\end{verbatim}

\hypertarget{saving-data}{%
\subsection{Saving Data}\label{saving-data}}

It may also be convinient to save your R projects. They contains all the objects that you have created in your workspace by using the \texttt{save.image(\ )} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{save.image}\NormalTok{(}\StringTok{"week1_envs453.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This creates a file labelled ``week1\_envs453.RData'' in your working directory. You can load this at a later stage using the \texttt{load(\ )} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"week1_envs453.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively you can save / export your data into a \texttt{csv} file. The first argument in the function is the object name, and the second: the name of the csv we want to create.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(join_dfs, }\StringTok{"join_censusdfs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-spatial-data-frames}{%
\section{Using Spatial Data Frames}\label{using-spatial-data-frames}}

A core area of this module is learning to work with spatial data in R. R has various purposedly designed \textbf{packages} for manipulation of spatial data and spatial analysis techniques. Various R packages exist in CRAN eg. \texttt{spatial}, \texttt{sgeostat}, \texttt{splancs}, \texttt{maptools}, \texttt{tmap}, \texttt{rgdal}, \texttt{spand} and more recent development of \texttt{sf} - see \citet{Lovelace_et_al_2020_book} for a great description and historical context for some of these packages.

During this session, we will use \texttt{sf}.

We first need to import our spatial data. We will use a shapefile containing data at Output Area (OA) level for Liverpool. These data illustrates the hierarchical structure of spatial data.

\hypertarget{read-spatial-data}{%
\subsection{Read Spatial Data}\label{read-spatial-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/census/Liverpool_OA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `Liverpool_OA' from data source `/home/jovyan/work/data/census/Liverpool_OA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 1584 features and 18 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs
\end{verbatim}

Examine the input data. A spatial data frame stores a range of attributes derived from a shapefile including the \textbf{geometry} of features (e.g.~polygon shape and location), \textbf{attributes} for each feature (stored in the .dbf), \href{https://en.wikipedia.org/wiki/Map_projection}{projection} and coordinates of the shapefile's bounding box - for details, execute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?st_read}
\end{Highlighting}
\end{Shaded}

You can employ the usual functions to visualise the content of the created data frame:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# visualise variable names}
\KeywordTok{names}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "OA_CD"    "LSOA_CD"  "MSOA_CD"  "LAD_CD"   "pop"      "H_Vbad"  
##  [7] "H_bad"    "H_fair"   "H_good"   "H_Vgood"  "age_men"  "age_med" 
## [13] "age_60"   "S_Rent"   "Ethnic"   "illness"  "unemp"    "males"   
## [19] "geometry"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# data structure}
\KeywordTok{str}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'sf' and 'data.frame':   1584 obs. of  19 variables:
##  $ OA_CD   : Factor w/ 1584 levels "E00032987","E00032988",..: 1547 485 143 1567 980 1186 1122 889 863 433 ...
##  $ LSOA_CD : Factor w/ 298 levels "E01006512","E01006513",..: 291 96 33 124 187 231 220 175 173 83 ...
##  $ MSOA_CD : Factor w/ 61 levels "E02001347","E02001348",..: 59 12 19 23 29 20 31 14 30 10 ...
##  $ LAD_CD  : Factor w/ 1 level "E08000012": 1 1 1 1 1 1 1 1 1 1 ...
##  $ pop     : int  185 281 208 200 321 187 395 320 316 214 ...
##  $ H_Vbad  : int  1 2 3 7 4 4 5 9 5 4 ...
##  $ H_bad   : int  2 20 10 8 10 25 19 22 25 17 ...
##  $ H_fair  : int  9 47 22 17 32 70 42 53 55 39 ...
##  $ H_good  : int  53 111 71 52 112 57 131 104 104 53 ...
##  $ H_Vgood : int  120 101 102 116 163 31 198 132 127 101 ...
##  $ age_men : num  27.9 37.7 37.1 33.7 34.2 ...
##  $ age_med : num  25 36 32 29 34 53 23 30 34 29 ...
##  $ age_60  : num  0.0108 0.1637 0.1971 0.1 0.1402 ...
##  $ S_Rent  : num  0.0526 0.176 0.0235 0.2222 0.0222 ...
##  $ Ethnic  : num  0.3514 0.0463 0.0192 0.215 0.0779 ...
##  $ illness : int  185 281 208 200 321 187 395 320 316 214 ...
##  $ unemp   : num  0.0438 0.121 0.1121 0.036 0.0743 ...
##  $ males   : int  122 128 95 120 158 123 207 164 157 94 ...
##  $ geometry:sfc_MULTIPOLYGON of length 1584; first list element: List of 1
##   ..$ :List of 1
##   .. ..$ : num [1:14, 1:2] 335106 335130 335164 335173 335185 ...
##   ..- attr(*, "class")= chr  "XY" "MULTIPOLYGON" "sfg"
##  - attr(*, "sf_column")= chr "geometry"
##  - attr(*, "agr")= Factor w/ 3 levels "constant","aggregate",..: NA NA NA NA NA NA NA NA NA NA ...
##   ..- attr(*, "names")= chr  "OA_CD" "LSOA_CD" "MSOA_CD" "LAD_CD" ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# see first few observations}
\KeywordTok{head}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 6 features and 18 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs
##       OA_CD   LSOA_CD   MSOA_CD    LAD_CD pop H_Vbad H_bad H_fair H_good
## 1 E00176737 E01033761 E02006932 E08000012 185      1     2      9     53
## 2 E00033515 E01006614 E02001358 E08000012 281      2    20     47    111
## 3 E00033141 E01006546 E02001365 E08000012 208      3    10     22     71
## 4 E00176757 E01006646 E02001369 E08000012 200      7     8     17     52
## 5 E00034050 E01006712 E02001375 E08000012 321      4    10     32    112
## 6 E00034280 E01006761 E02001366 E08000012 187      4    25     70     57
##   H_Vgood  age_men age_med     age_60     S_Rent     Ethnic illness
## 1     120 27.94054      25 0.01081081 0.05263158 0.35135135     185
## 2     101 37.71174      36 0.16370107 0.17600000 0.04626335     281
## 3     102 37.08173      32 0.19711538 0.02352941 0.01923077     208
## 4     116 33.73000      29 0.10000000 0.22222222 0.21500000     200
## 5     163 34.19003      34 0.14018692 0.02222222 0.07788162     321
## 6      31 56.09091      53 0.44919786 0.88524590 0.11764706     187
##        unemp males                       geometry
## 1 0.04379562   122 MULTIPOLYGON (((335106.3 38...
## 2 0.12101911   128 MULTIPOLYGON (((335810.5 39...
## 3 0.11214953    95 MULTIPOLYGON (((336738 3931...
## 4 0.03597122   120 MULTIPOLYGON (((335914.5 39...
## 5 0.07428571   158 MULTIPOLYGON (((339325 3914...
## 6 0.44615385   123 MULTIPOLYGON (((338198.1 39...
\end{verbatim}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What are the geographical hierarchy in these data?
\item
  What is the smallest geography?
\item
  What is the largest geography?
\end{itemize}

\hypertarget{basic-mapping}{%
\subsection{Basic Mapping}\label{basic-mapping}}

Again, many functions exist in CRAN for creating maps:

\begin{itemize}
\tightlist
\item
  \texttt{plot} to create static maps
\item
  \texttt{tmap} to create static and interactive maps
\item
  \texttt{leaflet} to create interactive maps
\item
  \texttt{mapview} to create interactive maps
\item
  \texttt{ggplot2} to create data visualisations, including static maps
\item
  \texttt{shiny} to create web applications, including maps
\end{itemize}

Here this notebook demonstrates the use of \texttt{plot} and \texttt{tmap}. First \texttt{plot} is used to map the spatial distribution of non-British-born population in Liverpool. First we only map the geometries on the right,

\hypertarget{using-plot}{%
\subsubsection{\texorpdfstring{Using \texttt{plot}}{Using plot}}\label{using-plot}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# mapping geometry}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(oa_shp))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-40-1.pdf}
\caption{\label{fig:unnamed-chunk-40}OAs of Livepool}
\end{figure}

and then:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# map attributes, adding intervals}
\KeywordTok{plot}\NormalTok{(oa_shp[}\StringTok{"Ethnic"}\NormalTok{], }\DataTypeTok{key.pos =} \DecValTok{4}\NormalTok{, }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{key.width =} \KeywordTok{lcm}\NormalTok{(}\FloatTok{1.3}\NormalTok{), }\DataTypeTok{key.length =} \FloatTok{1.}\NormalTok{,}
     \DataTypeTok{breaks =} \StringTok{"jenks"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{border =} \StringTok{'grey'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-41-1.pdf}
\caption{\label{fig:unnamed-chunk-41}Spatial distribution of ethnic groups, Liverpool}
\end{figure}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What is the key pattern emerging from this map?
\end{itemize}

\hypertarget{using-tmap}{%
\subsubsection{\texorpdfstring{Using \texttt{tmap}}{Using tmap}}\label{using-tmap}}

Similar to \texttt{ggplot2}, \texttt{tmap} is based on the idea of a `grammar of graphics' which involves a separation between the input data and aesthetics (i.e.~the way data are visualised). Each data set can be mapped in various different ways, including location as defined by its geometry, colour and other features. The basic building block is \texttt{tm\_shape()} (which defines input data), followed by one or more layer elements such as \texttt{tm\_fill()} and \texttt{tm\_dots()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{oa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(oa_shp)}

\CommentTok{# map}
\NormalTok{legend_title =}\StringTok{ }\KeywordTok{expression}\NormalTok{(}\StringTok{"% ethnic pop."}\NormalTok{)}
\NormalTok{map_oa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(oa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"Ethnic"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# add fill}
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{.01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }\CommentTok{# add borders}
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# add compass}
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\CommentTok{# add scale bar}
\NormalTok{map_oa}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-intro_files/figure-latex/unnamed-chunk-42-1.pdf}

Note that the operation \texttt{+} is used to add new layers. You can set style themes by \texttt{tm\_style}. To visualise the existing styles use \texttt{tmap\_style\_catalogue()}, and you can also evaluate the code chunk below if you would like to create an interactive map.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tmap_mode}\NormalTok{(}\StringTok{"view"}\NormalTok{)}
\NormalTok{map_oa}
\end{Highlighting}
\end{Shaded}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  Try mapping other variables in the spatial data frame. Where do population aged 60 and over concentrate?
\end{itemize}

\hypertarget{comparing-geographies}{%
\subsection{Comparing geographies}\label{comparing-geographies}}

If you recall, one of the key issues of working with spatial data is the modifiable area unit problem (MAUP) - see lecture notes. To get a sense of the effects of MAUP, we analyse differences in the spatial patterns of the ethnic population in Liverpool between Middle Layer Super Output Areas (MSOAs) and OAs. So we map these geographies together.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data at the msoa level}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/census/Liverpool_MSOA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `Liverpool_MSOA' from data source `/home/jovyan/work/data/census/Liverpool_MSOA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 61 features and 16 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{msoa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(msoa_shp)}

\CommentTok{# create a map}
\NormalTok{map_msoa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(msoa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"Ethnic"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{.01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }

\CommentTok{# arrange maps }
\KeywordTok{tmap_arrange}\NormalTok{(map_msoa, map_oa) }
\end{Highlighting}
\end{Shaded}

\includegraphics{01-intro_files/figure-latex/unnamed-chunk-44-1.pdf}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What differences do you see between OAs and MSOAs?
\item
  Can you identify areas of spatial clustering? Where are they?
\end{itemize}

\hypertarget{useful-functions}{%
\section{Useful Functions}\label{useful-functions}}

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description\tabularnewline
\midrule
\endhead
read.csv() & read csv files into data frames\tabularnewline
str() & inspect data structure\tabularnewline
mutate() & create a new variable\tabularnewline
filter() & filter observations based on variable values\tabularnewline
\%\textgreater\% & pipe operator - chain operations\tabularnewline
select() & select variables\tabularnewline
merge() & join dat frames\tabularnewline
st\_read & read spatial data (ie. shapefiles)\tabularnewline
plot() & create a map based a spatial data set\tabularnewline
tm\_shape(), tm\_fill(), tm\_borders() & create a map using tmap functions\tabularnewline
tm\_arrange & display multiple maps in a single ``metaplot''\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{points}{%
\chapter{Points}\label{points}}

This chapter\footnote{This chapter is part of \href{index.html}{Spatial Analysis Notes} {Points -- Kernel Density Estimation and Spatial interpolation} by Dani Arribas-Bel is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.} is based on the following references, which are great follow-up's on the topic:

\begin{itemize}
\tightlist
\item
  \citet{lovelace2014introduction} is a great introduction.
\item
  Chapter 6 of \citet{comber2015}, in particular subsections 6.3 and 6.7.
\item
  \citet{bivand2013applied} provides an in-depth treatment of spatial data in R.
\end{itemize}

This chapter is part of \href{index.html}{Spatial Analysis Notes}, a compilation hosted as a GitHub repository that you can access it in a few ways:

\begin{itemize}
\tightlist
\item
  As a \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of a \texttt{.zip} file that contains all the materials.
\item
  As an \href{https://gdsl-ul.github.io/san/points.html}{html
  website}.
\item
  As a \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\hypertarget{dependencies-1}{%
\section{Dependencies}\label{dependencies-1}}

This tutorial relies on the following libraries that you will need to have installed on your machine to be able to interactively follow along\footnote{You can install package \texttt{mypackage} by running the command \texttt{install.packages("mypackage")} on the R prompt or through the \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in RStudio.}. Once installed, load them up with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Layout}
\KeywordTok{library}\NormalTok{(tufte)}
\CommentTok{# For pretty table}
\KeywordTok{library}\NormalTok{(knitr)}
\CommentTok{# Spatial Data management}
\KeywordTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## rgdal: version: 1.4-4, (SVN revision 833)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20
##  Path to GDAL shared files: /usr/share/gdal/2.2
##  GDAL binary built with GEOS: TRUE 
##  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]
##  Path to PROJ.4 shared files: (autodetected)
##  Linking to sp version: 1.3-1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pretty graphics}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap)}
\CommentTok{# Pretty maps}
\KeywordTok{library}\NormalTok{(ggmap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.
\end{verbatim}

\begin{verbatim}
## Please cite ggmap if you use it! See citation("ggmap") for details.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Various GIS utilities}
\KeywordTok{library}\NormalTok{(GISTools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: maptools
\end{verbatim}

\begin{verbatim}
## Checking rgeos availability: TRUE
\end{verbatim}

\begin{verbatim}
## Loading required package: RColorBrewer
\end{verbatim}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## Loading required package: rgeos
\end{verbatim}

\begin{verbatim}
## rgeos version: 0.5-1, (SVN revision 614)
##  GEOS runtime version: 3.6.2-CAPI-1.10.2 
##  Linking to sp version: 1.3-1 
##  Polygon checking: TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For all your interpolation needs}
\KeywordTok{library}\NormalTok{(gstat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Registered S3 method overwritten by 'xts':
##   method     from
##   as.zoo.xts zoo
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For data manipulation}
\KeywordTok{library}\NormalTok{(plyr)}
\end{Highlighting}
\end{Shaded}

Before we start any analysis, let us set the path to the directory where we are working. We can easily do that with \texttt{setwd()}. Please replace in the following line the path to the folder where you have placed this file -and where the \texttt{house\_transactions} folder with the data lives.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd('/media/dani/baul/AAA/Documents/teaching/u-lvl/2016/envs453/code')}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'.'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\section{Data}\label{data}}

For this session, we will use a subset of residential property transaction data for the city of Liverpool. These are provided by the Land Registry (as part of their \href{https://www.gov.uk/government/collections/price-paid-data}{Price Paid Data}) but have been cleaned and re-packaged by Dani Arribas-Bel.

Let us start by reading the data, which comes in a shapefile:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\DataTypeTok{dsn =} \StringTok{'data/house_transactions'}\NormalTok{, }\DataTypeTok{layer =} \StringTok{'liv_house_trans'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/home/jovyan/work/data/house_transactions", layer: "liv_house_trans"
## with 6324 features
## It has 18 fields
## Integer64 fields read as strings:  price
\end{verbatim}

Before we forget, let us make sure \texttt{price} is considered a number, not a factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{((db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price)))}
\end{Highlighting}
\end{Shaded}

The dataset spans the year 2014:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Format dates}
\NormalTok{dts <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trans_date)}
\CommentTok{# Set up summary table}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(dts)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Min.      1st Qu.       Median         Mean      3rd Qu. 
## "2014-01-02" "2014-04-11" "2014-07-09" "2014-07-08" "2014-10-03" 
##         Max. 
## "2014-12-30"
\end{verbatim}

We can then examine the elements of the object with the \texttt{summary} method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class SpatialPointsDataFrame
## Coordinates:
##              min    max
## coords.x1 333536 345449
## coords.x2 382684 397833
## Is projected: TRUE 
## proj4string :
## [+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
## +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy
## +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894]
## Number of points: 6324
## Data attributes:
##       pcds                                           id      
##  L1 6LS : 126   {00029226-80EF-4280-9809-109B8509656A}:   1  
##  L8 5TE :  63   {00041BD2-4A07-4D41-A5AE-6459CD5FD37C}:   1  
##  L1 5AQ :  34   {0005AE67-9150-41D4-8D56-6BFC868EECA3}:   1  
##  L24 1WA:  31   {00183CD7-EE48-434B-8A1A-C94B30A93691}:   1  
##  L17 6BT:  26   {003EA3A5-F804-458D-A66F-447E27569456}:   1  
##  L3 1EE :  24   {00411304-DD5B-4F11-9748-93789D6A000E}:   1  
##  (Other):6020   (Other)                               :6318  
##      price                     trans_date   type     new      duration
##  Min.   :    1000   2014-06-27 00:00: 109   D: 505   N:5495   F:3927  
##  1st Qu.:   70000   2014-12-19 00:00: 109   F:1371   Y: 829   L:2397  
##  Median :  110000   2014-02-28 00:00: 105   O: 119                    
##  Mean   :  144310   2014-10-31 00:00:  95   S:1478                    
##  3rd Qu.:  160000   2014-03-28 00:00:  94   T:2851                    
##  Max.   :26615720   2014-11-28 00:00:  94                             
##                     (Other)         :5718                             
##       paon               saon                   street    
##  3      : 203   FLAT 2     :  25   CROSSHALL STREET: 133  
##  11     : 151   FLAT 3     :  25   STANHOPE STREET :  63  
##  14     : 148   FLAT 1     :  24   PALL MALL       :  47  
##  5      : 146   APARTMENT 4:  23   DUKE STREET     :  41  
##  4      : 140   APARTMENT 2:  21   MANN ISLAND     :  41  
##  8      : 128   (Other)    : 893   OLD HALL STREET :  39  
##  (Other):5408   NA's       :5313   (Other)         :5960  
##          locality           town           district           county    
##  WAVERTREE   : 126   LIVERPOOL:6324   KNOWSLEY :  12   MERSEYSIDE:6324  
##  MOSSLEY HILL: 102                    LIVERPOOL:6311                    
##  WALTON      :  88                    WIRRAL   :   1                    
##  WEST DERBY  :  71                                                      
##  WOOLTON     :  66                                                      
##  (Other)     : 548                                                      
##  NA's        :5323                                                      
##  ppd_cat  status         lsoa11          LSOA11CD   
##  A:5393   A:6324   E01033762: 144   E01033762: 144  
##  B: 931            E01033756:  98   E01033756:  98  
##                    E01033752:  93   E01033752:  93  
##                    E01033750:  71   E01033750:  71  
##                    E01006518:  68   E01006518:  68  
##                    E01033755:  65   E01033755:  65  
##                    (Other)  :5785   (Other)  :5785
\end{verbatim}

See how it contains several pieces, some relating to the spatial information, some relating to the tabular data attached to it. We can access each of the separately if we need it. For example, to pull out the names of the columns in the \texttt{data.frame}, we can use the \texttt{@data} appendix:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colnames}\NormalTok{(db}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "pcds"       "id"         "price"      "trans_date" "type"      
##  [6] "new"        "duration"   "paon"       "saon"       "street"    
## [11] "locality"   "town"       "district"   "county"     "ppd_cat"   
## [16] "status"     "lsoa11"     "LSOA11CD"
\end{verbatim}

The rest of this session will focus on two main elements of the shapefile: the spatial dimension (as stored in the point coordinates), and the house price values contained in the \texttt{price} column. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the histogram}
\NormalTok{hist <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{db}\OperatorTok{@}\NormalTok{data,}\DataTypeTok{x=}\NormalTok{price)}
\NormalTok{hist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-8-1.pdf}
\caption{\label{fig:unnamed-chunk-8}Raw house prices in Liverpool}
\end{figure}

This basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to \emph{shrink} these differences is to take logarithms:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create log and add it to the table}
\NormalTok{logpr <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price))}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'logpr'}\NormalTok{] <-}\StringTok{ }\NormalTok{logpr}
\CommentTok{# Create the histogram}
\NormalTok{hist <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{logpr)}
\NormalTok{hist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-9-1.pdf}
\caption{\label{fig:unnamed-chunk-9}Log of house price in Liverpool}
\end{figure}

To obtain the spatial distribution of these houses, we need to turn away from the \texttt{@data} component of \texttt{db}. The easiest, quickest (and also dirtiest) way to get a sense of what the data look like over space is using \texttt{plot}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-10-1.pdf}
\caption{\label{fig:unnamed-chunk-10}Spatial distribution of house transactions in Liverpool}
\end{figure}

\hypertarget{kde}{%
\section{KDE}\label{kde}}

Kernel Density Estimation (KDE) is a technique that creates a \emph{continuous} representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions.

\hypertarget{one-dimensional-kde}{%
\subsection{One-dimensional KDE}\label{one-dimensional-kde}}

KDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the base}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(db}\OperatorTok{@}\NormalTok{data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{logpr))}
\CommentTok{# Histogram}
\NormalTok{hist <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins=}\DecValTok{50}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{..density..))}
\CommentTok{# Overlay density plot}
\NormalTok{kde <-}\StringTok{ }\NormalTok{hist }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"#FF6666"}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"#FF6666"}\NormalTok{)}
\NormalTok{kde}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Histogram and KDE of the log of house prices in Liverpool}
\end{figure}

The key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts.

\hypertarget{two-dimensional-spatial-kde}{%
\subsection{Two-dimensional (spatial) KDE}\label{two-dimensional-spatial-kde}}

Geography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates, \texttt{X} and \texttt{Y} (or latitude and longitude). Thanks to that, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a \emph{surface}, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above.

To create a spatial KDE in R, there are several ways. If you do not want to necessarily acknowledge the spatial nature of your data, or you they are not stored in a spatial format, you can plot them using \texttt{ggplot2}. Note we first need to convert the coordinates (stored in the spatial part of \texttt{db}) into columns of X and Y coordinates, then we can plot them:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Attach XY coordinates}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'X'}\NormalTok{] <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{1}\NormalTok{]}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'Y'}\NormalTok{] <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{2}\NormalTok{]}
\CommentTok{# Set up base layer}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{db}\OperatorTok{@}\NormalTok{data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X, }\DataTypeTok{y=}\NormalTok{Y))}
\CommentTok{# Create the KDE surface}
\NormalTok{kde <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{stat_density2d}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y, }\DataTypeTok{alpha =}\NormalTok{ ..level..), }
               \DataTypeTok{size =} \FloatTok{0.01}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{16}\NormalTok{, }\DataTypeTok{geom =} \StringTok{'polygon'}\NormalTok{) }\OperatorTok{+}
\StringTok{            }\KeywordTok{scale_fill_gradient}\NormalTok{()}
\NormalTok{kde}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-12-1.pdf}
\caption{\label{fig:unnamed-chunk-12}KDE of house transactions in Liverpool}
\end{figure}

Or, we can use a package such as the \texttt{GISTools}, which allows to pass a spatial object directly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute the KDE}
\NormalTok{kde <-}\StringTok{ }\KeywordTok{kde.points}\NormalTok{(db)}
\CommentTok{# Plot the KDE}
\KeywordTok{level.plot}\NormalTok{(kde)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-13-1.pdf}
\caption{\label{fig:unnamed-chunk-13}KDE of house transactions in Liverpool}
\end{figure}

Either of these approaches generate a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. To do it, we will leverage the library \texttt{ggmap}, which is designed to play nicely with the \texttt{ggplot2} family (hence the seemingly counterintuitive example above). Before we can plot them with the online map, we need to reproject them though.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Reproject coordinates}
\NormalTok{wgs84 <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"}\NormalTok{)}
\NormalTok{db_wgs84 <-}\StringTok{ }\KeywordTok{spTransform}\NormalTok{(db, wgs84)}
\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\StringTok{'lon'}\NormalTok{] <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{1}\NormalTok{]}
\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\StringTok{'lat'}\NormalTok{] <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{2}\NormalTok{]}
\NormalTok{xys <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\KeywordTok{c}\NormalTok{(}\StringTok{'lon'}\NormalTok{, }\StringTok{'lat'}\NormalTok{)]}
\CommentTok{# Bounding box}
\NormalTok{liv <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left =} \KeywordTok{min}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lon), }\DataTypeTok{bottom =} \KeywordTok{min}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lat), }
         \DataTypeTok{right =} \KeywordTok{max}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lon), }\DataTypeTok{top =} \KeywordTok{max}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lat))}
\CommentTok{# Download map tiles}
\NormalTok{basemap <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(liv, }\DataTypeTok{zoom =} \DecValTok{12}\NormalTok{, }
                         \DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1327.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1327.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1327.png
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Overlay KDE}
\NormalTok{final <-}\StringTok{ }\KeywordTok{ggmap}\NormalTok{(basemap, }\DataTypeTok{extent =} \StringTok{"device"}\NormalTok{, }
               \DataTypeTok{maprange=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_density2d}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ db_wgs84}\OperatorTok{@}\NormalTok{data, }
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ lon, }\DataTypeTok{y =}\NormalTok{ lat, }
                    \DataTypeTok{alpha=}\NormalTok{..level.., }
                    \DataTypeTok{fill =}\NormalTok{ ..level..), }
                \DataTypeTok{size =} \FloatTok{0.01}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{16}\NormalTok{, }
                \DataTypeTok{geom =} \StringTok{'polygon'}\NormalTok{, }
                \DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_gradient2}\NormalTok{(}\StringTok{"Transaction}\CharTok{\textbackslash{}n}\StringTok{Density"}\NormalTok{, }
                       \DataTypeTok{low =} \StringTok{"#fffff8"}\NormalTok{, }
                       \DataTypeTok{high =} \StringTok{"#8da0cb"}\NormalTok{)}
\NormalTok{final}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}KDE of house transactions in Liverpool}
\end{figure}

The plot above\footnote{\textbf{EXERCISE} The map above uses the Stamen map \texttt{toner-lite}. Explore additional tile styles on their \href{http://maps.stamen.com/\#watercolor/12/37.7706/-122.3782}{website} and try to recreate the plot above.} allows us to not only see the distribution of house transactions, but to relate it to what we know about Liverpool, allowing us to establish many more connections than we were previously able. Mainly, we can easily see that the area with a highest volume of houses being sold is the city centre, with a ``hole'' around it that displays very few to no transactions and then several pockets further away.

\hypertarget{spatial-interpolation}{%
\section{Spatial Interpolation}\label{spatial-interpolation}}

The previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house transactions, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example).

Many times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house price transactions, fits into this category as well: although where house are sold may be of relevance, more often we are interested in finding out what the ``surface of price'' looks like. Rather than \emph{where are most houses being sold?} we usually want to know \emph{where the most expensive or most affordable} houses are located.

In cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is \emph{spatial interpolation}, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g.~temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible\footnote{There is also an important economic incentive to do this: some of the most popular applications are in the oil and gas or mining industries. In fact, the very creator of this technique, \href{https://en.wikipedia.org/wiki/Danie_G._Krige}{Danie G. Krige}, was a mining engineer. His name is usually used to nickname spatial interpolation as \emph{kriging}.}. In this session, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, \citet{banerjee2014hierarchical} or \citet{cressie2015statistics} are hard but good overviews.

\hypertarget{inverse-distance-weight-idw-interpolation}{%
\subsection{Inverse Distance Weight (IDW) interpolation}\label{inverse-distance-weight-idw-interpolation}}

The technique we will cover here is called \emph{Inverse Distance Weighting}, or IDW for convenience. \citet{comber2015} offer a good description:

\begin{quote}
In the \emph{inverse distance weighting} (IDW) approach to interpolation, to estimate the value of \(z\) at location \(x\) a weighted mean of nearby observations is taken {[}\ldots{]}. To accommodate the idea that observations of \(z\) at points closer to \(x\) should be given more importance in the interpolation, greater weight is given to these points {[}\ldots{]}

--- Page 204
\end{quote}

The math\footnote{Essentially, for any point \(x\) in space, the IDW estimate for value \(z\) is equivalent to \(\hat{z} (x) = \dfrac{\sum_i w_i z_i}{\sum_i w_i}\) where \(i\) are the observations for which we do have a value, and \(w_i\) is a weight given to location \(i\) based on its distance to \(x\).} is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. Essentially, the idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing.

Enough chat, let's code.

From what we have just mentioned, there are a few steps to perform an IDW spatial interpolation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a regular grid over the area where we have house transactions.
\item
  Obtain IDW estimates for each point in the grid, based on the values of \(k\) nearest neighbors.
\item
  Plot a smoothed version of the grid, effectively representing the surface of house prices.
\end{enumerate}

Let us go in detail into each of them\footnote{For the relevant calculations, we will be using the \texttt{gstat} library.}. First, let us set up a grid:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liv.grid <-}\StringTok{ }\KeywordTok{spsample}\NormalTok{(db, }\DataTypeTok{type=}\StringTok{'regular'}\NormalTok{, }\DataTypeTok{n=}\DecValTok{25000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That's it, we're done! The function \texttt{spsample} hugely simplifies the task by taking a spatial object and returning the grid we need. Not a couple of additional arguments we pass: \texttt{type} allows us to get a set of points that are \emph{uniformly} distributed over space, which is handy for the later smoothing; \texttt{n} controls how many points we want to create in that grid.

On to the IDW. Again, this is hugely simplified by \texttt{gstat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idw.hp <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{liv.grid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [inverse distance weighted interpolation]
\end{verbatim}

Boom! We've got it. Let us pause for a second to see how we just did it. First, we pass \texttt{price\ \textasciitilde{}\ 1}. This specifies the formula we are using to model house prices. The name on the left of \texttt{\textasciitilde{}} represents the variable we want to explain, while everything to its right captures the \emph{explanatory} variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply write \texttt{1}. Then we specify the original locations for which we do have house prices (our original \texttt{db} object), and the points where we want to interpolate the house prices (the \texttt{liv.grid} object we just created above). One more note: by default, \texttt{idw.hp} uses all the available observations, weighted by distance, to provide an estimate for a given point. If you want to modify that and restrict the maximum number of neighbors to consider, you need to tweak the argument \texttt{nmax}, as we do above by using the 150 neares observations to each point\footnote{Have a play with this because the results do change significantly. Can you reason why?}.

The object we get from \texttt{idw} is another spatial table, just as \texttt{db}, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(idw.hp}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   var1.pred var1.var
## 1  158068.9       NA
## 2  158179.1       NA
## 3  158292.0       NA
## 4  158407.7       NA
## 5  158526.5       NA
## 6  158648.3       NA
\end{verbatim}

The column we will pay attention to is \texttt{var1.pred}. And to see the locations for which those correspond:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(idw.hp}\OperatorTok{@}\NormalTok{coords)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            x1       x2
## [1,] 333578.3 382690.2
## [2,] 333663.3 382690.2
## [3,] 333748.2 382690.2
## [4,] 333833.2 382690.2
## [5,] 333918.2 382690.2
## [6,] 334003.1 382690.2
\end{verbatim}

So, for a hypothetical house sold at the location in the first row of \texttt{idw.hp@coords} (expressed in the OSGB coordinate system), the price we would guess it would cost, based on the price of houses sold nearby, is the first element of column \texttt{var1.pred} in \texttt{idw.hp@data}.

\hypertarget{a-surface-of-housing-prices}{%
\subsection{A surface of housing prices}\label{a-surface-of-housing-prices}}

Once we have the IDW object computed, we can plot it to explore the distribution, not of house transactions in this case, but of house price over the geography of Liverpool. The easiest way to do this is by quickly calling the command \texttt{spplot}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{spplot}\NormalTok{(idw.hp[}\StringTok{'var1.pred'}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-19-1.pdf}

However, this is not entirely satisfactory for a number of reasons. Let us get an equivalen plot with the package \texttt{tmap}, which streamlines some of this and makes more aesthetically pleasant maps easier to build as it follows a ``ggplot-y'' approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load up the layer}
\NormalTok{liv.otl <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\StringTok{'data/house_transactions'}\NormalTok{, }\StringTok{'liv_outline'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/home/jovyan/work/data/house_transactions", layer: "liv_outline"
## with 1 features
## It has 1 fields
\end{verbatim}

The shape we will overlay looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qtm}\NormalTok{(liv.otl)}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-21-1.pdf}

Now let's give it a first go!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }
\NormalTok{p =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(liv.otl) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col=}\StringTok{'black'}\NormalTok{, }\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_shape}\NormalTok{(idw.hp) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_symbols}\NormalTok{(}\DataTypeTok{col=}\StringTok{'var1.pred'}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.25}\NormalTok{, }
             \DataTypeTok{border.lwd=}\FloatTok{0.}\NormalTok{, }\DataTypeTok{palette=}\StringTok{'YlGn'}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-22-1.pdf}

The last two plots, however, are not really a surface, but a representation of the points we have just estimated. To create a surface, we need to do an interim transformation to convert the spatial object \texttt{idw.hp} into a table that a ``surface plotter'' can understand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xyz <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{coordinates}\NormalTok{(idw.hp)[, }\DecValTok{1}\NormalTok{], }
                  \DataTypeTok{y=}\KeywordTok{coordinates}\NormalTok{(idw.hp)[, }\DecValTok{2}\NormalTok{], }
                  \DataTypeTok{z=}\NormalTok{idw.hp}\OperatorTok{$}\NormalTok{var1.pred)}
\end{Highlighting}
\end{Shaded}

Now we are ready to plot the surface as a contour:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_contour}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{z=}\NormalTok{z))}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-24-1.pdf}
\caption{\label{fig:unnamed-chunk-24}Contour of prices in Liverpool}
\end{figure}

Which can also be shown as a filled contour:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_raster}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{z))}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-25-1.pdf}

The problem here, when compared to the KDE above for example, is that a few values are extremely large:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\DataTypeTok{x=}\NormalTok{z, }\DataTypeTok{geom=}\StringTok{'density'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-26-1.pdf}
\caption{\label{fig:unnamed-chunk-26}Skewness of prices in Liverpool}
\end{figure}

Let us then take the logarithm before we plot the surface:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xyz[}\StringTok{'lz'}\NormalTok{] <-}\StringTok{ }\KeywordTok{log}\NormalTok{(xyz}\OperatorTok{$}\NormalTok{z)}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}
\StringTok{           }\KeywordTok{geom_raster}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{lz),}
                       \DataTypeTok{show.legend =}\NormalTok{ F)}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-27-1.pdf}
\caption{\label{fig:unnamed-chunk-27}Surface of log-prices in Liverpool}
\end{figure}

Now this looks better. We can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because \texttt{ggmap} --the library we have been using to display tiles from cloud providers-- does not play well with our own rasters (i.e.~the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now. However, developments such as the \href{https://github.com/edzer/sfr}{\texttt{sf}} project promise to make this easier in the future\footnote{\textbf{BONUS} if you can figure out a way to do it yourself!}.

\hypertarget{what-should-the-next-houses-price-be}{%
\subsection{\texorpdfstring{\emph{``What should the next house's price be?''}}{``What should the next house's price be?''}}\label{what-should-the-next-houses-price-be}}

The last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location.

Let us say, the house is located on the coordinates \texttt{x=340000,\ y=390000} as expressed in the GB National Grid coordinate system. In that case, we can do as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt <-}\StringTok{ }\KeywordTok{SpatialPoints}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x=}\DecValTok{340000}\NormalTok{, }\DataTypeTok{y=}\DecValTok{390000}\NormalTok{),}
                    \DataTypeTok{proj4string =}\NormalTok{ db}\OperatorTok{@}\NormalTok{proj4string)}
\NormalTok{idw.one <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [inverse distance weighted interpolation]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idw.one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class       : SpatialPointsDataFrame 
## features    : 1 
## extent      : 340000, 340000, 390000, 390000  (xmin, xmax, ymin, ymax)
## crs         : +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894 
## variables   : 2
## names       :        var1.pred, var1.var 
## value       : 157099.029513871,       NA
\end{verbatim}

And, as show above, the estimated value is GBP157,099\footnote{\textbf{PRO QUESTION} Is that house expensive or cheap, as compared to the other houses sold in this dataset? Can you figure out where the house is?}.

Using this predictive logic, and taking advantage of Google Maps and its geocoding capabilities, it is possible to devise a function that takes an arbitrary address in Liverpool and, based on the transactions occurred throughout 2014, provides an estimate of what the price for a property in that location could be.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{how.much.is <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(address, }\DataTypeTok{print.message=}\OtherTok{TRUE}\NormalTok{)\{}
  \CommentTok{# Convert the address into Lon/Lat coordinates}
  \CommentTok{# }\AlertTok{NOTE}\CommentTok{: this now requires an API key}
  \CommentTok{# https://github.com/dkahle/ggmap#google-maps-and-credentials}
\NormalTok{  ll.pt <-}\StringTok{ }\KeywordTok{geocode}\NormalTok{(address)}
  \CommentTok{# Process as spatial table}
\NormalTok{  wgs84 <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"}\NormalTok{)}
\NormalTok{  ll.pt <-}\StringTok{ }\KeywordTok{SpatialPoints}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x=}\NormalTok{ll.pt}\OperatorTok{$}\NormalTok{lon, }\DataTypeTok{y=}\NormalTok{ll.pt}\OperatorTok{$}\NormalTok{lat),}
                      \DataTypeTok{proj4string =}\NormalTok{ wgs84)}
  \CommentTok{# Transform Lon/Lat into OSGB}
\NormalTok{  pt <-}\StringTok{ }\KeywordTok{spTransform}\NormalTok{(ll.pt, db}\OperatorTok{@}\NormalTok{proj4string)}
  \CommentTok{# Obtain prediction}
\NormalTok{  idw.one <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{pt)}
\NormalTok{  price <-}\StringTok{ }\NormalTok{idw.one}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{var1.pred}
  \CommentTok{# Return predicted price}
  \ControlFlowTok{if}\NormalTok{(print.message}\OperatorTok{==}\NormalTok{T)\{}
    \KeywordTok{writeLines}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{Based on what surrounding properties were sold"}\NormalTok{,}
                    \StringTok{"for in 2014 a house located at"}\NormalTok{, address, }\StringTok{"would"}\NormalTok{, }
                    \StringTok{"cost"}\NormalTok{,  }\KeywordTok{paste}\NormalTok{(}\StringTok{"GBP"}\NormalTok{, }\KeywordTok{round}\NormalTok{(price), }\StringTok{"."}\NormalTok{, }\DataTypeTok{sep=}\StringTok{''}\NormalTok{), }\StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{))}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(price)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Ready to test!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{address <-}\StringTok{ "74 Bedford St S, Liverpool, L69 7ZT, UK"}
\CommentTok{#p <- how.much.is(address)}
\end{Highlighting}
\end{Shaded}

\hypertarget{flows}{%
\chapter{Flows}\label{flows}}

This chapter\footnote{This chapter is part of \href{index.html}{Spatial Analysis Notes} {Flows -- Exploring flows visually and through spatial interaction} by Dani Arribas-Bel is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.} covers spatial interaction flows. Using open data from the city of San Francisco about trips on its bikeshare system, we will estimate spatial interaction models that try to capture and explain the variation in the amount of trips on each given route. After visualizing the dataset, we begin with a very simple model and then build complexity progressively by augmenting it with more information, refined measurements, and better modeling approaches. Throughout the chapter, we explore different ways to grasp the predictive performance of each model. We finish with a prediction example that illustrates how these models can be deployed in a real-world application.

Content is based on the following references, which are great follow-up's on the topic:

\begin{itemize}
\tightlist
\item
  \citet{gds_ua17}, an online short course on R for Geographic Data Science and Urban Analytics. In particular, the section on \href{https://github.com/alexsingleton/GDS_UA_2017/tree/master/Mapping_Flows}{mapping flows} is specially relevant here.
\item
  The predictive checks section draws heavily from \citet{gelman2006data}, in particular Chapters 6 and 7.
\end{itemize}

This tutorial is part of \href{index.html}{Spatial Analysis Notes}, a compilation hosted as a GitHub repository that you can access in a few ways:

\begin{itemize}
\tightlist
\item
  As a \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of a \texttt{.zip} file that contains all the materials.
\item
  As an \href{https://gdsl-ul.github.io/san/flows.html}{html
  website}.
\item
  As a \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\hypertarget{dependencies-2}{%
\section{Dependencies}\label{dependencies-2}}

This tutorial relies on the following libraries that you will need to have installed on your machine to be able to interactively follow along\footnote{You can install package \texttt{mypackage} by running the command \texttt{install.packages("mypackage")} on the R prompt or through the \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in RStudio.}. Once installed, load them up with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Layout}
\KeywordTok{library}\NormalTok{(tufte)}
\CommentTok{# Spatial Data management}
\KeywordTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## rgdal: version: 1.4-4, (SVN revision 833)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20
##  Path to GDAL shared files: /usr/share/gdal/2.2
##  GDAL binary built with GEOS: TRUE 
##  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]
##  Path to PROJ.4 shared files: (autodetected)
##  Linking to sp version: 1.3-1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pretty graphics}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap)}
\CommentTok{# Pretty maps}
\KeywordTok{library}\NormalTok{(ggmap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.
\end{verbatim}

\begin{verbatim}
## Please cite ggmap if you use it! See citation("ggmap") for details.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Simulation methods}
\KeywordTok{library}\NormalTok{(arm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
## Loading required package: lme4
\end{verbatim}

\begin{verbatim}
## 
## arm (Version 1.10-1, built: 2018-4-12)
\end{verbatim}

\begin{verbatim}
## Working directory is /home/jovyan/work
\end{verbatim}

Before we start any analysis, let us set the path to the directory where we are working. We can easily do that with \texttt{setwd()}. Please replace in the following line the path to the folder where you have placed this file -and where the \texttt{sf\_bikes} folder with the data lives.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{'.'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-1}{%
\section{Data}\label{data-1}}

In this note, we will use data from the city of San Francisco representing bike trips on their public bike share system. The original source is the SF Open Data portal (\href{http://www.bayareabikeshare.com/open-data}{link}) and the dataset comprises both the location of each station in the Bay Area as well as information on trips (station of origin to station of destination) undertaken in the system from September 2014 to August 2015 and the following year. Since this note is about modeling and not data preparation, a cleanly reshaped version of the data, together with some additional information, has been created and placed in the \texttt{sf\_bikes} folder. The data file is named \texttt{flows.geojson} and, in case you are interested, the (Python) code required to created from the original files in the SF Data Portal is also available on the \texttt{flows\_prep.ipynb} notebook \href{https://github.com/darribas/spa_notes/blob/master/sf_bikes/flows_prep.ipynb}{{[}url{]}}, also in the same folder.

Let us then directly load the file with all the information necessary:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\StringTok{'./data/sf_bikes/flows.geojson'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: GeoJSON 
## Source: "/home/jovyan/work/data/sf_bikes/flows.geojson", layer: "flows"
## with 1722 features
## It has 9 fields
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rownames}\NormalTok{(db}\OperatorTok{@}\NormalTok{data) <-}\StringTok{ }\NormalTok{db}\OperatorTok{$}\NormalTok{flow_id}
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{flow_id <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

Note how the interface is slightly different since we are reading a \texttt{GeoJSON} file instead of a shapefile.

The data contains the geometries of the flows, as calculated from the \href{https://developers.google.com/maps/}{Google Maps API}, as well as a series of columns with characteristics of each flow:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(db}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       dest orig straight_dist street_dist total_down total_up trips15
## 39-41   41   39      1452.201   1804.1150  11.205753 4.698162      68
## 39-42   42   39      1734.861   2069.1557  10.290236 2.897886      23
## 39-45   45   39      1255.349   1747.9928  11.015596 4.593927      83
## 39-46   46   39      1323.303   1490.8361   3.511543 5.038044     258
## 39-47   47   39       715.689    769.9189   0.000000 3.282495     127
## 39-48   48   39      1996.778   2740.1290  11.375186 3.841296      81
##       trips16
## 39-41      68
## 39-42      29
## 39-45      50
## 39-46     163
## 39-47      73
## 39-48      56
\end{verbatim}

where \texttt{orig} and \texttt{dest} are the station IDs of the origin and destination, \texttt{street/straight\_dist} is the distance in metres between stations measured along the street network or as-the-crow-flies, \texttt{total\_down/up} is the total downhil and climb in the trip, and \texttt{tripsXX} contains the amount of trips undertaken in the years of study.

\hypertarget{seeing-flows}{%
\section{\texorpdfstring{``\emph{Seeing}'' flows}{``Seeing'' flows}}\label{seeing-flows}}

The easiest way to get a quick preview of what the data looks like spatially is to make a simple plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-5-1.pdf}
\caption{\label{fig:unnamed-chunk-5}Potential routes}
\end{figure}

Equally, if we want to visualize a single route, we can simply subset the table. For example, to get the shape of the trip from station \texttt{39} to station \texttt{48}, we can:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one39to48 <-}\StringTok{ }\NormalTok{db[ }\KeywordTok{which}\NormalTok{(}
\NormalTok{          db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{orig }\OperatorTok{==}\StringTok{ }\DecValTok{39} \OperatorTok{&}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{dest }\OperatorTok{==}\StringTok{ }\DecValTok{48}
\NormalTok{          ) , ]}
\KeywordTok{plot}\NormalTok{(one39to48)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-6-1.pdf}
\caption{\label{fig:unnamed-chunk-6}Trip from station 39 to 48}
\end{figure}

or, for the most popular route, we can:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{most_pop <-}\StringTok{ }\NormalTok{db[ }\KeywordTok{which}\NormalTok{(}
\NormalTok{          db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15 }\OperatorTok{==}\StringTok{ }\KeywordTok{max}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15)}
\NormalTok{          ) , ]}
\KeywordTok{plot}\NormalTok{(most_pop)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-7-1.pdf}
\caption{\label{fig:unnamed-chunk-7}Most popular trip}
\end{figure}

These however do not reveal a lot: there is no geographical context (\emph{why are there so many routes along the NE?}) and no sense of how volumes of bikers are allocated along different routes. Let us fix those two.

The easiest way to bring in geographical context is by overlaying the routes on top of a background map of tiles downloaded from the internet. Let us download this using \texttt{ggmap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf_bb <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'x'}\NormalTok{, }\StringTok{'min'}\NormalTok{],}
           \DataTypeTok{right=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'x'}\NormalTok{, }\StringTok{'max'}\NormalTok{],}
           \DataTypeTok{bottom=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'y'}\NormalTok{, }\StringTok{'min'}\NormalTok{],}
           \DataTypeTok{top=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'y'}\NormalTok{, }\StringTok{'max'}\NormalTok{])}
\NormalTok{SanFran <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(sf_bb, }
                         \DataTypeTok{zoom =} \DecValTok{14}\NormalTok{, }
                         \DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6333.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6333.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6333.png
\end{verbatim}

and make sure it looks like we intend it to look:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-9-1.pdf}

Now to combine tiles and routes, we need to pull out the coordinates that make up each line. For the route example above, this would be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xys1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{coordinates}\NormalTok{(most_pop))}
\end{Highlighting}
\end{Shaded}

Now we can plot the route\footnote{\textbf{EXERCISE}: \emph{can you plot the route for the largest climb?}} (note we also dim down the background to focus the attention on flows):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X1, }\DataTypeTok{y=}\NormalTok{X2), }
            \DataTypeTok{data=}\NormalTok{xys1,}
            \DataTypeTok{size=}\DecValTok{1}\NormalTok{,}
            \DataTypeTok{color=}\KeywordTok{rgb}\NormalTok{(}\FloatTok{0.996078431372549}\NormalTok{, }\FloatTok{0.7019607843137254}\NormalTok{, }\FloatTok{0.03137254901960784}\NormalTok{),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-11-1.pdf}

Now we can plot all of the lines by using a short \texttt{for} loop to build up the table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up shell data.frame}
\NormalTok{lines <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{lat =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{lon =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{trips =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                    \DataTypeTok{id =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{                    )}
\CommentTok{# Run loop}
\ControlFlowTok{for}\NormalTok{(x }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(db))\{}
  \CommentTok{# Pull out row}
\NormalTok{  r <-}\StringTok{ }\NormalTok{db[x, ]}
  \CommentTok{# Extract lon/lat coords}
\NormalTok{  xys <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{coordinates}\NormalTok{(r))}
  \KeywordTok{names}\NormalTok{(xys) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'lon'}\NormalTok{, }\StringTok{'lat'}\NormalTok{)}
  \CommentTok{# Insert trips and id}
\NormalTok{  xys[}\StringTok{'trips'}\NormalTok{] <-}\StringTok{ }\NormalTok{r}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15}
\NormalTok{  xys[}\StringTok{'id'}\NormalTok{] <-}\StringTok{ }\NormalTok{x}
  \CommentTok{# Append them to `lines`}
\NormalTok{  lines <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(lines, xys)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can go on and plot all of them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.75}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}
                \DataTypeTok{x=}\NormalTok{lon, }
                \DataTypeTok{y=}\NormalTok{lat,}
                \DataTypeTok{group=}\NormalTok{id}
\NormalTok{                ),}
            \DataTypeTok{data=}\NormalTok{lines,}
            \DataTypeTok{size=}\FloatTok{0.1}\NormalTok{,}
            \DataTypeTok{color=}\KeywordTok{rgb}\NormalTok{(}\FloatTok{0.996078431372549}\NormalTok{, }\FloatTok{0.7019607843137254}\NormalTok{, }\FloatTok{0.03137254901960784}\NormalTok{),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-13-1.pdf}

Finally, we can get a sense of the distribution of the flows by associating a color gradient to each flow based on its number of trips:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.75}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}
                \DataTypeTok{x=}\NormalTok{lon, }
                \DataTypeTok{y=}\NormalTok{lat,}
                \DataTypeTok{group=}\NormalTok{id,}
                \DataTypeTok{colour=}\NormalTok{trips}
\NormalTok{                ),}
            \DataTypeTok{data=}\NormalTok{lines,}
            \DataTypeTok{size=}\KeywordTok{log1p}\NormalTok{(lines}\OperatorTok{$}\NormalTok{trips }\OperatorTok{/}\StringTok{ }\KeywordTok{max}\NormalTok{(lines}\OperatorTok{$}\NormalTok{trips)),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_gradient}\NormalTok{(}\DataTypeTok{low=}\StringTok{'grey'}\NormalTok{,}
                        \DataTypeTok{high=}\StringTok{'#07eda0'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_blank}\NormalTok{(),}
      \DataTypeTok{axis.text.y =} \KeywordTok{element_blank}\NormalTok{(),}
      \DataTypeTok{axis.ticks =} \KeywordTok{element_blank}\NormalTok{()}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-14-1.pdf}

Note how we transform the size so it's a proportion of the largest trip and then it is compressed with a logarithm.

\hypertarget{modelling-flows}{%
\section{Modelling flows}\label{modelling-flows}}

Now we have an idea of the spatial distribution of flows, we can begin to think about modeling them. The core idea in this section is to fit a model that can capture the particular characteristics of our variable of interest (the volume of trips) using a set of predictors that describe the nature of a given flow. We will start from the simplest model and then progressively build complexity until we get to a satisfying point. Along the way, we will be exploring each model using concepts from \citet{gelman2006data} such as predictive performance checks\footnote{For a more elaborate introduction to PPC, have a look at Chapters 7 and 8.} (PPC)

Before we start running regressions, let us first standardize the predictors so we can interpret the intercept as the average flow when all the predictors take the average value, and so we can interpret the model coefficients as changes in standard deviation units:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale all the table}
\NormalTok{db_std <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(db}\OperatorTok{@}\NormalTok{data))}
\CommentTok{# Reset trips as we want the original version}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{trips15 <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{trips16 <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips16}
\CommentTok{# Reset origin and destination station and express them as factors}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{orig <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{orig)}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{dest <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{dest)}
\end{Highlighting}
\end{Shaded}

\textbf{Baseline model}

One of the simplest possible models we can fit in this context is a linear model that explains the number of trips as a function of the straight distance between the two stations and total amount of climb and downhill. We will take this as the baseline on which we can further build later:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down'}\NormalTok{, }\DataTypeTok{data=}\NormalTok{db_std)}
\KeywordTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "trips15 ~ straight_dist + total_up + total_down", 
##     data = db_std)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -261.9 -168.3 -102.4   30.8 3527.4 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    182.070      8.110  22.451  < 2e-16 ***
## straight_dist   17.906      9.108   1.966   0.0495 *  
## total_up       -44.100      9.353  -4.715 2.61e-06 ***
## total_down     -20.241      9.229  -2.193   0.0284 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 336.5 on 1718 degrees of freedom
## Multiple R-squared:  0.02196,    Adjusted R-squared:  0.02025 
## F-statistic: 12.86 on 3 and 1718 DF,  p-value: 2.625e-08
\end{verbatim}

To explore how good this model is, we will be comparing the predictions the model makes about the number of trips each flow should have with the actual number of trips. A first approach is to simply plot the distribution of both variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check, point estimates - Baseline model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-17-1.pdf}

The plot makes pretty obvious that our initial model captures very few aspects of the distribution we want to explain. However, we should not get too attached to this plot just yet. What it is showing is the distribution of predicted \emph{point} estimates from our model. Since our model is not deterministic but inferential, there is a certain degree of uncertainty attached to its predictions, and that is completely absent from this plot.

Generally speaking, a given model has two sources of uncertainty: \emph{predictive}, and \emph{inferential}. The former relates to the fact that the equation we fit does not capture all the elements or in the exact form they enter the true data generating process; the latter has to do with the fact that we never get to know the true value of the model parameters only guesses (estimates) subject to error and uncertainty. If you think of our linear model above as

\[
T_{ij} = X_{ij}\beta + \epsilon_{ij}
\]
where \(T_{ij}\) represents the number of trips undertaken between station \(i\) and \(j\), \(X_{ij}\) is the set of explanatory variables (length, climb, descent, etc.), and \(\epsilon_{ij}\) is an error term assumed to be distributed as a normal distribution \(N(0, \sigma)\); then predictive uncertainty comes from the fact that there are elements to some extent relevant for \(y\) that are not accounted for and thus subsummed into \(\epsilon_{ij}\). Inferential uncertainty comes from the fact that we never get to know \(\beta\) but only an estimate of it which is also subject to uncertainty itself.

Taking these two sources into consideration means that the black line in the plot above represents only the behaviour of our model we expect if the error term is absent (no predictive uncertainty) and the coefficients are the true estimates (no inferential uncertainty). However, this is not necessarily the case as our estimate for the uncertainty of the error term is certainly not zero, and our estimates for each parameter are also subject to a great deal of inferential variability. we do not know to what extent other outcomes would be just as likely. Predictive checking relates to simulating several feasible scenarios under our model and use those to assess uncertainty and to get a better grasp of the quality of our predictions.

Technically speaking, to do this, we need to build a mechanism to obtain a possible draw from our model and then repeat it several times. The first part of those two steps can be elegantly dealt with by writing a short function that takes a given model and a set of predictors, and produces a possible random draw from such model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate_draw <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m)\{}
  \CommentTok{# Set up predictors matrix}
\NormalTok{  x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(m)}
  \CommentTok{# Obtain draws of parameters (inferential uncertainty)}
\NormalTok{  sim_bs <-}\StringTok{ }\KeywordTok{sim}\NormalTok{(m, }\DecValTok{1}\NormalTok{)}
  \CommentTok{# Predicted value}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{x }\OperatorTok{%*%}\StringTok{ }\NormalTok{sim_bs}\OperatorTok{@}\NormalTok{coef[}\DecValTok{1}\NormalTok{, ]}
  \CommentTok{# Draw}
\NormalTok{  n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(mu)}
\NormalTok{  y_hat <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, mu, sim_bs}\OperatorTok{@}\NormalTok{sigma[}\DecValTok{1}\NormalTok{])}
  \KeywordTok{return}\NormalTok{(y_hat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function takes a model \texttt{m} and the set of covariates \texttt{x} used and returns a random realization of predictions from the model. To get a sense of how this works, we can get and plot a realization of the model, compared to the expected one and the actual values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_y <-}\StringTok{ }\KeywordTok{generate_draw}\NormalTok{(m1)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                      \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                      \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                      )}
\NormalTok{                   )}
\NormalTok{            ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(new_y), }
      \DataTypeTok{col=}\StringTok{'green'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{, }\StringTok{'Simulated'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{, }\StringTok{'green'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-19-1.pdf}

Once we have this ``draw engine'', we can set it to work as many times as we want using a simple \texttt{for} loop. In fact, we can directly plot these lines as compared to the expected one and the trip count:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{        ),}
     \DataTypeTok{col=}\StringTok{'white'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\CommentTok{# Loop for realizations}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{250}\NormalTok{)\{}
\NormalTok{  tmp_y <-}\StringTok{ }\KeywordTok{generate_draw}\NormalTok{(m1)}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(tmp_y),}
        \DataTypeTok{col=}\StringTok{'grey'}\NormalTok{,}
        \DataTypeTok{lwd=}\FloatTok{0.1}
\NormalTok{        )}
\NormalTok{\}}
\CommentTok{#}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
      \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Actual'}\NormalTok{, }\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Simulated (n=250)'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'red'}\NormalTok{, }\StringTok{'black'}\NormalTok{, }\StringTok{'grey'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Baseline model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-20-1.pdf}

The plot shows there is a significant mismatch between the fitted values, which are much more concentrated around small positive values, and the realizations of our ``inferential engine'', which depict a much less concentrated distribution of values. This is likely due to the combination of two different reasons: on the one hand, the accuracy of our estimates may be poor, causing them to jump around a wide range of potential values and hence resulting in very diverse predictions (inferential uncertainty); on the other hand, it may be that the amount of variation we are not able to account for in the model\footnote{The \(R^2\) of our model is around 2\%} is so large that the degree of uncertainty contained in the error term of the model is very large, hence resulting in such a flat predictive distribution.

It is important to keep in mind that the issues discussed in the paragraph above relate only to the uncertainty behind our model, not to the point predictions derived from them, which are a mechanistic result of the minimization of the squared residuals and hence are not subject to probability or inference. That allows them in this case to provide a fitted distribution much more accurate apparently (black line above). However, the lesson to take from this model is that, even if the point predictions (fitted values) are artificially accurate\footnote{which they are not really, in light of the comparison between the black and red lines.}, our capabilities to infer about the more general underlying process are fairly limited.

\textbf{Improving the model}

The bad news from the previous section is that our initial model is not great at explaining bike trips. The good news is there are several ways in which we can improve this. In this section we will cover three main extensions that exemplify three different routes you can take when enriching and augmenting models in general, and spatial interaction ones in particular\footnote{These principles are general and can be applied to pretty much any modeling exercise you run into. The specific approaches we take in this note relate to spatial interaction models}. These three routes are aligned around the following principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use better approximations to model your dependent variable.
\item
  Recognize the structure of your data.
\item
  Get better predictors.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Use better approximations to model your dependent variable}
\end{itemize}

Standard OLS regression assumes that the error term and, since the predictors are deterministic, the dependent variable are distributed following a normal (gaussian) distribution. This is usually a good approximation for several phenomena of interest, but maybe not the best one for trips along routes: for one, we know trips cannot be negative, which the normal distribution does not account for\footnote{For an illustration of this, consider the amount of probability mass to the left of zero in the predictive checks above.}; more subtly, their distribution is not really symmetric but skewed with a very long tail on the right. This is common in variables that represent counts and that is why usually it is more appropriate to fit a model that relies on a distribution different from the normal.

One of the most common distributions for this cases is the Poisson, which can be incorporated through a general linear model (or GLM). The underlying assumption here is that instead of \(T_{ij} \sim N(\mu_{ij}, \sigma)\), our model now follows:

\[
T_{ij} \sim Poisson (\exp^{X_{ij}\beta})
\]

As usual, such a model is easy to run in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson,}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

Now let's see how much better, if any, this approach is. To get a quick overview, we can simply plot the point predictions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check, point estimates - Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-22-1.pdf}

To incorporate uncertainty to these predictions, we need to tweak our \texttt{generate\_draw} function so it accommodates the fact that our model is not linear anymore.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate_draw_poi <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m)\{}
  \CommentTok{# Set up predictors matrix}
\NormalTok{  x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(m)}
  \CommentTok{# Obtain draws of parameters (inferential uncertainty)}
\NormalTok{  sim_bs <-}\StringTok{ }\KeywordTok{sim}\NormalTok{(m, }\DecValTok{1}\NormalTok{)}
  \CommentTok{# Predicted value}
\NormalTok{  xb <-}\StringTok{ }\NormalTok{x }\OperatorTok{%*%}\StringTok{ }\NormalTok{sim_bs}\OperatorTok{@}\NormalTok{coef[}\DecValTok{1}\NormalTok{, ]}
  \CommentTok{#xb <- x %*% m$coefficients}
  \CommentTok{# Transform using the link function}
\NormalTok{  mu <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(xb)}
  \CommentTok{# Obtain a random realization}
\NormalTok{  y_hat <-}\StringTok{ }\KeywordTok{rpois}\NormalTok{(}\DataTypeTok{n=}\KeywordTok{length}\NormalTok{(mu), }\DataTypeTok{lambda=}\NormalTok{mu)}
  \KeywordTok{return}\NormalTok{(y_hat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And then we can examine both point predictions an uncertainty around them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'white'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\CommentTok{# Loop for realizations}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{250}\NormalTok{)\{}
\NormalTok{  tmp_y <-}\StringTok{ }\KeywordTok{generate_draw_poi}\NormalTok{(m2)}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(tmp_y),}
        \DataTypeTok{col=}\StringTok{'grey'}\NormalTok{,}
        \DataTypeTok{lwd=}\FloatTok{0.1}
\NormalTok{        )}
\NormalTok{\}}
\CommentTok{#}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
      \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{, }\StringTok{'Simulated (n=250)'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{, }\StringTok{'grey'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-24-1.pdf}

Voila! Although the curve is still a bit off, centered too much to the right of the actual data, our predictive simulation leaves the fitted values right in the middle. This speaks to a better fit of the model to the actual distribution othe original data follow.

\begin{itemize}
\tightlist
\item
  \textbf{Recognize the structure of your data}
\end{itemize}

So far, we've treated our dataset as if it was flat (i.e.~comprise of fully independent realizations) when in fact it is not. Most crucially, our baseline model does not account for the fact that every observation in the dataset pertains to a trip between two stations. This means that all the trips from or to the same station probably share elements which likely help explain how many trips are undertaken between stations. For example, think of trips to an from a station located in the famous Embarcadero, a popular tourist spot. Every route to and from there probably has more trips due to the popularity of the area and we are currently not acknowledging it in the model.

A simple way to incorporate these effects into the model is through origin and destination fixed effects. This approach shares elements with both spatial fixed effects and multilevel modeling and essentially consists of including a binary variable for every origin and destination station. In mathematical notation, this equates to:

\[
T_{ij} = X_{ij}\beta + \delta_i + \delta_j + \epsilon_{ij}
\]

where \(\delta_i\) and \(\delta_j\) are origin and destination station fixed effects\footnote{In this session, \(\delta_i\) and \(\delta_j\) are estimated as independent variables so their estimates are similar to interpret to those in \(\beta\). An alternative approach could be to model them as random effects in a multilevel framework.}, and the rest is as above. This strategy accounts for all the unobserved heterogeneity associated with the location of the station. Technically speaking, we simply need to introduce \texttt{orig} and \texttt{dest} in the the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down + orig + dest'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson)}
\end{Highlighting}
\end{Shaded}

And with our new model, we can have a look at how well it does at predicting the overall number of trips\footnote{Although, theoretically, we could also include simulations of the model in the plot to get a better sense of the uncertainty behind our model, in practice this seems troublesome. The problems most likely arise from the fact that many of the origin and destination binary variable coefficients are estimated with a great deal of uncertainty. This causes some of the simulation to generate extreme values that, when passed through the exponential term of the Poisson link function, cause problems. If anything, this is testimony of how a simple fixed effect model can sometimes lack accuracy and generate very uncertain estimates. A potential extension to work around these problems could be to fit a multilevel model with two specific levels beyond the trip-level: one for origin and another one for destination stations.}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m3}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m3}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Orig/dest FE Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-26-1.pdf}

That looks significantly better, doesn't it? In fact, our model now better accounts for the long tail where a few routes take a lot of trips. This is likely because the distribution of trips is far from random across stations and our origin and destination fixed effects do a decent job at accounting for that structure. However our model is still notably underpredicting less popular routes and overpredicting routes with above average number of trips. Maybe we should think about moving beyond a simple linear model.

\begin{itemize}
\tightlist
\item
  \textbf{Get better predictors}
\end{itemize}

The final extension is, in principle, always available but, in practice, it can be tricky to implement. The core idea is that your baseline model might not have the best measurement of the phenomena you want to account for. In our example, we can think of the distance between stations. So far, we have been including the distance measured ``as the crow flies'' between stations. Although in some cases this is a good approximation (particularly when distances are long and likely route taken is as close to straight as possible), in some cases like ours, where the street layout and the presence of elevation probably matter more than the actual final distance pedalled, this is not necessarily a safe assumption.

As an exampe of this approach, we can replace the straight distance measurements for more refined ones based on the Google Maps API routes. This is very easy as all we need to do (once the distances have been calculated!) is to swap \texttt{straight\_dist} for \texttt{street\_dist}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m4 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ street_dist + total_up + total_down + orig + dest'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson)}
\end{Highlighting}
\end{Shaded}

And we can similarly get a sense of our predictive fitting with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m4}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m4}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Orig/dest FE Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-28-1.pdf}

Hard to tell any noticeable difference, right? To see if there is any, we can have a look at the estimates obtained:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m4)}\OperatorTok{$}\NormalTok{coefficients[}\StringTok{'street_dist'}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Estimate     Std. Error        z value       Pr(>|z|) 
##  -9.961619e-02   2.688731e-03  -3.704952e+01  1.828096e-300
\end{verbatim}

And compare this to that of the straight distances in the previous model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m3)}\OperatorTok{$}\NormalTok{coefficients[}\StringTok{'straight_dist'}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Estimate     Std. Error        z value       Pr(>|z|) 
##  -7.820014e-02   2.683052e-03  -2.914596e+01  9.399407e-187
\end{verbatim}

As we can see, the differences exist but ar not massive. Let's use this example to learn how to interpret coefficients in a Poisson model\footnote{See section 6.2 of \citet{gelman2006data} for a similar treatment of these.}. Effectively, these estimates can be understood as multiplicative effects. Since our model fits

\[
T_{ij} \sim Poisson (\exp^{X_{ij}\beta})
\]

we need to transform \(\beta\) through an exponential in order to get a sense of the effect of distance on the number of trips. This means that for the street distance, our original estimate is \(\beta_{street} = -0.0996\), but this needs to be translated through the exponential into \(e^{-0.0996} = 0.906\). In other words, since distance is expressed in standard deviations\footnote{Remember the transformation at the very beginning.}, we can expect a 10\% decrease in the number of trips for an increase of one standard deviation (about 1Km) in the distance between the stations. This can be compared with \(e^{-0.0782} = 0.925\) for the straight distances, or a reduction of about 8\% the number of trips for every increase of a standard deviation (about 720m).

\hypertarget{predicting-flows}{%
\section{Predicting flows}\label{predicting-flows}}

So far we have put all of our modeling efforts in understanding the model we fit and improving such model so it fits our data as closely as possible. This is essential in any modelling exercise but should be far from a stopping point. Once we're confident our model is a decent representation of the data generating process, we can start exploiting it. In this section, we will cover one specific case that showcases how a fitted model can help: out-of-sample forecasts.

It is August 2015, and you have just started working as a data scientist for the bikeshare company that runs the San Francisco system. You join them as they're planning for the next academic year and, in order to plan their operations (re-allocating vans, station maintenance, etc.), they need to get a sense of how many people are going to be pedalling across the city and, crucially, \emph{where} they are going to be pedalling through. What can you do to help them?

The easiest approach is to say ``well, a good guess for how many people will be going between two given stations this coming year is how many went through last year, isn't it?''. This is one prediction approach. However, you could see how, even if the same process governs over both datasets (2015 and 2016), each year will probably have some idiosyncracies and thus looking too closely into one year might not give the best possible answer for the next one. Ideally, you want a good stylized synthesis that captures the bits that stay constant over time and thus can be applied in the future and that ignores those aspects that are too particular to a given point in time. That is the rationale behind using a fitted model to obtain predictions.

However good any theory though, the truth is in the pudding. So, to see if a modeling approach is better at producing forecasts than just using the counts from last year, we can put them to a test. The way this is done when evaluating the predictive performance of a model (as this is called in the literature) relies on two basic steps: a) obtain predictions from a given model and b) compare those to the actual values (in our case, with the counts for 2016 in \texttt{trips16}) and get a sense of ``how off'' they are. We have essentially covered a) above; for b), there are several measures to use. We will use one of the most common ones, the root mean squared error (RMSE), which roughly gives a sense of the average difference between a predicted vector and the real deal:

\[
RMSE = \sqrt{ \sum_{ij} (\hat{T_{ij}} - T_{ij})^2}
\]

where \(\hat{T_{ij}}\) is the predicted amount of trips between stations \(i\) and \(j\). RMSE is straightforward in R and, since we will use it a couple of times, let's write a short function to make our lives easier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(t, p)\{}
\NormalTok{  se <-}\StringTok{ }\NormalTok{(t }\OperatorTok{-}\StringTok{ }\NormalTok{p)}\OperatorTok{^}\DecValTok{2}
\NormalTok{  mse <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(se)}
\NormalTok{  rmse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(mse)}
  \KeywordTok{return}\NormalTok{(rmse)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

where \texttt{t} stands for the vector of true values, and \texttt{p} is the vector of predictions. Let's give it a spin to make sure it works:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmse_m4 <-}\StringTok{ }\KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, m4}\OperatorTok{$}\NormalTok{fitted.values)}
\NormalTok{rmse_m4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 256.2197
\end{verbatim}

That means that, on average, predictions in our best model \texttt{m4} are 256 trips off. Is this good? Bad? Worse? It's hard to say but, being practical, what we can say is whether this better than our alternative. Let us have a look at the RMSE of the other models as well as that of simply plugging in last year's counts:\footnote{\textbf{EXERCISE}: can you create a single plot that displays the distribution of the predicted values of the five different ways to predict trips in 2016 and the actual counts of trips?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmses <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model=}\KeywordTok{c}\NormalTok{(}\StringTok{'OLS'}\NormalTok{, }\StringTok{'Poisson'}\NormalTok{, }\StringTok{'Poisson + FE'}\NormalTok{, }
                            \StringTok{'Poisson + FE + street dist.'}\NormalTok{,}
                            \StringTok{'Trips-2015'}
\NormalTok{                            ),}
                    \DataTypeTok{RMSE=}\KeywordTok{c}\NormalTok{(}\KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m1}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m2}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m3}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m4}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                                db_std}\OperatorTok{$}\NormalTok{trips15)}
\NormalTok{                           )}
\NormalTok{                      )}
\NormalTok{rmses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                         model     RMSE
## 1                         OLS 323.6135
## 2                     Poisson 320.8962
## 3                Poisson + FE 254.4468
## 4 Poisson + FE + street dist. 256.2197
## 5                  Trips-2015 131.0228
\end{verbatim}

The table is both encouraging and disheartning at the same time. On the one hand, all the modeling techniques covered above behave as we would expect: the baseline model displays the worst predicting power of all, and every improvement (except the street distances!) results in notable decreases of the RMSE. This is good news. However, on the other hand, all of our modelling efforts fall short of given a better guess than simply using the previous year's counts. \emph{Why? Does this mean that we should not pay attention to modeling and inference?} Not really. Generally speaking, a model is as good at predicting as it is able to mimic the underlying process that gave rise to the data in the first place. The results above point to a case where our model is not picking up all the factors that determine the amount of trips undertaken in a give route. This could be improved by enriching the model with more/better predictors, as we have seen above. Also, the example above seems to point to a case where those idiosyncracies in 2015 that the model does not pick up seem to be at work in 2016 as well. This is great news for our prediction efforts this time, but we have no idea why this is the case and, for all that matters, it could change the coming year. Besides the elegant quantification of uncertainty, the true advantage of a modeling approach in this context is that, if well fit, it is able to pick up the fundamentals that apply over and over. This means that, if next year we're not as lucky as this one and previous counts are not good predictors but the variables we used in our model continue to have a role in determining the outcome, the data scientist should be luckier and hit a better prediction.

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{spatial-econometrics}{%
\chapter{Spatial Econometrics}\label{spatial-econometrics}}

DA-B to fill in

\hypertarget{multilevel-models-pt.-i}{%
\chapter{Multilevel Models (Pt. I)}\label{multilevel-models-pt.-i}}

FR to fill in

\hypertarget{multilevel-models-pt.-ii}{%
\chapter{Multilevel Models (Pt. II)}\label{multilevel-models-pt.-ii}}

FR to fill in

\hypertarget{gwr}{%
\chapter{GWR}\label{gwr}}

FR

\hypertarget{space-time-analysis}{%
\chapter{Space-Time Analysis}\label{space-time-analysis}}

FR

\bibliography{book.bib,packages.bib}


\end{document}
