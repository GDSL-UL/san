\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Spatial Analysis Notes},
            pdfauthor={Francisco Rowe \& Dani Arribas-Bel},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{Spatial Analysis Notes}
\author{Francisco Rowe \& Dani Arribas-Bel}
\date{2020-03-27}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Spatial Analysis Notes}\label{spatial-analysis-notes}

This book contains computational illustrations on spatial analytical
approaches using R.

\chapter{Introduction}\label{intro}

This chapter\footnote{This chapter is part of \href{index.html}{Spatial
  Analysis Notes} {Introduction -- R Notebooks + Basic Functions + Data
  Types} by Francisco Rowe is licensed under a Creative Commons
  Attribution-NonCommercial-ShareAlike 4.0 International License.}
introduces R Notebooks, basic functions and data types. These are all
important concepts that we will use during the module.

If you are already familiar with R, R notebooks and data types, you may
want to jump to Section \protect\hyperlink{sec_readdata}{Read Data} and
start from there. This section describes how to read and manipulate data
using \texttt{sf} and \texttt{tidyverse} functions, including
\texttt{mutate()}, \texttt{\%\textgreater{}\%} (known as pipe operator),
\texttt{select()}, \texttt{filter()} and specific packages and functions
how to manipulate spatial data.

The chapter is based on:

\begin{itemize}
\item
  \citet{grolemund_wickham_2019_book}, this book illustrates key
  libraries, including tidyverse, and functions for data manipulation in
  R
\item
  \citet{Xie_et_al_2019_book}, excellent introduction to R markdown!
\item
  \citet{envs450_2018}, some examples from the first lecture of ENVS450
  are used to explain the various types of random variables.
\item
  \citet{Lovelace_et_al_2020_book}, a really good book on handling
  spatial data and historical background of the evolution of R packages
  for spatial data analysis.
\end{itemize}

\section{Dependencies}\label{dependencies}

This tutorial uses the libraries below. Ensure they are installed on
your machine\footnote{You can install package \texttt{mypackage} by
  running the command \texttt{install.packages("mypackage")} on the R
  prompt or through the
  \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in
  RStudio.} before loading them executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Data manipulation, transformation and visualisation}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{# Nice tables}
\KeywordTok{library}\NormalTok{(kableExtra)}
\CommentTok{# Simple features (a standardised way to encode vector data ie. points, lines, polygons)}
\KeywordTok{library}\NormalTok{(sf) }
\CommentTok{# Spatial objects conversion}
\KeywordTok{library}\NormalTok{(sp) }
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap) }
\CommentTok{# Colour palettes}
\KeywordTok{library}\NormalTok{(RColorBrewer) }
\CommentTok{# More colour palettes}
\KeywordTok{library}\NormalTok{(viridis)}
\end{Highlighting}
\end{Shaded}

\section{Introducing R}\label{introducing-r}

R is a freely available language and environment for statistical
computing and graphics which provides a wide variety of statistical and
graphical techniques. It has gained widespread use in academia and
industry. R offers a wider array of functionality than a traditional
statistics package, such as SPSS and is composed of core (base)
functionality, and is expandable through libraries hosted on
\href{https://cran.r-project.org}{CRAN}. CRAN is a network of ftp and
web servers around the world that store identical, up-to-date, versions
of code and documentation for R.

Commands are sent to R using either the terminal / command line or the R
Console which is installed with R on either Windows or OS X. On Linux,
there is no equivalent of the console, however, third party solutions
exist. On your own machine, R can be installed from
\href{https://www.r-project.org/}{here}.

Normally RStudio is used to implement R coding. RStudio is an integrated
development environment (IDE) for R and provides a more user-friendly
front-end to R than the front-end provided with R.

To run R or RStudio, just double click on the R or RStudio icon.
Throughout this module, we will be using RStudio:

\begin{figure}
\centering
\includegraphics{figs/ch2/rstudio_features.png}
\caption{Fig. 1. RStudio features.}
\end{figure}

If you would like to know more about the various features of RStudio,
watch this \href{https://rstudio.com/products/rstudio/}{video}

\section{Setting the working
directory}\label{setting-the-working-directory}

Before we start any analysis, ensure to set the path to the directory
where we are working. We can easily do that with \texttt{setwd()}.
Please replace in the following line the path to the folder where you
have placed this file -and where the \texttt{data} folder lives.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd('../data/sar.csv')}
\CommentTok{#setwd('.')}
\end{Highlighting}
\end{Shaded}

Note: It is good practice to not include spaces when naming folders and
files. Use \emph{underscores} or \emph{dots}.

You can check your current working directory by typing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san"
\end{verbatim}

\section{R Scripts and Notebooks}\label{r-scripts-and-notebooks}

An \emph{R script} is a series of commands that you can execute at one
time and help you save time. So you don't repeat the same steps every
time you want to execute the same process with different datasets. An R
script is just a plain text file with R commands in it.

To create an R script in RStudio, you need to

\begin{itemize}
\item
  Open a new script file: \emph{File} \textgreater{} \emph{New File}
  \textgreater{} \emph{R Script}
\item
  Write some code on your new script window by typing eg.
  \texttt{mtcars}
\item
  Run the script. Click anywhere on the line of code, then hit
  \emph{Ctrl + Enter} (Windows) or \emph{Cmd + Enter} (Mac) to run the
  command or select the code chunk and click \emph{run} on the right-top
  corner of your script window. If do that, you should get:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Save the script: \emph{File} \textgreater{} \emph{Save As}, select
  your required destination folder, and enter any filename that you
  like, provided that it ends with the file extension \emph{.R}
\end{itemize}

An \emph{R Notebook} is an R Markdown document with descriptive text and
code chunks that can be executed independently and interactively, with
output visible immediately beneath a code chunk - see
\citet{Xie_et_al_2019_book}.

To create an R Notebook, you need to:

\begin{itemize}
\tightlist
\item
  Open a new script file: \emph{File} \textgreater{} \emph{New File}
  \textgreater{} \emph{R Notebook}
\end{itemize}

\begin{figure}
\centering
\includegraphics{figs/ch2/rnotebook_yaml.png}
\caption{Fig. 2. YAML metadata for notebooks.}
\end{figure}

\begin{itemize}
\tightlist
\item
  Insert code chunks, either:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  use the \emph{Insert} command on the editor toolbar;
\item
  use the keyboard shortcut \emph{Ctrl + Alt + I} or \emph{Cmd + Option
  + I} (Mac); or,
\item
  type the chunk delimiters
  \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}} and
  \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{enumerate}

In a chunk code you can produce text output, tables, graphics and write
code! You can control these outputs via chunk options which are provided
inside the curly brackets eg.

\begin{figure}
\centering
\includegraphics{figs/ch2/codechunk.png}
\caption{Fig. 3. Code chunk example. Details on the various options:
\url{https://rmarkdown.rstudio.com/lesson-3.html}}
\end{figure}

\begin{itemize}
\item
  Execute code: hit \emph{``Run Current Chunk''}, \emph{Ctrl + Shift +
  Enter} or \emph{Cmd + Shift + Enter} (Mac)
\item
  Save an R notebook: \emph{File} \textgreater{} \emph{Save As}. A
  notebook has a \texttt{*.Rmd} extension and when it is saved a
  \texttt{*.nb.html} file is automatically created. The latter is a
  self-contained HTML file which contains both a rendered copy of the
  notebook with all current chunk outputs and a copy of the *.Rmd file
  itself.
\end{itemize}

Rstudio also offers a \emph{Preview} option on the toolbar which can be
used to create pdf, html and word versions of the notebook. To do this,
choose from the drop-down list menu \texttt{knit\ to\ ...}

\section{Getting Help}\label{getting-help}

You can use \texttt{help} or \texttt{?} to ask for details for a
specific function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(sqrt) }\CommentTok{#or ?sqrt}
\end{Highlighting}
\end{Shaded}

And using \texttt{example} provides examples for said function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{example}\NormalTok{(sqrt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## sqrt> require(stats) # for spline
## 
## sqrt> require(graphics)
## 
## sqrt> xx <- -9:9
## 
## sqrt> plot(xx, sqrt(abs(xx)),  col = "red")
\end{verbatim}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-7-1.pdf}
\caption{\label{fig:unnamed-chunk-7}Example sqrt}
\end{figure}

\begin{verbatim}
## 
## sqrt> lines(spline(xx, sqrt(abs(xx)), n=101), col = "pink")
\end{verbatim}

\section{Variables and objects}\label{variables-and-objects}

An \emph{object} is a data structure having attributes and methods. In
fact, everything in R is an object!

A \emph{variable} is a type of data object. Data objects also include
list, vector, matrices and text.

\begin{itemize}
\tightlist
\item
  Creating a data object
\end{itemize}

In R a variable can be created by using the symbol \texttt{\textless{}-}
to assign a value to a variable name. The variable name is entered on
the left \texttt{\textless{}-} and the value on the right. Note: Data
objects can be given any name, provided that they start with a letter of
the alphabet, and include only letters of the alphabet, numbers and the
characters \texttt{.} and \texttt{\_}. Hence AgeGroup, Age\_Group and
Age.Group are all valid names for an R data object. Note also that R is
case-sensitive, to agegroup and AgeGroup would be treated as different
data objects.

To save the value \emph{28} to a variable (data object) labelled
\emph{age}, run the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\DecValTok{28}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Inspecting a data object
\end{itemize}

To inspect the contents of the data object \emph{age} run the following
line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 28
\end{verbatim}

Find out what kind (class) of data object \emph{age} is using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(age) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

Inspect the structure of the \emph{age} data object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(age) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num 28
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The \emph{vector} data object
\end{itemize}

What if we have more than one response? We can use the \texttt{c(\ )}
function to combine multiple values into one data vector object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{32}\NormalTok{)}
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 28 36 25 24 32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(age) }\CommentTok{#Still numeric..}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(age) }\CommentTok{#..but now a vector (set) of 5 separate values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:5] 28 36 25 24 32
\end{verbatim}

Note that on each line in the code above any text following the
\texttt{\#} character is ignored by R when executing the code. Instead,
text following a \texttt{\#} can be used to add comments to the code to
make clear what the code is doing. Two marks of good code are a clear
layout and clear commentary on the code.

\subsection{Basic Data Types}\label{basic-data-types}

There are a number of data types. Four are the most common. In R,
\textbf{numeric} is the default type for numbers. It stores all numbers
as floating-point numbers (numbers with decimals). This is because most
statistical calculations deal with numbers with up to two decimals.

\begin{itemize}
\tightlist
\item
  Numeric
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num <-}\StringTok{ }\FloatTok{4.5} \CommentTok{# Decimal values}
\KeywordTok{class}\NormalTok{(num)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Integer
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{int <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\DecValTok{4}\NormalTok{) }\CommentTok{# Natural numbers. Note integers are also numerics.}
\KeywordTok{class}\NormalTok{(int)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Character
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cha <-}\StringTok{ "are you enjoying this?"} \CommentTok{# text or string. You can also type `as.character("are you enjoying this?")`}
\KeywordTok{class}\NormalTok{(cha)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Logical
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log <-}\StringTok{ }\DecValTok{2} \OperatorTok{<}\StringTok{ }\DecValTok{1} \CommentTok{# assigns TRUE or FALSE. In this case, FALSE as 2 is greater than 1}
\NormalTok{log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(log)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\subsection{Random Variables}\label{random-variables}

In statistics, we differentiate between data to capture:

\begin{itemize}
\item
  \emph{Qualitative attributes} categorise objects eg.gender, marital
  status. To measure these attributes, we use \emph{Categorical} data
  which can be divided into:
\item
  \emph{Nominal} data in categories that have no inherent order eg.
  gender
\item
  \emph{Ordinal} data in categories that have an inherent order eg.
  income bands
\item
  \emph{Quantitative attributes}:
\item
  \emph{Discrete} data: count objects of a certain category eg. number
  of kids, cars
\item
  \emph{Continuous} data: precise numeric measures eg. weight, income,
  length.
\end{itemize}

In R these three types of random variables are represented by the
following types of R data object:

\begin{tabular}{l|l}
\hline
variables & objects\\
\hline
nominal & factor\\
\hline
ordinal & ordered factor\\
\hline
discrete & numeric\\
\hline
continuous & numeric\\
\hline
\end{tabular}

We have already encountered the R data type \emph{numeric}. The next
section introduces the \emph{factor} data type.

\subsubsection{Factor}\label{factor}

\textbf{What is a factor?}

A factor variable assigns a numeric code to each possible category
(\emph{level}) in a variable. Behind the scenes, R stores the variable
using these numeric codes to save space and speed up computing. For
example, compare the size of a list of \texttt{10,000} \emph{males} and
\emph{females} to a list of \texttt{10,000} \texttt{1s} and \texttt{0s}.
At the same time R also saves the category names associated with each
numeric code (level). These are used for display purposes.

For example, the variable \emph{gender}, converted to a factor, would be
stored as a series of \texttt{1s} and \texttt{2s}, where
\texttt{1\ =\ female} and \texttt{2\ =\ male}; but would be displayed in
all outputs using their category labels of \emph{female} and
\emph{male}.

\textbf{Creating a factor}

To convert a numeric or character vector into a factor use the
\texttt{factor(\ )} function. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gender <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"female"}\NormalTok{,}\StringTok{"male"}\NormalTok{,}\StringTok{"male"}\NormalTok{,}\StringTok{"female"}\NormalTok{,}\StringTok{"female"}\NormalTok{) }\CommentTok{# create a gender variable}
\NormalTok{gender <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(gender) }\CommentTok{# replace character vector with a factor version}
\NormalTok{gender}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] female male   male   female female
## Levels: female male
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Factor w/ 2 levels "female","male": 1 2 2 1 1
\end{verbatim}

Now \emph{gender} is a factor and is stored as a series of \texttt{1s}
and \texttt{2s}, with \texttt{1s} representing \texttt{females} and
\texttt{2s} representing \texttt{males}. The function
\texttt{levels(\ )} lists the levels (categories) associated with a
given factor variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "female" "male"
\end{verbatim}

The categories are reported in the order that they have been numbered
(starting from \texttt{1}). Hence from the output we can infer that
\texttt{females} are coded as \texttt{1}, and \texttt{males} as
\texttt{2}.

\section{Data Frames}\label{data-frames}

R stores different types of data using different types of data
structure. Data are normally stored as a \emph{data.frame}. A data
frames contain one row per observation (e.g.~wards) and one column per
attribute (eg. population and health).

We create three variables wards, population (\texttt{pop}) and people
with good health (\texttt{ghealth}). We use 2011 census data counts for
total population and good health for wards in Liverpool.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wards <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Allerton and Hunts Cross"}\NormalTok{,}\StringTok{"Anfield"}\NormalTok{,}\StringTok{"Belle Vale"}\NormalTok{,}\StringTok{"Central"}\NormalTok{,}\StringTok{"Childwall"}\NormalTok{,}\StringTok{"Church"}\NormalTok{,}\StringTok{"Clubmoor"}\NormalTok{,}\StringTok{"County"}\NormalTok{,}\StringTok{"Cressington"}\NormalTok{,}\StringTok{"Croxteth"}\NormalTok{,}\StringTok{"Everton"}\NormalTok{,}\StringTok{"Fazakerley"}\NormalTok{,}\StringTok{"Greenbank"}\NormalTok{,}\StringTok{"Kensington and Fairfield"}\NormalTok{,}\StringTok{"Kirkdale"}\NormalTok{,}\StringTok{"Knotty Ash"}\NormalTok{,}\StringTok{"Mossley Hill"}\NormalTok{,}\StringTok{"Norris Green"}\NormalTok{,}\StringTok{"Old Swan"}\NormalTok{,}\StringTok{"Picton"}\NormalTok{,}\StringTok{"Princes Park"}\NormalTok{,}\StringTok{"Riverside"}\NormalTok{,}\StringTok{"St Michael's"}\NormalTok{,}\StringTok{"Speke-Garston"}\NormalTok{,}\StringTok{"Tuebrook and Stoneycroft"}\NormalTok{,}\StringTok{"Warbreck"}\NormalTok{,}\StringTok{"Wavertree"}\NormalTok{,}\StringTok{"West Derby"}\NormalTok{,}\StringTok{"Woolton"}\NormalTok{,}\StringTok{"Yew Tree"}\NormalTok{)}

\NormalTok{pop <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{14853}\NormalTok{,}\DecValTok{14510}\NormalTok{,}\DecValTok{15004}\NormalTok{,}\DecValTok{20340}\NormalTok{,}\DecValTok{13908}\NormalTok{,}\DecValTok{13974}\NormalTok{,}\DecValTok{15272}\NormalTok{,}\DecValTok{14045}\NormalTok{,}\DecValTok{14503}\NormalTok{,}
                \DecValTok{14561}\NormalTok{,}\DecValTok{14782}\NormalTok{,}\DecValTok{16786}\NormalTok{,}\DecValTok{16132}\NormalTok{,}\DecValTok{15377}\NormalTok{,}\DecValTok{16115}\NormalTok{,}\DecValTok{13312}\NormalTok{,}\DecValTok{13816}\NormalTok{,}\DecValTok{15047}\NormalTok{,}
                \DecValTok{16461}\NormalTok{,}\DecValTok{17009}\NormalTok{,}\DecValTok{17104}\NormalTok{,}\DecValTok{18422}\NormalTok{,}\DecValTok{12991}\NormalTok{,}\DecValTok{20300}\NormalTok{,}\DecValTok{16489}\NormalTok{,}\DecValTok{16481}\NormalTok{,}\DecValTok{14772}\NormalTok{,}
                \DecValTok{14382}\NormalTok{,}\DecValTok{12921}\NormalTok{,}\DecValTok{16746}\NormalTok{)}

\NormalTok{ghealth <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{7274}\NormalTok{,}\DecValTok{6124}\NormalTok{,}\DecValTok{6129}\NormalTok{,}\DecValTok{11925}\NormalTok{,}\DecValTok{7219}\NormalTok{,}\DecValTok{7461}\NormalTok{,}\DecValTok{6403}\NormalTok{,}\DecValTok{5930}\NormalTok{,}\DecValTok{7094}\NormalTok{,}\DecValTok{6992}\NormalTok{,}
                 \DecValTok{5517}\NormalTok{,}\DecValTok{7879}\NormalTok{,}\DecValTok{8990}\NormalTok{,}\DecValTok{6495}\NormalTok{,}\DecValTok{6662}\NormalTok{,}\DecValTok{5981}\NormalTok{,}\DecValTok{7322}\NormalTok{,}\DecValTok{6529}\NormalTok{,}\DecValTok{7192}\NormalTok{,}\DecValTok{7953}\NormalTok{,}
                 \DecValTok{7636}\NormalTok{,}\DecValTok{9001}\NormalTok{,}\DecValTok{6450}\NormalTok{,}\DecValTok{8973}\NormalTok{,}\DecValTok{7302}\NormalTok{,}\DecValTok{7521}\NormalTok{,}\DecValTok{7268}\NormalTok{,}\DecValTok{7013}\NormalTok{,}\DecValTok{6025}\NormalTok{,}\DecValTok{7717}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that \texttt{pop} and \texttt{ghealth} and \texttt{wards} contains
characters.

\subsection{Creating A Data Frame}\label{creating-a-data-frame}

We can create a data frame and examine its structure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(wards, pop, ghealth)}
\NormalTok{df }\CommentTok{# or use view(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                       wards   pop ghealth
## 1  Allerton and Hunts Cross 14853    7274
## 2                   Anfield 14510    6124
## 3                Belle Vale 15004    6129
## 4                   Central 20340   11925
## 5                 Childwall 13908    7219
## 6                    Church 13974    7461
## 7                  Clubmoor 15272    6403
## 8                    County 14045    5930
## 9               Cressington 14503    7094
## 10                 Croxteth 14561    6992
## 11                  Everton 14782    5517
## 12               Fazakerley 16786    7879
## 13                Greenbank 16132    8990
## 14 Kensington and Fairfield 15377    6495
## 15                 Kirkdale 16115    6662
## 16               Knotty Ash 13312    5981
## 17             Mossley Hill 13816    7322
## 18             Norris Green 15047    6529
## 19                 Old Swan 16461    7192
## 20                   Picton 17009    7953
## 21             Princes Park 17104    7636
## 22                Riverside 18422    9001
## 23             St Michael's 12991    6450
## 24            Speke-Garston 20300    8973
## 25 Tuebrook and Stoneycroft 16489    7302
## 26                 Warbreck 16481    7521
## 27                Wavertree 14772    7268
## 28               West Derby 14382    7013
## 29                  Woolton 12921    6025
## 30                 Yew Tree 16746    7717
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(df) }\CommentTok{# or use glimpse(data) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    30 obs. of  3 variables:
##  $ wards  : Factor w/ 30 levels "Allerton and Hunts Cross",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ pop    : num  14853 14510 15004 20340 13908 ...
##  $ ghealth: num  7274 6124 6129 11925 7219 ...
\end{verbatim}

\subsection{Referencing Data Frames}\label{referencing-data-frames}

Throughout this module, you will need to refer to particular parts of a
dataframe - perhaps a particular column (an area attribute); or a
particular subset of respondents. Hence it is worth spending some time
now mastering this particular skill.

The relevant R function, \texttt{{[}\ {]}}, has the format
\texttt{{[}row,col{]}} or, more generally,
\texttt{{[}set\ of\ rows,\ set\ of\ cols{]}}.

Run the following commands to get a feel of how to extract different
slices of the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\CommentTok{# whole data.frame}
\NormalTok{df[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] }\CommentTok{# contents of first row and column}
\NormalTok{df[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{# contents of the second row, second and third columns}
\NormalTok{df[}\DecValTok{1}\NormalTok{, ] }\CommentTok{# first row, ALL columns [the default if no columns specified]}
\NormalTok{df[ ,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{] }\CommentTok{# ALL rows; first and second columns}
\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{), ] }\CommentTok{# rows 1,3,5; ALL columns}
\NormalTok{df[ , }\DecValTok{2}\NormalTok{] }\CommentTok{# ALL rows; second column (by default results containing only }
             \CommentTok{#one column are converted back into a vector)}
\NormalTok{df[ , }\DecValTok{2}\NormalTok{, drop=}\OtherTok{FALSE}\NormalTok{] }\CommentTok{# ALL rows; second column (returned as a data.frame)}
\end{Highlighting}
\end{Shaded}

In the above, note that we have used two other R functions:

\begin{itemize}
\item
  \texttt{1:3} The colon operator tells R to produce a list of numbers
  including the named start and end points.
\item
  \texttt{c(1,3,5)} Tells R to combine the contents within the brackets
  into one list of objects
\end{itemize}

Run both of these fuctions on their own to get a better understanding of
what they do.

Three other methods for referencing the contents of a data.frame make
direct use of the variable names within the data.frame, which tends to
make for easier to read/understand code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[,}\StringTok{"pop"}\NormalTok{] }\CommentTok{# variable name in quotes inside the square brackets}
\NormalTok{df}\OperatorTok{$}\NormalTok{pop }\CommentTok{# variable name prefixed with $ and appended to the data.frame name}
\CommentTok{# or you can use attach}
\KeywordTok{attach}\NormalTok{(df)}
\NormalTok{pop }\CommentTok{# but be careful if you already have an age variable in your local workspace}
\end{Highlighting}
\end{Shaded}

Want to check the variables available, use the \texttt{names(\ )}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "wards"   "pop"     "ghealth"
\end{verbatim}

\hypertarget{sec_readdata}{\section{Read Data}\label{sec_readdata}}

Ensure your memory is clear

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{()) }\CommentTok{# rm for targeted deletion / ls for listing all existing objects}
\end{Highlighting}
\end{Shaded}

There are many commands to read / load data onto R. The command to use
will depend upon the format they have been saved. Normally they are
saved in \emph{csv} format from Excel or other software packages. So we
use either:

\begin{itemize}
\tightlist
\item
  \texttt{df\ \textless{}-\ read.table("path/file\_name.csv",\ header\ =\ FALSE,\ sep\ =",")}
\item
  \texttt{df\ \textless{}-\ read("path/file\_name.csv",\ header\ =\ FALSE)}
\item
  \texttt{df\ \textless{}-\ read.csv2("path/file\_name.csv",\ header\ =\ FALSE)}
\end{itemize}

To read files in other formats, refer to this useful
\href{https://www.datacamp.com/community/tutorials/r-data-import-tutorial?utm_source=adwords_ppc\&utm_campaignid=1655852085\&utm_adgroupid=61045434382\&utm_device=c\&utm_keyword=\%2Bread\%20\%2Bdata\%20\%2Br\&utm_matchtype=b\&utm_network=g\&utm_adpostion=1t1\&utm_creative=318880582308\&utm_targetid=kwd-309793905111\&utm_loc_interest_ms=\&utm_loc_physical_ms=9046551\&gclid=CjwKCAiA3uDwBRBFEiwA1VsajJO0QK0Jg7VipIt8_t82qQrnUliI0syAlh8CIxnE76Rb0kh3FbiehxoCzCgQAvD_BwE\#csv}{DataCamp
tutorial}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/census/census_data.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        code                     ward pop16_74 higher_managerial   pop ghealth
## 1 E05000886 Allerton and Hunts Cross    10930              1103 14853    7274
## 2 E05000887                  Anfield    10712               312 14510    6124
## 3 E05000888               Belle Vale    10987               432 15004    6129
## 4 E05000889                  Central    19174              1346 20340   11925
## 5 E05000890                Childwall    10410              1123 13908    7219
## 6 E05000891                   Church    10569              1843 13974    7461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }\AlertTok{NOTE}\CommentTok{: always ensure your are setting the correct directory leading to the data. }
\CommentTok{# It may differ from your existing working directory}
\end{Highlighting}
\end{Shaded}

\subsection{Quickly inspect the data}\label{quickly-inspect-the-data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What class?
\item
  What R data types?
\item
  What data types?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1}
\KeywordTok{class}\NormalTok{(census)}
\CommentTok{# 2 & 3}
\KeywordTok{str}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

Just interested in the variable names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(census)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "code"              "ward"              "pop16_74"         
## [4] "higher_managerial" "pop"               "ghealth"
\end{verbatim}

or want to view the data:

\texttt{View(census)}

\section{Manipulation Data}\label{manipulation-data}

\subsection{Adding New Variables}\label{adding-new-variables}

Usually you want to add / create new variables to your data frame using
existing variables eg. computing percentages by dividing two variables.
There are many ways in which you can do this i.e.~referecing a data
frame as we have done above, or using \texttt{\$} (e.g.
\texttt{census\$pop}). For this module, we'll use \texttt{tidyverse}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{per_ghealth =}\NormalTok{ ghealth }\OperatorTok{/}\StringTok{ }\NormalTok{pop)}
\end{Highlighting}
\end{Shaded}

Note we used a \emph{pipe operator} \texttt{\%\textgreater{}\%}, which
helps make the code more efficient and readable - more details, see
\citet{grolemund_wickham_2019_book}. When using the pipe operator,
recall to first indicate the data frame before
\texttt{\%\textgreater{}\%}.

Note also the use a variable name before the \texttt{=} sign in brackets
to indicate the name of the new variable after \texttt{mutate}.

\subsection{Selecting Variables}\label{selecting-variables}

Usually you want to select a subset of variables for your analysis as
storing to large data sets in your R memory can reduce the processing
speed of your machine. A selection of data can be achieved by using the
\texttt{select} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndf <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(ward, pop16_}\DecValTok{74}\NormalTok{, per_ghealth)}
\end{Highlighting}
\end{Shaded}

Again first indicate the data frame and then the variable you want to
select to build a new data frame. Note the code chunk above has created
a new data frame called \texttt{ndf}. Explore it.

\subsection{Filtering Data}\label{filtering-data}

You may also want to filter values based on defined conditions. You may
want to filter observations greater than a certain threshold or only
areas within a certain region. For example, you may want to select areas
with a percentage of good health population over 50\%:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndf2 <-}\StringTok{ }\NormalTok{census }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(per_ghealth }\OperatorTok{<}\StringTok{ }\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can use more than one variables to set conditions. Use
``\texttt{,}'' to add a condition.

\subsection{Joining Data Drames}\label{joining-data-drames}

When working with spatial data, we often need to join data. To this end,
you need a common unique \texttt{id\ variable}. Let's say, we want to
add a data frame containing census data on households for Liverpool, and
join the new attributes to one of the existing data frames in the
workspace. First we will read the data frame we want to join (ie.
\texttt{census\_data2.csv}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{census2 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/census/census_data2.csv"}\NormalTok{)}
\CommentTok{# visualise data structure}
\KeywordTok{str}\NormalTok{(census2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    30 obs. of  3 variables:
##  $ geo_code               : Factor w/ 30 levels "E05000886","E05000887",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ households             : int  6359 6622 6622 7139 5391 5884 6576 6745 6317 6024 ...
##  $ socialrented_households: int  827 1508 2818 1311 374 178 2859 1564 1023 1558 ...
\end{verbatim}

The variable \texttt{geo\_code} in this data frame corresponds to the
\texttt{code} in the existing data frame and they are unique so they can
be automatically matched by using the \texttt{merge()} function. The
\texttt{merge()} function uses two arguments: \texttt{x} and \texttt{y}.
The former refers to data frame 1 and the latter to data frame 2. Both
of these two data frames must have a \texttt{id} variable containing the
same information. Note they can have different names. Another key
argument to include is \texttt{all.x=TRUE} which tells the function to
keep all the records in \texttt{x}, but only those in \texttt{y} that
match in case there are discrepancies in the \texttt{id} variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join data frames}
\NormalTok{join_dfs <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(census, census2, }\DataTypeTok{by.x=}\StringTok{"code"}\NormalTok{, }\DataTypeTok{by.y=}\StringTok{"geo_code"}\NormalTok{, }\DataTypeTok{all.x =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# check data}
\KeywordTok{head}\NormalTok{(join_dfs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        code                     ward pop16_74 higher_managerial   pop ghealth
## 1 E05000886 Allerton and Hunts Cross    10930              1103 14853    7274
## 2 E05000887                  Anfield    10712               312 14510    6124
## 3 E05000888               Belle Vale    10987               432 15004    6129
## 4 E05000889                  Central    19174              1346 20340   11925
## 5 E05000890                Childwall    10410              1123 13908    7219
## 6 E05000891                   Church    10569              1843 13974    7461
##   per_ghealth households socialrented_households
## 1   0.4897327       6359                     827
## 2   0.4220538       6622                    1508
## 3   0.4084911       6622                    2818
## 4   0.5862832       7139                    1311
## 5   0.5190538       5391                     374
## 6   0.5339201       5884                     178
\end{verbatim}

\subsection{Saving Data}\label{saving-data}

It may also be convinient to save your R projects. They contains all the
objects that you have created in your workspace by using the
\texttt{save.image(\ )} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{save.image}\NormalTok{(}\StringTok{"week1_envs453.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This creates a file labelled ``week1\_envs453.RData'' in your working
directory. You can load this at a later stage using the
\texttt{load(\ )} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"week1_envs453.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively you can save / export your data into a \texttt{csv} file.
The first argument in the function is the object name, and the second:
the name of the csv we want to create.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(join_dfs, }\StringTok{"join_censusdfs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Using Spatial Data Frames}\label{using-spatial-data-frames}

A core area of this module is learning to work with spatial data in R. R
has various purposedly designed \textbf{packages} for manipulation of
spatial data and spatial analysis techniques. Various R packages exist
in CRAN eg. \texttt{spatial}, \texttt{sgeostat}, \texttt{splancs},
\texttt{maptools}, \texttt{tmap}, \texttt{rgdal}, \texttt{spand} and
more recent development of \texttt{sf} - see
\citet{Lovelace_et_al_2020_book} for a great description and historical
context for some of these packages.

During this session, we will use \texttt{sf}.

We first need to import our spatial data. We will use a shapefile
containing data at Output Area (OA) level for Liverpool. These data
illustrates the hierarchical structure of spatial data.

\subsection{Read Spatial Data}\label{read-spatial-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/census/Liverpool_OA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `Liverpool_OA' from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/census/Liverpool_OA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 1584 features and 18 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 332390.2 ymin: 379748.5 xmax: 345636 ymax: 397980.1
## epsg (SRID):    27700
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs
\end{verbatim}

Examine the input data. A spatial data frame stores a range of
attributes derived from a shapefile including the \textbf{geometry} of
features (e.g.~polygon shape and location), \textbf{attributes} for each
feature (stored in the .dbf),
\href{https://en.wikipedia.org/wiki/Map_projection}{projection} and
coordinates of the shapefile's bounding box - for details, execute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?st_read}
\end{Highlighting}
\end{Shaded}

You can employ the usual functions to visualise the content of the
created data frame:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# visualise variable names}
\KeywordTok{names}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "OA_CD"    "LSOA_CD"  "MSOA_CD"  "LAD_CD"   "pop"      "H_Vbad"  
##  [7] "H_bad"    "H_fair"   "H_good"   "H_Vgood"  "age_men"  "age_med" 
## [13] "age_60"   "S_Rent"   "Ethnic"   "illness"  "unemp"    "males"   
## [19] "geometry"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# data structure}
\KeywordTok{str}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'sf' and 'data.frame':   1584 obs. of  19 variables:
##  $ OA_CD   : Factor w/ 1584 levels "E00032987","E00032988",..: 1547 485 143 1567 980 1186 1122 889 863 433 ...
##  $ LSOA_CD : Factor w/ 298 levels "E01006512","E01006513",..: 291 96 33 124 187 231 220 175 173 83 ...
##  $ MSOA_CD : Factor w/ 61 levels "E02001347","E02001348",..: 59 12 19 23 29 20 31 14 30 10 ...
##  $ LAD_CD  : Factor w/ 1 level "E08000012": 1 1 1 1 1 1 1 1 1 1 ...
##  $ pop     : int  185 281 208 200 321 187 395 320 316 214 ...
##  $ H_Vbad  : int  1 2 3 7 4 4 5 9 5 4 ...
##  $ H_bad   : int  2 20 10 8 10 25 19 22 25 17 ...
##  $ H_fair  : int  9 47 22 17 32 70 42 53 55 39 ...
##  $ H_good  : int  53 111 71 52 112 57 131 104 104 53 ...
##  $ H_Vgood : int  120 101 102 116 163 31 198 132 127 101 ...
##  $ age_men : num  27.9 37.7 37.1 33.7 34.2 ...
##  $ age_med : num  25 36 32 29 34 53 23 30 34 29 ...
##  $ age_60  : num  0.0108 0.1637 0.1971 0.1 0.1402 ...
##  $ S_Rent  : num  0.0526 0.176 0.0235 0.2222 0.0222 ...
##  $ Ethnic  : num  0.3514 0.0463 0.0192 0.215 0.0779 ...
##  $ illness : int  185 281 208 200 321 187 395 320 316 214 ...
##  $ unemp   : num  0.0438 0.121 0.1121 0.036 0.0743 ...
##  $ males   : int  122 128 95 120 158 123 207 164 157 94 ...
##  $ geometry:sfc_MULTIPOLYGON of length 1584; first list element: List of 1
##   ..$ :List of 1
##   .. ..$ : num [1:14, 1:2] 335106 335130 335164 335173 335185 ...
##   ..- attr(*, "class")= chr  "XY" "MULTIPOLYGON" "sfg"
##  - attr(*, "sf_column")= chr "geometry"
##  - attr(*, "agr")= Factor w/ 3 levels "constant","aggregate",..: NA NA NA NA NA NA NA NA NA NA ...
##   ..- attr(*, "names")= chr  "OA_CD" "LSOA_CD" "MSOA_CD" "LAD_CD" ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# see first few observations}
\KeywordTok{head}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 6 features and 18 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479
## epsg (SRID):    27700
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs
##       OA_CD   LSOA_CD   MSOA_CD    LAD_CD pop H_Vbad H_bad H_fair H_good
## 1 E00176737 E01033761 E02006932 E08000012 185      1     2      9     53
## 2 E00033515 E01006614 E02001358 E08000012 281      2    20     47    111
## 3 E00033141 E01006546 E02001365 E08000012 208      3    10     22     71
## 4 E00176757 E01006646 E02001369 E08000012 200      7     8     17     52
## 5 E00034050 E01006712 E02001375 E08000012 321      4    10     32    112
## 6 E00034280 E01006761 E02001366 E08000012 187      4    25     70     57
##   H_Vgood  age_men age_med     age_60     S_Rent     Ethnic illness      unemp
## 1     120 27.94054      25 0.01081081 0.05263158 0.35135135     185 0.04379562
## 2     101 37.71174      36 0.16370107 0.17600000 0.04626335     281 0.12101911
## 3     102 37.08173      32 0.19711538 0.02352941 0.01923077     208 0.11214953
## 4     116 33.73000      29 0.10000000 0.22222222 0.21500000     200 0.03597122
## 5     163 34.19003      34 0.14018692 0.02222222 0.07788162     321 0.07428571
## 6      31 56.09091      53 0.44919786 0.88524590 0.11764706     187 0.44615385
##   males                       geometry
## 1   122 MULTIPOLYGON (((335106.3 38...
## 2   128 MULTIPOLYGON (((335810.5 39...
## 3    95 MULTIPOLYGON (((336738 3931...
## 4   120 MULTIPOLYGON (((335914.5 39...
## 5   158 MULTIPOLYGON (((339325 3914...
## 6   123 MULTIPOLYGON (((338198.1 39...
\end{verbatim}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What are the geographical hierarchy in these data?
\item
  What is the smallest geography?
\item
  What is the largest geography?
\end{itemize}

\subsection{Basic Mapping}\label{basic-mapping}

Again, many functions exist in CRAN for creating maps:

\begin{itemize}
\tightlist
\item
  \texttt{plot} to create static maps
\item
  \texttt{tmap} to create static and interactive maps
\item
  \texttt{leaflet} to create interactive maps
\item
  \texttt{mapview} to create interactive maps
\item
  \texttt{ggplot2} to create data visualisations, including static maps
\item
  \texttt{shiny} to create web applications, including maps
\end{itemize}

Here this notebook demonstrates the use of \texttt{plot} and
\texttt{tmap}. First \texttt{plot} is used to map the spatial
distribution of non-British-born population in Liverpool. First we only
map the geometries on the right,

\subsubsection{\texorpdfstring{Using
\texttt{plot}}{Using plot}}\label{using-plot}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# mapping geometry}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(oa_shp))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-40-1.pdf}
\caption{\label{fig:unnamed-chunk-40}OAs of Livepool}
\end{figure}

and then:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# map attributes, adding intervals}
\KeywordTok{plot}\NormalTok{(oa_shp[}\StringTok{"Ethnic"}\NormalTok{], }\DataTypeTok{key.pos =} \DecValTok{4}\NormalTok{, }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{key.width =} \KeywordTok{lcm}\NormalTok{(}\FloatTok{1.3}\NormalTok{), }\DataTypeTok{key.length =} \DecValTok{1}\NormalTok{.,}
     \DataTypeTok{breaks =} \StringTok{"jenks"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{border =} \StringTok{'grey'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{01-intro_files/figure-latex/unnamed-chunk-41-1.pdf}
\caption{\label{fig:unnamed-chunk-41}Spatial distribution of ethnic groups,
Liverpool}
\end{figure}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What is the key pattern emerging from this map?
\end{itemize}

\subsubsection{\texorpdfstring{Using
\texttt{tmap}}{Using tmap}}\label{using-tmap}

Similar to \texttt{ggplot2}, \texttt{tmap} is based on the idea of a
`grammar of graphics' which involves a separation between the input data
and aesthetics (i.e.~the way data are visualised). Each data set can be
mapped in various different ways, including location as defined by its
geometry, colour and other features. The basic building block is
\texttt{tm\_shape()} (which defines input data), followed by one or more
layer elements such as \texttt{tm\_fill()} and \texttt{tm\_dots()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{oa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(oa_shp)}

\CommentTok{# map}
\NormalTok{legend_title =}\StringTok{ }\KeywordTok{expression}\NormalTok{(}\StringTok{"% ethnic pop."}\NormalTok{)}
\NormalTok{map_oa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(oa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"Ethnic"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# add fill}
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }\CommentTok{# add borders}
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# add compass}
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\CommentTok{# add scale bar}
\NormalTok{map_oa}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-intro_files/figure-latex/unnamed-chunk-42-1.pdf}

Note that the operation \texttt{+} is used to add new layers. You can
set style themes by \texttt{tm\_style}. To visualise the existing styles
use \texttt{tmap\_style\_catalogue()}, and you can also evaluate the
code chunk below if you would like to create an interactive map.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tmap_mode}\NormalTok{(}\StringTok{"view"}\NormalTok{)}
\NormalTok{map_oa}
\end{Highlighting}
\end{Shaded}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  Try mapping other variables in the spatial data frame. Where do
  population aged 60 and over concentrate?
\end{itemize}

\subsection{Comparing geographies}\label{comparing-geographies}

If you recall, one of the key issues of working with spatial data is the
modifiable area unit problem (MAUP) - see lecture notes. To get a sense
of the effects of MAUP, we analyse differences in the spatial patterns
of the ethnic population in Liverpool between Middle Layer Super Output
Areas (MSOAs) and OAs. So we map these geographies together.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data at the msoa level}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/census/Liverpool_MSOA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `Liverpool_MSOA' from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/census/Liverpool_MSOA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 61 features and 16 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1
## epsg (SRID):    27700
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{msoa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(msoa_shp)}

\CommentTok{# create a map}
\NormalTok{map_msoa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(msoa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"Ethnic"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }

\CommentTok{# arrange maps }
\KeywordTok{tmap_arrange}\NormalTok{(map_msoa, map_oa) }
\end{Highlighting}
\end{Shaded}

\includegraphics{01-intro_files/figure-latex/unnamed-chunk-44-1.pdf}

\textbf{TASK:}

\begin{itemize}
\tightlist
\item
  What differences do you see between OAs and MSOAs?
\item
  Can you identify areas of spatial clustering? Where are they?
\end{itemize}

\section{Useful Functions}\label{useful-functions}

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description\tabularnewline
\midrule
\endhead
read.csv() & read csv files into data frames\tabularnewline
str() & inspect data structure\tabularnewline
mutate() & create a new variable\tabularnewline
filter() & filter observations based on variable values\tabularnewline
\%\textgreater{}\% & pipe operator - chain operations\tabularnewline
select() & select variables\tabularnewline
merge() & join dat frames\tabularnewline
st\_read & read spatial data (ie. shapefiles)\tabularnewline
plot() & create a map based a spatial data set\tabularnewline
tm\_shape(), tm\_fill(), tm\_borders() & create a map using tmap
functions\tabularnewline
tm\_arrange & display multiple maps in a single
``metaplot''\tabularnewline
\bottomrule
\end{longtable}

\chapter{Points}\label{points}

This chapter\footnote{This chapter is part of \href{index.html}{Spatial
  Analysis Notes} {Points -- Kernel Density Estimation and Spatial
  interpolation} by Dani Arribas-Bel is licensed under a Creative
  Commons Attribution-NonCommercial-ShareAlike 4.0 International
  License.} is based on the following references, which are great
follow-up's on the topic:

\begin{itemize}
\tightlist
\item
  \citet{lovelace2014introduction} is a great introduction.
\item
  Chapter 6 of \citet{comber2015}, in particular subsections 6.3 and
  6.7.
\item
  \citet{bivand2013applied} provides an in-depth treatment of spatial
  data in R.
\end{itemize}

This chapter is part of \href{index.html}{Spatial Analysis Notes}, a
compilation hosted as a GitHub repository that you can access it in a
few ways:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of
  a \texttt{.zip} file that contains all the materials.
\item
  As an \href{https://gdsl-ul.github.io/san/points.html}{html website}.
\item
  As a
  \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\section{Dependencies}\label{dependencies-1}

This tutorial relies on the following libraries that you will need to
have installed on your machine to be able to interactively follow
along\footnote{You can install package \texttt{mypackage} by running the
  command \texttt{install.packages("mypackage")} on the R prompt or
  through the \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...}
  menu in RStudio.}. Once installed, load them up with the following
commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Layout}
\KeywordTok{library}\NormalTok{(tufte)}
\CommentTok{# For pretty table}
\KeywordTok{library}\NormalTok{(knitr)}
\CommentTok{# Spatial Data management}
\KeywordTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## rgdal: version: 1.4-8, (SVN revision 845)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
##  Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
##  GDAL binary built with GEOS: FALSE 
##  Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520]
##  Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
##  Linking to sp version: 1.3-2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pretty graphics}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap)}
\CommentTok{# Pretty maps}
\KeywordTok{library}\NormalTok{(ggmap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.
\end{verbatim}

\begin{verbatim}
## Please cite ggmap if you use it! See citation("ggmap") for details.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Various GIS utilities}
\KeywordTok{library}\NormalTok{(GISTools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: maptools
\end{verbatim}

\begin{verbatim}
## Checking rgeos availability: TRUE
\end{verbatim}

\begin{verbatim}
## Loading required package: RColorBrewer
\end{verbatim}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## Loading required package: rgeos
\end{verbatim}

\begin{verbatim}
## rgeos version: 0.5-2, (SVN revision 621)
##  GEOS runtime version: 3.7.2-CAPI-1.11.2 
##  Linking to sp version: 1.3-1 
##  Polygon checking: TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For all your interpolation needs}
\KeywordTok{library}\NormalTok{(gstat)}
\CommentTok{# For data manipulation}
\KeywordTok{library}\NormalTok{(plyr)}
\end{Highlighting}
\end{Shaded}

Before we start any analysis, let us set the path to the directory where
we are working. We can easily do that with \texttt{setwd()}. Please
replace in the following line the path to the folder where you have
placed this file -and where the \texttt{house\_transactions} folder with
the data lives.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd('/media/dani/baul/AAA/Documents/teaching/u-lvl/2016/envs453/code')}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'.'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Data}\label{data}

For this session, we will use a subset of residential property
transaction data for the city of Liverpool. These are provided by the
Land Registry (as part of their
\href{https://www.gov.uk/government/collections/price-paid-data}{Price
Paid Data}) but have been cleaned and re-packaged by Dani Arribas-Bel.

Let us start by reading the data, which comes in a shapefile:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\DataTypeTok{dsn =} \StringTok{'data/house_transactions'}\NormalTok{, }\DataTypeTok{layer =} \StringTok{'liv_house_trans'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/house_transactions", layer: "liv_house_trans"
## with 6324 features
## It has 18 fields
## Integer64 fields read as strings:  price
\end{verbatim}

Before we forget, let us make sure \texttt{price} is considered a
number, not a factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{((db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price)))}
\end{Highlighting}
\end{Shaded}

The dataset spans the year 2014:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Format dates}
\NormalTok{dts <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trans_date)}
\CommentTok{# Set up summary table}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(dts)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Min.      1st Qu.       Median         Mean      3rd Qu.         Max. 
## "2014-01-02" "2014-04-11" "2014-07-09" "2014-07-08" "2014-10-03" "2014-12-30"
\end{verbatim}

We can then examine the elements of the object with the \texttt{summary}
method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class SpatialPointsDataFrame
## Coordinates:
##              min    max
## coords.x1 333536 345449
## coords.x2 382684 397833
## Is projected: TRUE 
## proj4string :
## [+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
## +y_0=-100000 +ellps=airy
## +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m
## +no_defs]
## Number of points: 6324
## Data attributes:
##       pcds                                           id      
##  L1 6LS : 126   {00029226-80EF-4280-9809-109B8509656A}:   1  
##  L8 5TE :  63   {00041BD2-4A07-4D41-A5AE-6459CD5FD37C}:   1  
##  L1 5AQ :  34   {0005AE67-9150-41D4-8D56-6BFC868EECA3}:   1  
##  L24 1WA:  31   {00183CD7-EE48-434B-8A1A-C94B30A93691}:   1  
##  L17 6BT:  26   {003EA3A5-F804-458D-A66F-447E27569456}:   1  
##  L3 1EE :  24   {00411304-DD5B-4F11-9748-93789D6A000E}:   1  
##  (Other):6020   (Other)                               :6318  
##      price                     trans_date   type     new      duration
##  Min.   :    1000   2014-06-27 00:00: 109   D: 505   N:5495   F:3927  
##  1st Qu.:   70000   2014-12-19 00:00: 109   F:1371   Y: 829   L:2397  
##  Median :  110000   2014-02-28 00:00: 105   O: 119                    
##  Mean   :  144310   2014-10-31 00:00:  95   S:1478                    
##  3rd Qu.:  160000   2014-03-28 00:00:  94   T:2851                    
##  Max.   :26615720   2014-11-28 00:00:  94                             
##                     (Other)         :5718                             
##       paon               saon                   street             locality   
##  3      : 203   FLAT 2     :  25   CROSSHALL STREET: 133   WAVERTREE   : 126  
##  11     : 151   FLAT 3     :  25   STANHOPE STREET :  63   MOSSLEY HILL: 102  
##  14     : 148   FLAT 1     :  24   PALL MALL       :  47   WALTON      :  88  
##  5      : 146   APARTMENT 4:  23   DUKE STREET     :  41   WEST DERBY  :  71  
##  4      : 140   APARTMENT 2:  21   MANN ISLAND     :  41   WOOLTON     :  66  
##  8      : 128   (Other)    : 893   OLD HALL STREET :  39   (Other)     : 548  
##  (Other):5408   NA's       :5313   (Other)         :5960   NA's        :5323  
##         town           district           county     ppd_cat  status  
##  LIVERPOOL:6324   KNOWSLEY :  12   MERSEYSIDE:6324   A:5393   A:6324  
##                   LIVERPOOL:6311                     B: 931           
##                   WIRRAL   :   1                                      
##                                                                       
##                                                                       
##                                                                       
##                                                                       
##        lsoa11          LSOA11CD   
##  E01033762: 144   E01033762: 144  
##  E01033756:  98   E01033756:  98  
##  E01033752:  93   E01033752:  93  
##  E01033750:  71   E01033750:  71  
##  E01006518:  68   E01006518:  68  
##  E01033755:  65   E01033755:  65  
##  (Other)  :5785   (Other)  :5785
\end{verbatim}

See how it contains several pieces, some relating to the spatial
information, some relating to the tabular data attached to it. We can
access each of the separately if we need it. For example, to pull out
the names of the columns in the \texttt{data.frame}, we can use the
\texttt{@data} appendix:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colnames}\NormalTok{(db}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "pcds"       "id"         "price"      "trans_date" "type"      
##  [6] "new"        "duration"   "paon"       "saon"       "street"    
## [11] "locality"   "town"       "district"   "county"     "ppd_cat"   
## [16] "status"     "lsoa11"     "LSOA11CD"
\end{verbatim}

The rest of this session will focus on two main elements of the
shapefile: the spatial dimension (as stored in the point coordinates),
and the house price values contained in the \texttt{price} column. To
get a sense of what they look like first, let us plot both. We can get a
quick look at the non-spatial distribution of house values with the
following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the histogram}
\NormalTok{hist <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{db}\OperatorTok{@}\NormalTok{data,}\DataTypeTok{x=}\NormalTok{price)}
\NormalTok{hist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-8-1.pdf}
\caption{\label{fig:unnamed-chunk-8}Raw house prices in Liverpool}
\end{figure}

This basically shows there is a lot of values concentrated around the
lower end of the distribution but a few very large ones. A usual
transformation to \emph{shrink} these differences is to take logarithms:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create log and add it to the table}
\NormalTok{logpr <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price))}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'logpr'}\NormalTok{] <-}\StringTok{ }\NormalTok{logpr}
\CommentTok{# Create the histogram}
\NormalTok{hist <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{logpr)}
\NormalTok{hist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-9-1.pdf}
\caption{\label{fig:unnamed-chunk-9}Log of house price in Liverpool}
\end{figure}

To obtain the spatial distribution of these houses, we need to turn away
from the \texttt{@data} component of \texttt{db}. The easiest, quickest
(and also dirtiest) way to get a sense of what the data look like over
space is using \texttt{plot}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-10-1.pdf}
\caption{\label{fig:unnamed-chunk-10}Spatial distribution of house
transactions in Liverpool}
\end{figure}

\section{KDE}\label{kde}

Kernel Density Estimation (KDE) is a technique that creates a
\emph{continuous} representation of the distribution of a given
variable, such as house prices. Although theoretically it can be applied
to any dimension, usually, KDE is applied to either one or two
dimensions.

\subsection{One-dimensional KDE}\label{one-dimensional-kde}

KDE over a single dimension is essentially a contiguous version of a
histogram. We can see that by overlaying a KDE on top of the histogram
of logs that we have created before:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the base}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(db}\OperatorTok{@}\NormalTok{data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{logpr))}
\CommentTok{# Histogram}
\NormalTok{hist <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins=}\DecValTok{50}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{..density..))}
\CommentTok{# Overlay density plot}
\NormalTok{kde <-}\StringTok{ }\NormalTok{hist }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"#FF6666"}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"#FF6666"}\NormalTok{)}
\NormalTok{kde}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Histogram and KDE of the log of house
prices in Liverpool}
\end{figure}

The key idea is that we are smoothing out the discrete binning that the
histogram involves. Note how the histogram is exactly the same as above
shape-wise, but it has been rescalend on the Y axis to reflect
probabilities rather than counts.

\subsection{Two-dimensional (spatial)
KDE}\label{two-dimensional-spatial-kde}

Geography, at the end of the day, is usually represented as a
two-dimensional space where we locate objects using a system of dual
coordinates, \texttt{X} and \texttt{Y} (or latitude and longitude).
Thanks to that, we can use the same technique as above to obtain a
smooth representation of the distribution of a two-dimensional variable.
The crucial difference is that, instead of obtaining a curve as the
output, we will create a \emph{surface}, where intensity will be
represented with a color gradient, rather than with the second
dimension, as it is the case in the figure above.

To create a spatial KDE in R, there are several ways. If you do not want
to necessarily acknowledge the spatial nature of your data, or you they
are not stored in a spatial format, you can plot them using
\texttt{ggplot2}. Note we first need to convert the coordinates (stored
in the spatial part of \texttt{db}) into columns of X and Y coordinates,
then we can plot them:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Attach XY coordinates}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'X'}\NormalTok{] <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{1}\NormalTok{]}
\NormalTok{db}\OperatorTok{@}\NormalTok{data[}\StringTok{'Y'}\NormalTok{] <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{2}\NormalTok{]}
\CommentTok{# Set up base layer}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{db}\OperatorTok{@}\NormalTok{data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X, }\DataTypeTok{y=}\NormalTok{Y))}
\CommentTok{# Create the KDE surface}
\NormalTok{kde <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{stat_density2d}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y, }\DataTypeTok{alpha =}\NormalTok{ ..level..), }
               \DataTypeTok{size =} \FloatTok{0.01}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{16}\NormalTok{, }\DataTypeTok{geom =} \StringTok{'polygon'}\NormalTok{) }\OperatorTok{+}
\StringTok{            }\KeywordTok{scale_fill_gradient}\NormalTok{()}
\NormalTok{kde}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-12-1.pdf}
\caption{\label{fig:unnamed-chunk-12}KDE of house transactions in Liverpool}
\end{figure}

Or, we can use a package such as the \texttt{GISTools}, which allows to
pass a spatial object directly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute the KDE}
\NormalTok{kde <-}\StringTok{ }\KeywordTok{kde.points}\NormalTok{(db)}
\CommentTok{# Plot the KDE}
\KeywordTok{level.plot}\NormalTok{(kde)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-13-1.pdf}
\caption{\label{fig:unnamed-chunk-13}KDE of house transactions in Liverpool}
\end{figure}

Either of these approaches generate a surface that represents the
density of dots, that is an estimation of the probability of finding a
house transaction at a given coordinate. However, without any further
information, they are hard to interpret and link with previous knowledge
of the area. To bring such context to the figure, we can plot an
underlying basemap, using a cloud provider such as Google Maps or, as in
this case, OpenStreetMap. To do it, we will leverage the library
\texttt{ggmap}, which is designed to play nicely with the
\texttt{ggplot2} family (hence the seemingly counterintuitive example
above). Before we can plot them with the online map, we need to
reproject them though.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Reproject coordinates}
\NormalTok{wgs84 <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"}\NormalTok{)}
\NormalTok{db_wgs84 <-}\StringTok{ }\KeywordTok{spTransform}\NormalTok{(db, wgs84)}
\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\StringTok{'lon'}\NormalTok{] <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{1}\NormalTok{]}
\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\StringTok{'lat'}\NormalTok{] <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{coords[, }\DecValTok{2}\NormalTok{]}
\NormalTok{xys <-}\StringTok{ }\NormalTok{db_wgs84}\OperatorTok{@}\NormalTok{data[}\KeywordTok{c}\NormalTok{(}\StringTok{'lon'}\NormalTok{, }\StringTok{'lat'}\NormalTok{)]}
\CommentTok{# Bounding box}
\NormalTok{liv <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left =} \KeywordTok{min}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lon), }\DataTypeTok{bottom =} \KeywordTok{min}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lat), }
         \DataTypeTok{right =} \KeywordTok{max}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lon), }\DataTypeTok{top =} \KeywordTok{max}\NormalTok{(xys}\OperatorTok{$}\NormalTok{lat))}
\CommentTok{# Download map tiles}
\NormalTok{basemap <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(liv, }\DataTypeTok{zoom =} \DecValTok{12}\NormalTok{, }
                         \DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1325.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1326.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2013/1327.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2014/1327.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/12/2015/1327.png
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Overlay KDE}
\NormalTok{final <-}\StringTok{ }\KeywordTok{ggmap}\NormalTok{(basemap, }\DataTypeTok{extent =} \StringTok{"device"}\NormalTok{, }
               \DataTypeTok{maprange=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_density2d}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ db_wgs84}\OperatorTok{@}\NormalTok{data, }
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ lon, }\DataTypeTok{y =}\NormalTok{ lat, }
                    \DataTypeTok{alpha=}\NormalTok{..level.., }
                    \DataTypeTok{fill =}\NormalTok{ ..level..), }
                \DataTypeTok{size =} \FloatTok{0.01}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{16}\NormalTok{, }
                \DataTypeTok{geom =} \StringTok{'polygon'}\NormalTok{, }
                \DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_gradient2}\NormalTok{(}\StringTok{"Transaction}\CharTok{\textbackslash{}n}\StringTok{Density"}\NormalTok{, }
                       \DataTypeTok{low =} \StringTok{"#fffff8"}\NormalTok{, }
                       \DataTypeTok{high =} \StringTok{"#8da0cb"}\NormalTok{)}
\NormalTok{final}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}KDE of house transactions in Liverpool}
\end{figure}

The plot above\footnote{\textbf{EXERCISE} The map above uses the Stamen
  map \texttt{toner-lite}. Explore additional tile styles on their
  \href{http://maps.stamen.com/\#watercolor/12/37.7706/-122.3782}{website}
  and try to recreate the plot above.} allows us to not only see the
distribution of house transactions, but to relate it to what we know
about Liverpool, allowing us to establish many more connections than we
were previously able. Mainly, we can easily see that the area with a
highest volume of houses being sold is the city centre, with a ``hole''
around it that displays very few to no transactions and then several
pockets further away.

\section{Spatial Interpolation}\label{spatial-interpolation}

The previous section demonstrates how to visualize the distribution of a
set of spatial objects represented as points. In particular, given a
bunch of house transactions, it shows how one can effectively visualize
their distribution over space and get a sense of the density of
occurrences. Such visualization, because it is based on KDE, is based on
a smooth continuum, rather than on a discrete approach (as a choropleth
would do, for example).

Many times however, we are not particularly interested in learning about
the density of occurrences, but about the distribution of a given value
attached to each location. Think for example of weather stations and
temperature: the location of the stations is no secret and rarely
changes, so it is not of particular interest to visualize the density of
stations; what we are usually interested instead is to know how
temperature is distributed over space, given we only measure it in a few
places. One could argue the example we have been working with so far,
house price transactions, fits into this category as well: although
where house are sold may be of relevance, more often we are interested
in finding out what the ``surface of price'' looks like. Rather than
\emph{where are most houses being sold?} we usually want to know
\emph{where the most expensive or most affordable} houses are located.

In cases where we are interested in creating a surface of a given value,
rather than a simple density surface of occurrences, KDE cannot help us.
In these cases, what we are interested in is \emph{spatial
interpolation}, a family of techniques that aim at exactly that:
creating continuous surfaces for a particular phenomenon
(e.g.~temperature, house prices) given only a finite sample of
observations. Spatial interpolation is a large field of research that is
still being actively developed and that can involve a substantial amount
of mathematical complexity in order to obtain the most accurate
estimates possible\footnote{There is also an important economic
  incentive to do this: some of the most popular applications are in the
  oil and gas or mining industries. In fact, the very creator of this
  technique, \href{https://en.wikipedia.org/wiki/Danie_G._Krige}{Danie
  G. Krige}, was a mining engineer. His name is usually used to nickname
  spatial interpolation as \emph{kriging}.}. In this session, we will
introduce the simplest possible way of interpolating values, hoping this
will give you a general understanding of the methodology and, if you are
interested, you can check out further literature. For example,
\citet{banerjee2014hierarchical} or \citet{cressie2015statistics} are
hard but good overviews.

\subsection{Inverse Distance Weight (IDW)
interpolation}\label{inverse-distance-weight-idw-interpolation}

The technique we will cover here is called \emph{Inverse Distance
Weighting}, or IDW for convenience. \citet{comber2015} offer a good
description:

\begin{quote}
In the \emph{inverse distance weighting} (IDW) approach to
interpolation, to estimate the value of \(z\) at location \(x\) a
weighted mean of nearby observations is taken {[}\ldots{}{]}. To
accommodate the idea that observations of \(z\) at points closer to
\(x\) should be given more importance in the interpolation, greater
weight is given to these points {[}\ldots{}{]}

--- Page 204
\end{quote}

The math\footnote{Essentially, for any point \(x\) in space, the IDW
  estimate for value \(z\) is equivalent to
  \(\hat{z} (x) = \dfrac{\sum_i w_i z_i}{\sum_i w_i}\) where \(i\) are
  the observations for which we do have a value, and \(w_i\) is a weight
  given to location \(i\) based on its distance to \(x\).} is not
particularly complicated and may be found in detail elsewhere (the
reference above is a good starting point), so we will not spend too much
time on it. More relevant in this context is the intuition behind.
Essentially, the idea is that we will create a surface of house price by
smoothing many values arranged along a regular grid and obtained by
interpolating from the known locations to the regular grid locations.
This will give us full and equal coverage to soundly perform the
smoothing.

Enough chat, let's code.

From what we have just mentioned, there are a few steps to perform an
IDW spatial interpolation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a regular grid over the area where we have house transactions.
\item
  Obtain IDW estimates for each point in the grid, based on the values
  of \(k\) nearest neighbors.
\item
  Plot a smoothed version of the grid, effectively representing the
  surface of house prices.
\end{enumerate}

Let us go in detail into each of them\footnote{For the relevant
  calculations, we will be using the \texttt{gstat} library.}. First,
let us set up a grid:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liv.grid <-}\StringTok{ }\KeywordTok{spsample}\NormalTok{(db, }\DataTypeTok{type=}\StringTok{'regular'}\NormalTok{, }\DataTypeTok{n=}\DecValTok{25000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That's it, we're done! The function \texttt{spsample} hugely simplifies
the task by taking a spatial object and returning the grid we need. Not
a couple of additional arguments we pass: \texttt{type} allows us to get
a set of points that are \emph{uniformly} distributed over space, which
is handy for the later smoothing; \texttt{n} controls how many points we
want to create in that grid.

On to the IDW. Again, this is hugely simplified by \texttt{gstat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idw.hp <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{liv.grid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [inverse distance weighted interpolation]
\end{verbatim}

Boom! We've got it. Let us pause for a second to see how we just did it.
First, we pass \texttt{price\ \textasciitilde{}\ 1}. This specifies the
formula we are using to model house prices. The name on the left of
\texttt{\textasciitilde{}} represents the variable we want to explain,
while everything to its right captures the \emph{explanatory} variables.
Since we are considering the simplest possible case, we do not have
further variables to add, so we simply write \texttt{1}. Then we specify
the original locations for which we do have house prices (our original
\texttt{db} object), and the points where we want to interpolate the
house prices (the \texttt{liv.grid} object we just created above). One
more note: by default, \texttt{idw.hp} uses all the available
observations, weighted by distance, to provide an estimate for a given
point. If you want to modify that and restrict the maximum number of
neighbors to consider, you need to tweak the argument \texttt{nmax}, as
we do above by using the 150 neares observations to each point\footnote{Have
  a play with this because the results do change significantly. Can you
  reason why?}.

The object we get from \texttt{idw} is another spatial table, just as
\texttt{db}, containing the interpolated values. As such, we can inspect
it just as with any other of its kind. For example, to check out the top
of the estimated table:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(idw.hp}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   var1.pred var1.var
## 1  158087.3       NA
## 2  158197.9       NA
## 3  158311.3       NA
## 4  158427.6       NA
## 5  158547.0       NA
## 6  158669.7       NA
\end{verbatim}

The column we will pay attention to is \texttt{var1.pred}. And to see
the locations for which those correspond:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(idw.hp}\OperatorTok{@}\NormalTok{coords)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            x1       x2
## [1,] 333582.6 382745.7
## [2,] 333667.5 382745.7
## [3,] 333752.5 382745.7
## [4,] 333837.5 382745.7
## [5,] 333922.4 382745.7
## [6,] 334007.4 382745.7
\end{verbatim}

So, for a hypothetical house sold at the location in the first row of
\texttt{idw.hp@coords} (expressed in the OSGB coordinate system), the
price we would guess it would cost, based on the price of houses sold
nearby, is the first element of column \texttt{var1.pred} in
\texttt{idw.hp@data}.

\subsection{A surface of housing
prices}\label{a-surface-of-housing-prices}

Once we have the IDW object computed, we can plot it to explore the
distribution, not of house transactions in this case, but of house price
over the geography of Liverpool. The easiest way to do this is by
quickly calling the command \texttt{spplot}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{spplot}\NormalTok{(idw.hp[}\StringTok{'var1.pred'}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-19-1.pdf}

However, this is not entirely satisfactory for a number of reasons. Let
us get an equivalen plot with the package \texttt{tmap}, which
streamlines some of this and makes more aesthetically pleasant maps
easier to build as it follows a ``ggplot-y'' approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load up the layer}
\NormalTok{liv.otl <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\StringTok{'data/house_transactions'}\NormalTok{, }\StringTok{'liv_outline'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/house_transactions", layer: "liv_outline"
## with 1 features
## It has 1 fields
\end{verbatim}

The shape we will overlay looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qtm}\NormalTok{(liv.otl)}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-21-1.pdf}

Now let's give it a first go!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }
\NormalTok{p =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(liv.otl) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col=}\StringTok{'black'}\NormalTok{, }\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_shape}\NormalTok{(idw.hp) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_symbols}\NormalTok{(}\DataTypeTok{col=}\StringTok{'var1.pred'}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.25}\NormalTok{, }
             \DataTypeTok{border.lwd=}\DecValTok{0}\NormalTok{., }\DataTypeTok{palette=}\StringTok{'YlGn'}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-22-1.pdf}

The last two plots, however, are not really a surface, but a
representation of the points we have just estimated. To create a
surface, we need to do an interim transformation to convert the spatial
object \texttt{idw.hp} into a table that a ``surface plotter'' can
understand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xyz <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{coordinates}\NormalTok{(idw.hp)[, }\DecValTok{1}\NormalTok{], }
                  \DataTypeTok{y=}\KeywordTok{coordinates}\NormalTok{(idw.hp)[, }\DecValTok{2}\NormalTok{], }
                  \DataTypeTok{z=}\NormalTok{idw.hp}\OperatorTok{$}\NormalTok{var1.pred)}
\end{Highlighting}
\end{Shaded}

Now we are ready to plot the surface as a contour:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_contour}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{z=}\NormalTok{z))}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-24-1.pdf}
\caption{\label{fig:unnamed-chunk-24}Contour of prices in Liverpool}
\end{figure}

Which can also be shown as a filled contour:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_raster}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{z))}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-points_files/figure-latex/unnamed-chunk-25-1.pdf}

The problem here, when compared to the KDE above for example, is that a
few values are extremely large:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\DataTypeTok{x=}\NormalTok{z, }\DataTypeTok{geom=}\StringTok{'density'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-26-1.pdf}
\caption{\label{fig:unnamed-chunk-26}Skewness of prices in Liverpool}
\end{figure}

Let us then take the logarithm before we plot the surface:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xyz[}\StringTok{'lz'}\NormalTok{] <-}\StringTok{ }\KeywordTok{log}\NormalTok{(xyz}\OperatorTok{$}\NormalTok{z)}
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{xyz, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y))}
\NormalTok{surface <-}\StringTok{ }\NormalTok{base }\OperatorTok{+}
\StringTok{           }\KeywordTok{geom_raster}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{lz),}
                       \DataTypeTok{show.legend =}\NormalTok{ F)}
\NormalTok{surface}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-points_files/figure-latex/unnamed-chunk-27-1.pdf}
\caption{\label{fig:unnamed-chunk-27}Surface of log-prices in Liverpool}
\end{figure}

Now this looks better. We can start to tell some patterns. To bring in
context, it would be great to be able to add a basemap layer, as we did
for the KDE. This is conceptually very similar to what we did above,
starting by reprojecting the points and continuing by overlaying them on
top of the basemap. However, technically speaking it is not possible
because \texttt{ggmap} --the library we have been using to display tiles
from cloud providers-- does not play well with our own rasters (i.e.~the
price surface). At the moment, it is surprisingly tricky to get this to
work, so we will park it for now. However, developments such as the
\href{https://github.com/edzer/sfr}{\texttt{sf}} project promise to make
this easier in the future\footnote{\textbf{BONUS} if you can figure out
  a way to do it yourself!}.

\subsection{\texorpdfstring{\emph{``What should the next house's price
be?''}}{What should the next house's price be?}}\label{what-should-the-next-houses-price-be}

The last bit we will explore in this session relates to prediction for
new values. Imagine you are a real state data scientist and your boss
asks you to give an estimate of how much a new house going into the
market should cost. The only information you have to make such a guess
is the location of the house. In this case, the IDW model we have just
fitted can help you. The trick is realizing that, instead of creating an
entire grid, all we need is to obtain an estimate of a single location.

Let us say, the house is located on the coordinates
\texttt{x=340000,\ y=390000} as expressed in the GB National Grid
coordinate system. In that case, we can do as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt <-}\StringTok{ }\KeywordTok{SpatialPoints}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x=}\DecValTok{340000}\NormalTok{, }\DataTypeTok{y=}\DecValTok{390000}\NormalTok{),}
                    \DataTypeTok{proj4string =}\NormalTok{ db}\OperatorTok{@}\NormalTok{proj4string)}
\NormalTok{idw.one <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{pt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [inverse distance weighted interpolation]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idw.one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class       : SpatialPointsDataFrame 
## features    : 1 
## extent      : 340000, 340000, 390000, 390000  (xmin, xmax, ymin, ymax)
## crs         : +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs 
## variables   : 2
## names       :        var1.pred, var1.var 
## value       : 157099.029513871,       NA
\end{verbatim}

And, as show above, the estimated value is GBP157,099\footnote{\textbf{PRO
  QUESTION} Is that house expensive or cheap, as compared to the other
  houses sold in this dataset? Can you figure out where the house is?}.

Using this predictive logic, and taking advantage of Google Maps and its
geocoding capabilities, it is possible to devise a function that takes
an arbitrary address in Liverpool and, based on the transactions
occurred throughout 2014, provides an estimate of what the price for a
property in that location could be.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{how.much.is <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(address, }\DataTypeTok{print.message=}\OtherTok{TRUE}\NormalTok{)\{}
  \CommentTok{# Convert the address into Lon/Lat coordinates}
  \CommentTok{# }\AlertTok{NOTE}\CommentTok{: this now requires an API key}
  \CommentTok{# https://github.com/dkahle/ggmap#google-maps-and-credentials}
\NormalTok{  ll.pt <-}\StringTok{ }\KeywordTok{geocode}\NormalTok{(address)}
  \CommentTok{# Process as spatial table}
\NormalTok{  wgs84 <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"}\NormalTok{)}
\NormalTok{  ll.pt <-}\StringTok{ }\KeywordTok{SpatialPoints}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x=}\NormalTok{ll.pt}\OperatorTok{$}\NormalTok{lon, }\DataTypeTok{y=}\NormalTok{ll.pt}\OperatorTok{$}\NormalTok{lat),}
                      \DataTypeTok{proj4string =}\NormalTok{ wgs84)}
  \CommentTok{# Transform Lon/Lat into OSGB}
\NormalTok{  pt <-}\StringTok{ }\KeywordTok{spTransform}\NormalTok{(ll.pt, db}\OperatorTok{@}\NormalTok{proj4string)}
  \CommentTok{# Obtain prediction}
\NormalTok{  idw.one <-}\StringTok{ }\KeywordTok{idw}\NormalTok{(price }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{locations=}\NormalTok{db, }\DataTypeTok{newdata=}\NormalTok{pt)}
\NormalTok{  price <-}\StringTok{ }\NormalTok{idw.one}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{var1.pred}
  \CommentTok{# Return predicted price}
  \ControlFlowTok{if}\NormalTok{(print.message}\OperatorTok{==}\NormalTok{T)\{}
    \KeywordTok{writeLines}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{Based on what surrounding properties were sold"}\NormalTok{,}
                    \StringTok{"for in 2014 a house located at"}\NormalTok{, address, }\StringTok{"would"}\NormalTok{, }
                    \StringTok{"cost"}\NormalTok{,  }\KeywordTok{paste}\NormalTok{(}\StringTok{"GBP"}\NormalTok{, }\KeywordTok{round}\NormalTok{(price), }\StringTok{"."}\NormalTok{, }\DataTypeTok{sep=}\StringTok{''}\NormalTok{), }\StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{))}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(price)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Ready to test!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{address <-}\StringTok{ "74 Bedford St S, Liverpool, L69 7ZT, UK"}
\CommentTok{#p <- how.much.is(address)}
\end{Highlighting}
\end{Shaded}

\chapter{Flows}\label{flows}

This chapter\footnote{This chapter is part of \href{index.html}{Spatial
  Analysis Notes} {Flows -- Exploring flows visually and through spatial
  interaction} by Dani Arribas-Bel is licensed under a Creative Commons
  Attribution-NonCommercial-ShareAlike 4.0 International License.}
covers spatial interaction flows. Using open data from the city of San
Francisco about trips on its bikeshare system, we will estimate spatial
interaction models that try to capture and explain the variation in the
amount of trips on each given route. After visualizing the dataset, we
begin with a very simple model and then build complexity progressively
by augmenting it with more information, refined measurements, and better
modeling approaches. Throughout the chapter, we explore different ways
to grasp the predictive performance of each model. We finish with a
prediction example that illustrates how these models can be deployed in
a real-world application.

Content is based on the following references, which are great
follow-up's on the topic:

\begin{itemize}
\tightlist
\item
  \citet{gds_ua17}, an online short course on R for Geographic Data
  Science and Urban Analytics. In particular, the section on
  \href{https://github.com/alexsingleton/GDS_UA_2017/tree/master/Mapping_Flows}{mapping
  flows} is specially relevant here.
\item
  The predictive checks section draws heavily from
  \citet{gelman2006data}, in particular Chapters 6 and 7.
\end{itemize}

This tutorial is part of \href{index.html}{Spatial Analysis Notes}, a
compilation hosted as a GitHub repository that you can access in a few
ways:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of
  a \texttt{.zip} file that contains all the materials.
\item
  As an \href{https://gdsl-ul.github.io/san/flows.html}{html website}.
\item
  As a
  \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\section{Dependencies}\label{dependencies-2}

This tutorial relies on the following libraries that you will need to
have installed on your machine to be able to interactively follow
along\footnote{You can install package \texttt{mypackage} by running the
  command \texttt{install.packages("mypackage")} on the R prompt or
  through the \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...}
  menu in RStudio.}. Once installed, load them up with the following
commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Layout}
\KeywordTok{library}\NormalTok{(tufte)}
\CommentTok{# Spatial Data management}
\KeywordTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## rgdal: version: 1.4-8, (SVN revision 845)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
##  Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
##  GDAL binary built with GEOS: FALSE 
##  Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520]
##  Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
##  Linking to sp version: 1.3-2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pretty graphics}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap)}
\CommentTok{# Pretty maps}
\KeywordTok{library}\NormalTok{(ggmap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.
\end{verbatim}

\begin{verbatim}
## Please cite ggmap if you use it! See citation("ggmap") for details.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Simulation methods}
\KeywordTok{library}\NormalTok{(arm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
## Loading required package: lme4
\end{verbatim}

\begin{verbatim}
## 
## arm (Version 1.10-1, built: 2018-4-12)
\end{verbatim}

\begin{verbatim}
## Working directory is /Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san
\end{verbatim}

Before we start any analysis, let us set the path to the directory where
we are working. We can easily do that with \texttt{setwd()}. Please
replace in the following line the path to the folder where you have
placed this file -and where the \texttt{sf\_bikes} folder with the data
lives.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{'.'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Data}\label{data-1}

In this note, we will use data from the city of San Francisco
representing bike trips on their public bike share system. The original
source is the SF Open Data portal
(\href{http://www.bayareabikeshare.com/open-data}{link}) and the dataset
comprises both the location of each station in the Bay Area as well as
information on trips (station of origin to station of destination)
undertaken in the system from September 2014 to August 2015 and the
following year. Since this note is about modeling and not data
preparation, a cleanly reshaped version of the data, together with some
additional information, has been created and placed in the
\texttt{sf\_bikes} folder. The data file is named \texttt{flows.geojson}
and, in case you are interested, the (Python) code required to created
from the original files in the SF Data Portal is also available on the
\texttt{flows\_prep.ipynb} notebook
\href{https://github.com/darribas/spa_notes/blob/master/sf_bikes/flows_prep.ipynb}{{[}url{]}},
also in the same folder.

Let us then directly load the file with all the information necessary:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\StringTok{'./data/sf_bikes/flows.geojson'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: GeoJSON 
## Source: "/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/sf_bikes/flows.geojson", layer: "flows"
## with 1722 features
## It has 9 fields
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rownames}\NormalTok{(db}\OperatorTok{@}\NormalTok{data) <-}\StringTok{ }\NormalTok{db}\OperatorTok{$}\NormalTok{flow_id}
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{flow_id <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

Note how the interface is slightly different since we are reading a
\texttt{GeoJSON} file instead of a shapefile.

The data contains the geometries of the flows, as calculated from the
\href{https://developers.google.com/maps/}{Google Maps API}, as well as
a series of columns with characteristics of each flow:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(db}\OperatorTok{@}\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       dest orig straight_dist street_dist total_down total_up trips15 trips16
## 39-41   41   39      1452.201   1804.1150  11.205753 4.698162      68      68
## 39-42   42   39      1734.861   2069.1557  10.290236 2.897886      23      29
## 39-45   45   39      1255.349   1747.9928  11.015596 4.593927      83      50
## 39-46   46   39      1323.303   1490.8361   3.511543 5.038044     258     163
## 39-47   47   39       715.689    769.9189   0.000000 3.282495     127      73
## 39-48   48   39      1996.778   2740.1290  11.375186 3.841296      81      56
\end{verbatim}

where \texttt{orig} and \texttt{dest} are the station IDs of the origin
and destination, \texttt{street/straight\_dist} is the distance in
metres between stations measured along the street network or
as-the-crow-flies, \texttt{total\_down/up} is the total downhil and
climb in the trip, and \texttt{tripsXX} contains the amount of trips
undertaken in the years of study.

\section{\texorpdfstring{``\emph{Seeing}''
flows}{Seeing flows}}\label{seeing-flows}

The easiest way to get a quick preview of what the data looks like
spatially is to make a simple plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-5-1.pdf}
\caption{\label{fig:unnamed-chunk-5}Potential routes}
\end{figure}

Equally, if we want to visualize a single route, we can simply subset
the table. For example, to get the shape of the trip from station
\texttt{39} to station \texttt{48}, we can:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one39to48 <-}\StringTok{ }\NormalTok{db[ }\KeywordTok{which}\NormalTok{(}
\NormalTok{          db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{orig }\OperatorTok{==}\StringTok{ }\DecValTok{39} \OperatorTok{&}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{dest }\OperatorTok{==}\StringTok{ }\DecValTok{48}
\NormalTok{          ) , ]}
\KeywordTok{plot}\NormalTok{(one39to48)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-6-1.pdf}
\caption{\label{fig:unnamed-chunk-6}Trip from station 39 to 48}
\end{figure}

or, for the most popular route, we can:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{most_pop <-}\StringTok{ }\NormalTok{db[ }\KeywordTok{which}\NormalTok{(}
\NormalTok{          db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15 }\OperatorTok{==}\StringTok{ }\KeywordTok{max}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15)}
\NormalTok{          ) , ]}
\KeywordTok{plot}\NormalTok{(most_pop)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{03-flows_files/figure-latex/unnamed-chunk-7-1.pdf}
\caption{\label{fig:unnamed-chunk-7}Most popular trip}
\end{figure}

These however do not reveal a lot: there is no geographical context
(\emph{why are there so many routes along the NE?}) and no sense of how
volumes of bikers are allocated along different routes. Let us fix those
two.

The easiest way to bring in geographical context is by overlaying the
routes on top of a background map of tiles downloaded from the internet.
Let us download this using \texttt{ggmap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf_bb <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'x'}\NormalTok{, }\StringTok{'min'}\NormalTok{],}
           \DataTypeTok{right=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'x'}\NormalTok{, }\StringTok{'max'}\NormalTok{],}
           \DataTypeTok{bottom=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'y'}\NormalTok{, }\StringTok{'min'}\NormalTok{],}
           \DataTypeTok{top=}\NormalTok{db}\OperatorTok{@}\NormalTok{bbox[}\StringTok{'y'}\NormalTok{, }\StringTok{'max'}\NormalTok{])}
\NormalTok{SanFran <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(sf_bb, }
                         \DataTypeTok{zoom =} \DecValTok{14}\NormalTok{, }
                         \DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6330.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6331.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6332.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2620/6333.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2621/6333.png
\end{verbatim}

\begin{verbatim}
## Source : http://tile.stamen.com/toner-lite/14/2622/6333.png
\end{verbatim}

and make sure it looks like we intend it to look:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-9-1.pdf}

Now to combine tiles and routes, we need to pull out the coordinates
that make up each line. For the route example above, this would be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xys1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{coordinates}\NormalTok{(most_pop))}
\end{Highlighting}
\end{Shaded}

Now we can plot the route\footnote{\textbf{EXERCISE}: \emph{can you plot
  the route for the largest climb?}} (note we also dim down the
background to focus the attention on flows):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X1, }\DataTypeTok{y=}\NormalTok{X2), }
            \DataTypeTok{data=}\NormalTok{xys1,}
            \DataTypeTok{size=}\DecValTok{1}\NormalTok{,}
            \DataTypeTok{color=}\KeywordTok{rgb}\NormalTok{(}\FloatTok{0.996078431372549}\NormalTok{, }\FloatTok{0.7019607843137254}\NormalTok{, }\FloatTok{0.03137254901960784}\NormalTok{),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-11-1.pdf}

Now we can plot all of the lines by using a short \texttt{for} loop to
build up the table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up shell data.frame}
\NormalTok{lines <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{lat =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{lon =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{trips =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                    \DataTypeTok{id =} \KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{                    )}
\CommentTok{# Run loop}
\ControlFlowTok{for}\NormalTok{(x }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(db))\{}
  \CommentTok{# Pull out row}
\NormalTok{  r <-}\StringTok{ }\NormalTok{db[x, ]}
  \CommentTok{# Extract lon/lat coords}
\NormalTok{  xys <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{coordinates}\NormalTok{(r))}
  \KeywordTok{names}\NormalTok{(xys) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'lon'}\NormalTok{, }\StringTok{'lat'}\NormalTok{)}
  \CommentTok{# Insert trips and id}
\NormalTok{  xys[}\StringTok{'trips'}\NormalTok{] <-}\StringTok{ }\NormalTok{r}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15}
\NormalTok{  xys[}\StringTok{'id'}\NormalTok{] <-}\StringTok{ }\NormalTok{x}
  \CommentTok{# Append them to `lines`}
\NormalTok{  lines <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(lines, xys)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can go on and plot all of them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.75}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}
                \DataTypeTok{x=}\NormalTok{lon, }
                \DataTypeTok{y=}\NormalTok{lat,}
                \DataTypeTok{group=}\NormalTok{id}
\NormalTok{                ),}
            \DataTypeTok{data=}\NormalTok{lines,}
            \DataTypeTok{size=}\FloatTok{0.1}\NormalTok{,}
            \DataTypeTok{color=}\KeywordTok{rgb}\NormalTok{(}\FloatTok{0.996078431372549}\NormalTok{, }\FloatTok{0.7019607843137254}\NormalTok{, }\FloatTok{0.03137254901960784}\NormalTok{),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-13-1.pdf}

Finally, we can get a sense of the distribution of the flows by
associating a color gradient to each flow based on its number of trips:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(SanFran, }\DataTypeTok{darken=}\FloatTok{0.75}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}
                \DataTypeTok{x=}\NormalTok{lon, }
                \DataTypeTok{y=}\NormalTok{lat,}
                \DataTypeTok{group=}\NormalTok{id,}
                \DataTypeTok{colour=}\NormalTok{trips}
\NormalTok{                ),}
            \DataTypeTok{data=}\NormalTok{lines,}
            \DataTypeTok{size=}\KeywordTok{log1p}\NormalTok{(lines}\OperatorTok{$}\NormalTok{trips }\OperatorTok{/}\StringTok{ }\KeywordTok{max}\NormalTok{(lines}\OperatorTok{$}\NormalTok{trips)),}
            \DataTypeTok{lineend=}\StringTok{'round'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_gradient}\NormalTok{(}\DataTypeTok{low=}\StringTok{'grey'}\NormalTok{,}
                        \DataTypeTok{high=}\StringTok{'#07eda0'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_blank}\NormalTok{(),}
      \DataTypeTok{axis.text.y =} \KeywordTok{element_blank}\NormalTok{(),}
      \DataTypeTok{axis.ticks =} \KeywordTok{element_blank}\NormalTok{()}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-14-1.pdf}

Note how we transform the size so it's a proportion of the largest trip
and then it is compressed with a logarithm.

\section{Modelling flows}\label{modelling-flows}

Now we have an idea of the spatial distribution of flows, we can begin
to think about modeling them. The core idea in this section is to fit a
model that can capture the particular characteristics of our variable of
interest (the volume of trips) using a set of predictors that describe
the nature of a given flow. We will start from the simplest model and
then progressively build complexity until we get to a satisfying point.
Along the way, we will be exploring each model using concepts from
\citet{gelman2006data} such as predictive performance checks\footnote{For
  a more elaborate introduction to PPC, have a look at Chapters 7 and 8.}
(PPC)

Before we start running regressions, let us first standardize the
predictors so we can interpret the intercept as the average flow when
all the predictors take the average value, and so we can interpret the
model coefficients as changes in standard deviation units:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale all the table}
\NormalTok{db_std <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(db}\OperatorTok{@}\NormalTok{data))}
\CommentTok{# Reset trips as we want the original version}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{trips15 <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips15}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{trips16 <-}\StringTok{ }\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trips16}
\CommentTok{# Reset origin and destination station and express them as factors}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{orig <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{orig)}
\NormalTok{db_std}\OperatorTok{$}\NormalTok{dest <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{dest)}
\end{Highlighting}
\end{Shaded}

\textbf{Baseline model}

One of the simplest possible models we can fit in this context is a
linear model that explains the number of trips as a function of the
straight distance between the two stations and total amount of climb and
downhill. We will take this as the baseline on which we can further
build later:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down'}\NormalTok{, }\DataTypeTok{data=}\NormalTok{db_std)}
\KeywordTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "trips15 ~ straight_dist + total_up + total_down", 
##     data = db_std)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -261.9 -168.3 -102.4   30.8 3527.4 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    182.070      8.110  22.451  < 2e-16 ***
## straight_dist   17.906      9.108   1.966   0.0495 *  
## total_up       -44.100      9.353  -4.715 2.61e-06 ***
## total_down     -20.241      9.229  -2.193   0.0284 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 336.5 on 1718 degrees of freedom
## Multiple R-squared:  0.02196,    Adjusted R-squared:  0.02025 
## F-statistic: 12.86 on 3 and 1718 DF,  p-value: 2.625e-08
\end{verbatim}

To explore how good this model is, we will be comparing the predictions
the model makes about the number of trips each flow should have with the
actual number of trips. A first approach is to simply plot the
distribution of both variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check, point estimates - Baseline model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-17-1.pdf}

The plot makes pretty obvious that our initial model captures very few
aspects of the distribution we want to explain. However, we should not
get too attached to this plot just yet. What it is showing is the
distribution of predicted \emph{point} estimates from our model. Since
our model is not deterministic but inferential, there is a certain
degree of uncertainty attached to its predictions, and that is
completely absent from this plot.

Generally speaking, a given model has two sources of uncertainty:
\emph{predictive}, and \emph{inferential}. The former relates to the
fact that the equation we fit does not capture all the elements or in
the exact form they enter the true data generating process; the latter
has to do with the fact that we never get to know the true value of the
model parameters only guesses (estimates) subject to error and
uncertainty. If you think of our linear model above as

\[
T_{ij} = X_{ij}\beta + \epsilon_{ij}
\] where \(T_{ij}\) represents the number of trips undertaken between
station \(i\) and \(j\), \(X_{ij}\) is the set of explanatory variables
(length, climb, descent, etc.), and \(\epsilon_{ij}\) is an error term
assumed to be distributed as a normal distribution \(N(0, \sigma)\);
then predictive uncertainty comes from the fact that there are elements
to some extent relevant for \(y\) that are not accounted for and thus
subsummed into \(\epsilon_{ij}\). Inferential uncertainty comes from the
fact that we never get to know \(\beta\) but only an estimate of it
which is also subject to uncertainty itself.

Taking these two sources into consideration means that the black line in
the plot above represents only the behaviour of our model we expect if
the error term is absent (no predictive uncertainty) and the
coefficients are the true estimates (no inferential uncertainty).
However, this is not necessarily the case as our estimate for the
uncertainty of the error term is certainly not zero, and our estimates
for each parameter are also subject to a great deal of inferential
variability. we do not know to what extent other outcomes would be just
as likely. Predictive checking relates to simulating several feasible
scenarios under our model and use those to assess uncertainty and to get
a better grasp of the quality of our predictions.

Technically speaking, to do this, we need to build a mechanism to obtain
a possible draw from our model and then repeat it several times. The
first part of those two steps can be elegantly dealt with by writing a
short function that takes a given model and a set of predictors, and
produces a possible random draw from such model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate_draw <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m)\{}
  \CommentTok{# Set up predictors matrix}
\NormalTok{  x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(m)}
  \CommentTok{# Obtain draws of parameters (inferential uncertainty)}
\NormalTok{  sim_bs <-}\StringTok{ }\KeywordTok{sim}\NormalTok{(m, }\DecValTok{1}\NormalTok{)}
  \CommentTok{# Predicted value}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{x }\OperatorTok{%*%}\StringTok{ }\NormalTok{sim_bs}\OperatorTok{@}\NormalTok{coef[}\DecValTok{1}\NormalTok{, ]}
  \CommentTok{# Draw}
\NormalTok{  n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(mu)}
\NormalTok{  y_hat <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, mu, sim_bs}\OperatorTok{@}\NormalTok{sigma[}\DecValTok{1}\NormalTok{])}
  \KeywordTok{return}\NormalTok{(y_hat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function takes a model \texttt{m} and the set of covariates
\texttt{x} used and returns a random realization of predictions from the
model. To get a sense of how this works, we can get and plot a
realization of the model, compared to the expected one and the actual
values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_y <-}\StringTok{ }\KeywordTok{generate_draw}\NormalTok{(m1)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                      \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                      \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                      )}
\NormalTok{                   )}
\NormalTok{            ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(new_y), }
      \DataTypeTok{col=}\StringTok{'green'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{, }\StringTok{'Simulated'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{, }\StringTok{'green'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-19-1.pdf}

Once we have this ``draw engine'', we can set it to work as many times
as we want using a simple \texttt{for} loop. In fact, we can directly
plot these lines as compared to the expected one and the trip count:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{        ),}
     \DataTypeTok{col=}\StringTok{'white'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\CommentTok{# Loop for realizations}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{250}\NormalTok{)\{}
\NormalTok{  tmp_y <-}\StringTok{ }\KeywordTok{generate_draw}\NormalTok{(m1)}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(tmp_y),}
        \DataTypeTok{col=}\StringTok{'grey'}\NormalTok{,}
        \DataTypeTok{lwd=}\FloatTok{0.1}
\NormalTok{        )}
\NormalTok{\}}
\CommentTok{#}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(m1}\OperatorTok{$}\NormalTok{fitted.values), }
      \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Actual'}\NormalTok{, }\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Simulated (n=250)'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'red'}\NormalTok{, }\StringTok{'black'}\NormalTok{, }\StringTok{'grey'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Baseline model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-20-1.pdf}

The plot shows there is a significant mismatch between the fitted
values, which are much more concentrated around small positive values,
and the realizations of our ``inferential engine'', which depict a much
less concentrated distribution of values. This is likely due to the
combination of two different reasons: on the one hand, the accuracy of
our estimates may be poor, causing them to jump around a wide range of
potential values and hence resulting in very diverse predictions
(inferential uncertainty); on the other hand, it may be that the amount
of variation we are not able to account for in the model\footnote{The
  \(R^2\) of our model is around 2\%} is so large that the degree of
uncertainty contained in the error term of the model is very large,
hence resulting in such a flat predictive distribution.

It is important to keep in mind that the issues discussed in the
paragraph above relate only to the uncertainty behind our model, not to
the point predictions derived from them, which are a mechanistic result
of the minimization of the squared residuals and hence are not subject
to probability or inference. That allows them in this case to provide a
fitted distribution much more accurate apparently (black line above).
However, the lesson to take from this model is that, even if the point
predictions (fitted values) are artificially accurate\footnote{which
  they are not really, in light of the comparison between the black and
  red lines.}, our capabilities to infer about the more general
underlying process are fairly limited.

\textbf{Improving the model}

The bad news from the previous section is that our initial model is not
great at explaining bike trips. The good news is there are several ways
in which we can improve this. In this section we will cover three main
extensions that exemplify three different routes you can take when
enriching and augmenting models in general, and spatial interaction ones
in particular\footnote{These principles are general and can be applied
  to pretty much any modeling exercise you run into. The specific
  approaches we take in this note relate to spatial interaction models}.
These three routes are aligned around the following principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use better approximations to model your dependent variable.
\item
  Recognize the structure of your data.
\item
  Get better predictors.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Use better approximations to model your dependent variable}
\end{itemize}

Standard OLS regression assumes that the error term and, since the
predictors are deterministic, the dependent variable are distributed
following a normal (gaussian) distribution. This is usually a good
approximation for several phenomena of interest, but maybe not the best
one for trips along routes: for one, we know trips cannot be negative,
which the normal distribution does not account for\footnote{For an
  illustration of this, consider the amount of probability mass to the
  left of zero in the predictive checks above.}; more subtly, their
distribution is not really symmetric but skewed with a very long tail on
the right. This is common in variables that represent counts and that is
why usually it is more appropriate to fit a model that relies on a
distribution different from the normal.

One of the most common distributions for this cases is the Poisson,
which can be incorporated through a general linear model (or GLM). The
underlying assumption here is that instead of
\(T_{ij} \sim N(\mu_{ij}, \sigma)\), our model now follows:

\[
T_{ij} \sim Poisson (\exp^{X_{ij}\beta})
\]

As usual, such a model is easy to run in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson,}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

Now let's see how much better, if any, this approach is. To get a quick
overview, we can simply plot the point predictions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check, point estimates - Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-22-1.pdf}

To incorporate uncertainty to these predictions, we need to tweak our
\texttt{generate\_draw} function so it accommodates the fact that our
model is not linear anymore.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate_draw_poi <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m)\{}
  \CommentTok{# Set up predictors matrix}
\NormalTok{  x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(m)}
  \CommentTok{# Obtain draws of parameters (inferential uncertainty)}
\NormalTok{  sim_bs <-}\StringTok{ }\KeywordTok{sim}\NormalTok{(m, }\DecValTok{1}\NormalTok{)}
  \CommentTok{# Predicted value}
\NormalTok{  xb <-}\StringTok{ }\NormalTok{x }\OperatorTok{%*%}\StringTok{ }\NormalTok{sim_bs}\OperatorTok{@}\NormalTok{coef[}\DecValTok{1}\NormalTok{, ]}
  \CommentTok{#xb <- x %*% m$coefficients}
  \CommentTok{# Transform using the link function}
\NormalTok{  mu <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(xb)}
  \CommentTok{# Obtain a random realization}
\NormalTok{  y_hat <-}\StringTok{ }\KeywordTok{rpois}\NormalTok{(}\DataTypeTok{n=}\KeywordTok{length}\NormalTok{(mu), }\DataTypeTok{lambda=}\NormalTok{mu)}
  \KeywordTok{return}\NormalTok{(y_hat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And then we can examine both point predictions an uncertainty around
them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'white'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\CommentTok{# Loop for realizations}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{250}\NormalTok{)\{}
\NormalTok{  tmp_y <-}\StringTok{ }\KeywordTok{generate_draw_poi}\NormalTok{(m2)}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(tmp_y),}
        \DataTypeTok{col=}\StringTok{'grey'}\NormalTok{,}
        \DataTypeTok{lwd=}\FloatTok{0.1}
\NormalTok{        )}
\NormalTok{\}}
\CommentTok{#}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(m2}\OperatorTok{$}\NormalTok{fitted.values), }
      \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{, }\StringTok{'Simulated (n=250)'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{, }\StringTok{'grey'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-24-1.pdf}

Voila! Although the curve is still a bit off, centered too much to the
right of the actual data, our predictive simulation leaves the fitted
values right in the middle. This speaks to a better fit of the model to
the actual distribution othe original data follow.

\begin{itemize}
\tightlist
\item
  \textbf{Recognize the structure of your data}
\end{itemize}

So far, we've treated our dataset as if it was flat (i.e.~comprise of
fully independent realizations) when in fact it is not. Most crucially,
our baseline model does not account for the fact that every observation
in the dataset pertains to a trip between two stations. This means that
all the trips from or to the same station probably share elements which
likely help explain how many trips are undertaken between stations. For
example, think of trips to an from a station located in the famous
Embarcadero, a popular tourist spot. Every route to and from there
probably has more trips due to the popularity of the area and we are
currently not acknowledging it in the model.

A simple way to incorporate these effects into the model is through
origin and destination fixed effects. This approach shares elements with
both spatial fixed effects and multilevel modeling and essentially
consists of including a binary variable for every origin and destination
station. In mathematical notation, this equates to:

\[
T_{ij} = X_{ij}\beta + \delta_i + \delta_j + \epsilon_{ij}
\]

where \(\delta_i\) and \(\delta_j\) are origin and destination station
fixed effects\footnote{In this session, \(\delta_i\) and \(\delta_j\)
  are estimated as independent variables so their estimates are similar
  to interpret to those in \(\beta\). An alternative approach could be
  to model them as random effects in a multilevel framework.}, and the
rest is as above. This strategy accounts for all the unobserved
heterogeneity associated with the location of the station. Technically
speaking, we simply need to introduce \texttt{orig} and \texttt{dest} in
the the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ straight_dist + total_up + total_down + orig + dest'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson)}
\end{Highlighting}
\end{Shaded}

And with our new model, we can have a look at how well it does at
predicting the overall number of trips\footnote{Although, theoretically,
  we could also include simulations of the model in the plot to get a
  better sense of the uncertainty behind our model, in practice this
  seems troublesome. The problems most likely arise from the fact that
  many of the origin and destination binary variable coefficients are
  estimated with a great deal of uncertainty. This causes some of the
  simulation to generate extreme values that, when passed through the
  exponential term of the Poisson link function, cause problems. If
  anything, this is testimony of how a simple fixed effect model can
  sometimes lack accuracy and generate very uncertain estimates. A
  potential extension to work around these problems could be to fit a
  multilevel model with two specific levels beyond the trip-level: one
  for origin and another one for destination stations.}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m3}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m3}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Orig/dest FE Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-26-1.pdf}

That looks significantly better, doesn't it? In fact, our model now
better accounts for the long tail where a few routes take a lot of
trips. This is likely because the distribution of trips is far from
random across stations and our origin and destination fixed effects do a
decent job at accounting for that structure. However our model is still
notably underpredicting less popular routes and overpredicting routes
with above average number of trips. Maybe we should think about moving
beyond a simple linear model.

\begin{itemize}
\tightlist
\item
  \textbf{Get better predictors}
\end{itemize}

The final extension is, in principle, always available but, in practice,
it can be tricky to implement. The core idea is that your baseline model
might not have the best measurement of the phenomena you want to account
for. In our example, we can think of the distance between stations. So
far, we have been including the distance measured ``as the crow flies''
between stations. Although in some cases this is a good approximation
(particularly when distances are long and likely route taken is as close
to straight as possible), in some cases like ours, where the street
layout and the presence of elevation probably matter more than the
actual final distance pedalled, this is not necessarily a safe
assumption.

As an exampe of this approach, we can replace the straight distance
measurements for more refined ones based on the Google Maps API routes.
This is very easy as all we need to do (once the distances have been
calculated!) is to swap \texttt{straight\_dist} for
\texttt{street\_dist}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m4 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\StringTok{'trips15 ~ street_dist + total_up + total_down + orig + dest'}\NormalTok{, }
          \DataTypeTok{data=}\NormalTok{db_std,}
          \DataTypeTok{family=}\NormalTok{poisson)}
\end{Highlighting}
\end{Shaded}

And we can similarly get a sense of our predictive fitting with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(m4}\OperatorTok{$}\NormalTok{fitted.values), }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{100}\NormalTok{, }\KeywordTok{max}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(m4}\OperatorTok{$}\NormalTok{fitted.values)}\OperatorTok{$}\NormalTok{y), }
                  \KeywordTok{max}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15)}\OperatorTok{$}\NormalTok{y)}
\NormalTok{                  )}
\NormalTok{               )}
\NormalTok{      ),}
     \DataTypeTok{col=}\StringTok{'black'}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips15), }
      \DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}
      \DataTypeTok{main=}\StringTok{''}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{'Predicted'}\NormalTok{, }\StringTok{'Actual'}\NormalTok{),}
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{, }\StringTok{'red'}\NormalTok{),}
       \DataTypeTok{lwd=}\DecValTok{1}\NormalTok{)}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{main=}\StringTok{"Predictive check - Orig/dest FE Poisson model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-flows_files/figure-latex/unnamed-chunk-28-1.pdf}

Hard to tell any noticeable difference, right? To see if there is any,
we can have a look at the estimates obtained:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m4)}\OperatorTok{$}\NormalTok{coefficients[}\StringTok{'street_dist'}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Estimate     Std. Error        z value       Pr(>|z|) 
##  -9.961619e-02   2.688731e-03  -3.704952e+01  1.828096e-300
\end{verbatim}

And compare this to that of the straight distances in the previous
model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m3)}\OperatorTok{$}\NormalTok{coefficients[}\StringTok{'straight_dist'}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Estimate     Std. Error        z value       Pr(>|z|) 
##  -7.820014e-02   2.683052e-03  -2.914596e+01  9.399407e-187
\end{verbatim}

As we can see, the differences exist but ar not massive. Let's use this
example to learn how to interpret coefficients in a Poisson
model\footnote{See section 6.2 of \citet{gelman2006data} for a similar
  treatment of these.}. Effectively, these estimates can be understood
as multiplicative effects. Since our model fits

\[
T_{ij} \sim Poisson (\exp^{X_{ij}\beta})
\]

we need to transform \(\beta\) through an exponential in order to get a
sense of the effect of distance on the number of trips. This means that
for the street distance, our original estimate is
\(\beta_{street} = -0.0996\), but this needs to be translated through
the exponential into \(e^{-0.0996} = 0.906\). In other words, since
distance is expressed in standard deviations\footnote{Remember the
  transformation at the very beginning.}, we can expect a 10\% decrease
in the number of trips for an increase of one standard deviation (about
1Km) in the distance between the stations. This can be compared with
\(e^{-0.0782} = 0.925\) for the straight distances, or a reduction of
about 8\% the number of trips for every increase of a standard deviation
(about 720m).

\section{Predicting flows}\label{predicting-flows}

So far we have put all of our modeling efforts in understanding the
model we fit and improving such model so it fits our data as closely as
possible. This is essential in any modelling exercise but should be far
from a stopping point. Once we're confident our model is a decent
representation of the data generating process, we can start exploiting
it. In this section, we will cover one specific case that showcases how
a fitted model can help: out-of-sample forecasts.

It is August 2015, and you have just started working as a data scientist
for the bikeshare company that runs the San Francisco system. You join
them as they're planning for the next academic year and, in order to
plan their operations (re-allocating vans, station maintenance, etc.),
they need to get a sense of how many people are going to be pedalling
across the city and, crucially, \emph{where} they are going to be
pedalling through. What can you do to help them?

The easiest approach is to say ``well, a good guess for how many people
will be going between two given stations this coming year is how many
went through last year, isn't it?''. This is one prediction approach.
However, you could see how, even if the same process governs over both
datasets (2015 and 2016), each year will probably have some
idiosyncracies and thus looking too closely into one year might not give
the best possible answer for the next one. Ideally, you want a good
stylized synthesis that captures the bits that stay constant over time
and thus can be applied in the future and that ignores those aspects
that are too particular to a given point in time. That is the rationale
behind using a fitted model to obtain predictions.

However good any theory though, the truth is in the pudding. So, to see
if a modeling approach is better at producing forecasts than just using
the counts from last year, we can put them to a test. The way this is
done when evaluating the predictive performance of a model (as this is
called in the literature) relies on two basic steps: a) obtain
predictions from a given model and b) compare those to the actual values
(in our case, with the counts for 2016 in \texttt{trips16}) and get a
sense of ``how off'' they are. We have essentially covered a) above; for
b), there are several measures to use. We will use one of the most
common ones, the root mean squared error (RMSE), which roughly gives a
sense of the average difference between a predicted vector and the real
deal:

\[
RMSE = \sqrt{ \sum_{ij} (\hat{T_{ij}} - T_{ij})^2}
\]

where \(\hat{T_{ij}}\) is the predicted amount of trips between stations
\(i\) and \(j\). RMSE is straightforward in R and, since we will use it
a couple of times, let's write a short function to make our lives
easier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(t, p)\{}
\NormalTok{  se <-}\StringTok{ }\NormalTok{(t }\OperatorTok{-}\StringTok{ }\NormalTok{p)}\OperatorTok{^}\DecValTok{2}
\NormalTok{  mse <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(se)}
\NormalTok{  rmse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(mse)}
  \KeywordTok{return}\NormalTok{(rmse)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

where \texttt{t} stands for the vector of true values, and \texttt{p} is
the vector of predictions. Let's give it a spin to make sure it works:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmse_m4 <-}\StringTok{ }\KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, m4}\OperatorTok{$}\NormalTok{fitted.values)}
\NormalTok{rmse_m4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 256.2197
\end{verbatim}

That means that, on average, predictions in our best model \texttt{m4}
are 256 trips off. Is this good? Bad? Worse? It's hard to say but, being
practical, what we can say is whether this better than our alternative.
Let us have a look at the RMSE of the other models as well as that of
simply plugging in last year's counts:\footnote{\textbf{EXERCISE}: can
  you create a single plot that displays the distribution of the
  predicted values of the five different ways to predict trips in 2016
  and the actual counts of trips?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmses <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model=}\KeywordTok{c}\NormalTok{(}\StringTok{'OLS'}\NormalTok{, }\StringTok{'Poisson'}\NormalTok{, }\StringTok{'Poisson + FE'}\NormalTok{, }
                            \StringTok{'Poisson + FE + street dist.'}\NormalTok{,}
                            \StringTok{'Trips-2015'}
\NormalTok{                            ),}
                    \DataTypeTok{RMSE=}\KeywordTok{c}\NormalTok{(}\KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m1}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m2}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m3}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                              m4}\OperatorTok{$}\NormalTok{fitted.values),}
                           \KeywordTok{rmse}\NormalTok{(db_std}\OperatorTok{$}\NormalTok{trips16, }
\NormalTok{                                db_std}\OperatorTok{$}\NormalTok{trips15)}
\NormalTok{                           )}
\NormalTok{                      )}
\NormalTok{rmses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                         model     RMSE
## 1                         OLS 323.6135
## 2                     Poisson 320.8962
## 3                Poisson + FE 254.4468
## 4 Poisson + FE + street dist. 256.2197
## 5                  Trips-2015 131.0228
\end{verbatim}

The table is both encouraging and disheartning at the same time. On the
one hand, all the modeling techniques covered above behave as we would
expect: the baseline model displays the worst predicting power of all,
and every improvement (except the street distances!) results in notable
decreases of the RMSE. This is good news. However, on the other hand,
all of our modelling efforts fall short of given a better guess than
simply using the previous year's counts. \emph{Why? Does this mean that
we should not pay attention to modeling and inference?} Not really.
Generally speaking, a model is as good at predicting as it is able to
mimic the underlying process that gave rise to the data in the first
place. The results above point to a case where our model is not picking
up all the factors that determine the amount of trips undertaken in a
give route. This could be improved by enriching the model with
more/better predictors, as we have seen above. Also, the example above
seems to point to a case where those idiosyncracies in 2015 that the
model does not pick up seem to be at work in 2016 as well. This is great
news for our prediction efforts this time, but we have no idea why this
is the case and, for all that matters, it could change the coming year.
Besides the elegant quantification of uncertainty, the true advantage of
a modeling approach in this context is that, if well fit, it is able to
pick up the fundamentals that apply over and over. This means that, if
next year we're not as lucky as this one and previous counts are not
good predictors but the variables we used in our model continue to have
a role in determining the outcome, the data scientist should be luckier
and hit a better prediction.

\section{References}\label{references}

\chapter{Spatial Econometrics}\label{spatial-econometrics}

This chapter\footnote{{Spatial Econometrics} by Dani Arribas-Bel is
  licensed under a Creative Commons Attribution-NonCommercial-ShareAlike
  4.0 International License.} is based on the following references,
which are good follow-up's on the topic:

\begin{itemize}
\tightlist
\item
  \href{http://darribas.org/sdar_mini/notes/Class_03.html}{Session III}
  of \citet{arribas2014spatial}. Check the ``Related readings'' section
  on the session page for more in-depth discussions.
\item
  \citet{anselin2005spatial}, freely available to download
  {[}\href{http://csiss.org/GISPopSci/workshops/2011/PSU/readings/W15_Anselin2007.pdf}{\texttt{pdf}}{]}.
\item
  The second part of this tutorial assumes you have reviewed Lecture 5
  of \citet{darribas_gds19}.
  {[}\href{http://darribas.org/gds19/notes/Class_05.html}{html}{]}
\item
  A similar coverage of topics is presented in Chapter 11 of the
  upcoming book ``Geographic Data Science with Python'', which is
  currently in progress.
  {[}\href{https://geographicdata.science/book/notebooks/11_regression.html}{\texttt{url}}{]}
\end{itemize}

This tutorial is part of \href{index.html}{Spatial Analysis Notes}, a
compilation hosted as a GitHub repository that you can access it in a
few ways:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of
  a \texttt{.zip} file that contains all the materials.
\item
  As an
  \href{https://gdsl-ul.github.io/san/spatial-econometrics.html}{html
  website}.
\item
  As a
  \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\section{Dependencies}\label{dependencies-3}

The illustration below relies on the following libraries that you will
need to have installed on your machine to be able to interactively
follow along\footnote{You can install package \texttt{mypackage} by
  running the command \texttt{install.packages("mypackage")} on the R
  prompt or through the
  \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in
  RStudio.}. Once installed, load them up with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Layout}
\KeywordTok{library}\NormalTok{(tufte)}
\CommentTok{# For pretty table}
\KeywordTok{library}\NormalTok{(knitr)}
\CommentTok{# Spatial Data management}
\KeywordTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## rgdal: version: 1.4-8, (SVN revision 845)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
##  Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
##  GDAL binary built with GEOS: FALSE 
##  Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520]
##  Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
##  Linking to sp version: 1.3-2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pretty graphics}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Pretty maps}
\KeywordTok{library}\NormalTok{(ggmap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.
\end{verbatim}

\begin{verbatim}
## Please cite ggmap if you use it! See citation("ggmap") for details.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Various GIS utilities}
\KeywordTok{library}\NormalTok{(GISTools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: maptools
\end{verbatim}

\begin{verbatim}
## Checking rgeos availability: TRUE
\end{verbatim}

\begin{verbatim}
## Loading required package: RColorBrewer
\end{verbatim}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## Loading required package: rgeos
\end{verbatim}

\begin{verbatim}
## rgeos version: 0.5-2, (SVN revision 621)
##  GEOS runtime version: 3.7.2-CAPI-1.11.2 
##  Linking to sp version: 1.3-1 
##  Polygon checking: TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For all your interpolation needs}
\KeywordTok{library}\NormalTok{(gstat)}
\CommentTok{# For data manipulation}
\KeywordTok{library}\NormalTok{(plyr)}
\CommentTok{# Spatial regression}
\KeywordTok{library}\NormalTok{(spdep)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: spData
\end{verbatim}

\begin{verbatim}
## To access larger datasets in this package, install the spDataLarge
## package with: `install.packages('spDataLarge',
## repos='https://nowosad.github.io/drat/', type='source')`
\end{verbatim}

\begin{verbatim}
## Loading required package: sf
\end{verbatim}

\begin{verbatim}
## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0
\end{verbatim}

Before we start any analysis, let us set the path to the directory where
we are working. We can easily do that with \texttt{setwd()}. Please
replace in the following line the path to the folder where you have
placed this file -and where the \texttt{house\_transactions} folder with
the data lives.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd('/media/dani/baul/AAA/Documents/teaching/u-lvl/2016/envs453/code/GIT/kde_idw_r/')}
\KeywordTok{setwd}\NormalTok{(}\StringTok{'.'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Data}\label{data-2}

To explore ideas in spatial regression, we will be using house price
data for the municipality of Liverpool. Our main dataset is provided by
the Land Registry (as part of their
\href{https://www.gov.uk/government/collections/price-paid-data}{Price
Paid Data}) but has been cleaned and re-packaged into a shapefile by
Dani Arribas-Bel.

Let us load it up first of all:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hst <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\DataTypeTok{dsn =} \StringTok{'data/house_transactions'}\NormalTok{, }\DataTypeTok{layer =} \StringTok{'liv_house_trans'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/house_transactions", layer: "liv_house_trans"
## with 6324 features
## It has 18 fields
## Integer64 fields read as strings:  price
\end{verbatim}

The tabular component of the spatial frame contains the followig
variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(hst)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "pcds"       "id"         "price"      "trans_date" "type"      
##  [6] "new"        "duration"   "paon"       "saon"       "street"    
## [11] "locality"   "town"       "district"   "county"     "ppd_cat"   
## [16] "status"     "lsoa11"     "LSOA11CD"
\end{verbatim}

The meaning for most of the variables can be found in the original
\href{https://www.gov.uk/guidance/about-the-price-paid-data\#explanations-of-column-headers-in-the-ppd}{Land
Registry documentation}. The dataset contains transactions that took
place during 2,014:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Format dates}
\NormalTok{dts <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(hst}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{trans_date)}
\CommentTok{# Set up summary table}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(dts)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Min.      1st Qu.       Median         Mean      3rd Qu.         Max. 
## "2014-01-02" "2014-04-11" "2014-07-09" "2014-07-08" "2014-10-03" "2014-12-30"
\end{verbatim}

Although the original Land Registry data contain some characteristics of
the house, all of them are categorical: \emph{is the house newly built?
What type of property is it?} To bring in a richer picture and
illustrate how continuous variables can also be included in a spatial
setting, we will augment the original transaction data with Deprivation
indices from the
\href{https://data.cdrc.ac.uk/dataset/cdrc-english-indices-of-deprivation-2015-geodata-pack-liverpool-e08000012}{CDRC}
at the
\href{http://neighbourhood.statistics.gov.uk/HTMLDocs/nessgeography/superoutputareasexplained/output-areas-explained.htm}{Lower
Layer Super Output Area} (LSOA) level.

Let us read the \texttt{csv} in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imd <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/house_transactions/E08000012.csv'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The table contains not only the overall IMD score and rank, but some of
the component scores, as well as the \texttt{LSOA} code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(imd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "LSOA11CD"   "imd_rank"   "imd_score"  "income"     "employment"
##  [6] "education"  "health"     "crime"      "housing"    "living_env"
## [11] "idaci"      "idaopi"
\end{verbatim}

That bit of information, \texttt{LSOA11CD}, is crucial to be able to
connect it to each house transaction. To ``join'' both tables, we can
use the base command \texttt{merge}, which will assign values from
\texttt{imd} into \texttt{hst} making sure that each house transaction
get the IMD data for the LSOA where it is located:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(hst, imd)}
\end{Highlighting}
\end{Shaded}

The resulting table, \texttt{db}, contains variables from both original
tables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "LSOA11CD"   "pcds"       "id"         "price"      "trans_date"
##  [6] "type"       "new"        "duration"   "paon"       "saon"      
## [11] "street"     "locality"   "town"       "district"   "county"    
## [16] "ppd_cat"    "status"     "lsoa11"     "imd_rank"   "imd_score" 
## [21] "income"     "employment" "education"  "health"     "crime"     
## [26] "housing"    "living_env" "idaci"      "idaopi"
\end{verbatim}

Since we will heavily rely on \texttt{price}, we need to turn it into a
numeric column, rather than as a factor, which is how it is picked up:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{price))}
\end{Highlighting}
\end{Shaded}

For some of our analysis, we will need the coarse postcode of each
house, rather than the finely specified one in the original data. This
means using the available one

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(db}\OperatorTok{@}\NormalTok{data[}\StringTok{'pcds'}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      pcds
## 62 L1 0AB
## 63 L1 0AB
## 64 L1 0AB
## 65 L1 0AB
## 66 L1 0AB
## 67 L1 0AB
\end{verbatim}

to create a new column that only contains the first bit of the postcode
(\texttt{L1} in the examples above). The following lines of code will do
that for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{$}\NormalTok{pc <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(}\KeywordTok{strsplit}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(db}\OperatorTok{$}\NormalTok{pcds), }\DataTypeTok{split=}\StringTok{" "}\NormalTok{), }\StringTok{"["}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Given there are 6,324 transactions in the dataset, a simple plot of the
point coordinates implicitly draws the shape of the Liverpool
municipality:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(db)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-spatial_econometrics_files/figure-latex/unnamed-chunk-13-1.pdf}
\caption{\label{fig:unnamed-chunk-13}Spatial distribution of house
transactions in Liverpool}
\end{figure}

\section{Non-spatial regression, a
refresh}\label{non-spatial-regression-a-refresh}

Before we discuss how to explicitly include space into the linear
regression framework, let us show how basic regression can be carried
out in R, and how you can begin to interpret the results. By no means is
this a formal and complete introduction to regression so, if that is
what you are looking for, I suggest the first part of
\citet{gelman2006data}, in particular chapters 3 and 4.

The core idea of linear regression is to explain the variation in a
given (\emph{dependent}) variable as a linear function of a series of
other (\emph{explanatory}) variables. For example, in our case, we may
want to express/explain the price of a house as a function of whether it
is new and the degree of deprivation of the area where it is located. At
the individual level, we can express this as:

\[
P_i = \alpha + \beta_1 NEW_i + \beta_2 IMD_i + \epsilon_i
\]

where \(P_i\) is the price of house \(i\), \(NEW_i\) is a binary
variable that takes one if the house is newly built or zero otherwise
and \(IMD_i\) is the IMD score of the LSOA where \(i\) is located. The
parameters \(\beta_1\), \(\beta_2\), and \(\beta_3\) give us information
about in which way and to what extent each variable is related to the
price, and \(\alpha\), the constant term, is the average house price
when all the other variables are zero. The term \(\epsilon_i\) is
usually referred to as ``error'' and captures elements that influence
the price of a house but are not whether the house is new or the IMD
score of its area. We can also express this relation in matrix form,
excluding subindices for \(i\)\footnote{In this case, the equation would
  look like \[P = \alpha + \beta_1 NEW + \beta_2 IMD + \epsilon\] and
  would be interpreted in terms of vectors and matrices instead of
  scalar values.}.

Essentially, a regression can be seen as a multivariate extension of
simple bivariate correlations. Indeed, one way to interpret the
\(\beta_k\) coefficients in the equation above is as the degree of
correlation between the explanatory variable \(k\) and the dependent
variable, \emph{keeping all the other explanatory variables constant}.
When you calculate simple bivariate correlations, the coefficient of a
variable is picking up the correlation between the variables, but it is
also subsuming into it variation associated with other correlated
variables --also called confounding factors\footnote{\textbf{EXAMPLE}
  Assume that new houses tend to be built more often in areas with low
  deprivation. If that is the case, then \(NEW\) and \(IMD\) will be
  correlated with each other (as well as with the price of a house, as
  we are hypothesizing in this case). If we calculate a simple
  correlation between \(P\) and \(IMD\), the coefficient will represent
  the degree of association between both variables, but it will also
  include some of the association between \(IMD\) and \(NEW\). That is,
  part of the obtained correlation coefficient will be due not to the
  fact that higher prices tend to be found in areas with low IMD, but to
  the fact that new houses tend to be more expensive. This is because
  (in this example) new houses tend to be built in areas with low
  deprivation and simple bivariate correlation cannot account for that.}.
Regression allows you to isolate the distinct effect that a single
variable has on the dependent one, once we \emph{control} for those
other variables.

Practically speaking, running linear regressions in \texttt{R} is
straightforward. For example, to fit the model specified in the equation
above, we only need one line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'price ~ new + imd_score'}\NormalTok{, db)}
\end{Highlighting}
\end{Shaded}

We use the command \texttt{lm}, for linear model, and specify the
equation we want to fit using a string that relates the dependent
variable (\texttt{price}) with a set of explanatory ones (\texttt{new}
and \texttt{price}) by using a tilde \texttt{\textasciitilde{}} that is
akin the \(=\) symbol in the mathematical equation. Since we are using
names of variables that are stored in a table, we need to pass the table
object (\texttt{db}) as well.

In order to inspect the results of the model, the quickest way is to
call \texttt{summary}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "price ~ new + imd_score", data = db)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -184254   -59948   -29032    11430 26434741 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   235596      13326  17.679  < 2e-16 ***
## newY            4926      19104   0.258    0.797    
## imd_score      -2416        308  -7.843 5.12e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 509000 on 6321 degrees of freedom
## Multiple R-squared:  0.009712,   Adjusted R-squared:  0.009398 
## F-statistic: 30.99 on 2 and 6321 DF,  p-value: 4.027e-14
\end{verbatim}

A full detailed explanation of the output is beyond the scope of this
note, so we will focus on the relevant bits for our main purpose. This
is concentrated on the \texttt{Coefficients} section, which gives us the
estimates for the \(\beta_k\) coefficients in our model. Or, in other
words, the coefficients are the raw equivalent of the correlation
coefficient between each explanatory variable and the dependent one,
once the polluting effect of confounding factors has been accounted
for\footnote{Keep in mind that regression is no magic. We are only
  discounting the effect of other confounding factors that we include in
  the model, not of \emph{all} potentially confounding factors.}.
Results are as expected for the most part: houses tend to be
significantly more expensive in areas with lower deprivation (an average
of GBP2,416 for every additional score); and a newly built house is on
average GBP4,926 more expensive, although this association cannot be
ruled out to be random (probably due to the small relative number of new
houses).

Finally, before we jump into introducing space in our models, let us
modify our equation slightly to make it more useful when it comes to
interpretating it. Many house price models in the literature is
estimated in log-linear terms:

\[
\log{P_i} = \alpha + \beta_1 NEW_i + \beta_2 IMD_i + \epsilon_i
\]

This allows to interpret the coefficients more directly: as the
percentual change induced by a unit increase in the explanatory variable
of the estimate. To fit such a model, we can specify the logarithm of a
given variable directly in the formula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'log(price) ~ new + imd_score'}\NormalTok{, db)}
\KeywordTok{summary}\NormalTok{(m2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "log(price) ~ new + imd_score", data = db)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3172 -0.3231 -0.0149  0.3063  5.2769 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 12.2037784  0.0138634  880.28   <2e-16 ***
## newY         0.2456446  0.0198740   12.36   <2e-16 ***
## imd_score   -0.0169702  0.0003204  -52.96   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.5295 on 6321 degrees of freedom
## Multiple R-squared:  0.3101, Adjusted R-squared:  0.3099 
## F-statistic:  1421 on 2 and 6321 DF,  p-value: < 2.2e-16
\end{verbatim}

Looking at the results we can see a couple of differences with respect
to the original specification. First, the estimates are substantially
different numbers. This is because, although they consider the same
variable, the look at it from different angles, and provide different
interpretations. For example, the coefficient for the IMD, instead of
being interpretable in terms of GBP, the unit of the dependent variable,
it represents a percentage: a unit increase in the degree of deprivation
is associated with a 0.2\% decrease in the price of a house.\footnote{\textbf{EXERCISE}
  \emph{How does the type of a house affect the price at which it is
  sold, given whether it is new and the level of deprivation of the area
  where it is located?} To answer this, fit a model as we have done but
  including additionally the variable \texttt{type}. In order to
  interpret the codes, check the reference at the
  \href{https://www.gov.uk/guidance/about-the-price-paid-data\#explanations-of-column-headers-in-the-ppd}{Land
  Registry documentation}.} Second, the variable \texttt{new} is
significant in this case. This is probably related to the fact that, by
taking logs, we are also making the dependent variable look more normal
(Gaussian) and that allows the linear model to provide a better fit and,
hence, more accurate estimates. In this case, a house being newly built,
as compared to an old house, is overall 25\% more expensive.

\section{Spatial regression: a (very) first
dip}\label{spatial-regression-a-very-first-dip}

Spatial regression is about \emph{explicitly} introducing space or
geographical context into the statistical framework of a regression.
Conceptually, we want to introduce space into our model whenever we
think it plays an important role in the process we are interested in, or
when space can act as a reasonable proxy for other factors we cannot but
should include in our model. As an example of the former, we can imagine
how houses at the seafront are probably more expensive than those in the
second row, given their better views. To illustrate the latter, we can
think of how the character of a neighborhood is important in determining
the price of a house; however, it is very hard to identify and quantify
``character'' perse, although it might be easier to get at its spatial
variation, hence a case of space as a proxy.

Spatial regression is a large field of development in the econometrics
and statistics literatures. In this brief introduction, we will consider
two related but very different processes that give rise to spatial
effects: spatial heterogeneity and spatial dependence. For more rigorous
treatments of the topics introduced here, the reader is referred to
\citet{anselin2003spatial}, \citet{anselin2014modern}, and
\citet{gibbons2014spatial}.

\section{Spatial heterogeneity}\label{spatial-heterogeneity}

Spatial heterogeneity (SH) arises when we cannot safely assume the
process we are studying operates under the same ``rules'' throughout the
geography of interest. In other words, we can observe SH when there are
effects on the outcome variable that are intrinsically linked to
specific locations. A good example of this is the case of seafront
houses above: we are trying to model the price of a house and, the fact
some houses are located under certain conditions (i.e.~by the sea),
makes their price behave differently\footnote{\textbf{QUESTION} How
  would you incorporate this into a regression model that extends the
  log-log equation of the previous section?}.

This somewhat abstract concept of SH can be made operational in a model
in several ways. We will explore the following two: spatial
fixed-effects (FE); and spatial regimes, which is a generalization of
FE.

\textbf{Spatial FE}

Let us consider the house price example from the previous section to
introduce a more general illustration that relates to the second
motivation for spatial effects (``space as a proxy''). Given we are only
including two explanatory variables in the model, it is likely we are
missing some important factors that play a role at determining the price
at which a house is sold. Some of them, however, are likely to vary
systematically over space (e.g.~different neighborhood characteristics).
If that is the case, we can control for those unobserved factors by
using traditional dummy variables but basing their creation on a spatial
rule. For example, let us include a binary variable for every two-digit
postcode in Liverpool, indicating whether a given house is located
within such area (\texttt{1}) or not (\texttt{0}). Mathematically, we
are now fitting the following equation:

\[
\log{P_i} = \alpha_r + \beta_1 NEW_i + \beta_2 IMD_i + \epsilon_i
\]

where the main difference is that we are now allowing the constant term,
\(\alpha\), to vary by postcode \(r\), \(\alpha_r\).

Programmatically, this is straightforward to estimate:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Include `-1` to eliminate the constant term and include a dummy for every area}
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'log(price) ~ pc + new + imd_score - 1'}\NormalTok{, db)}
\KeywordTok{summary}\NormalTok{(m3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "log(price) ~ pc + new + imd_score - 1", data = db)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2680 -0.2833 -0.0235  0.2599  5.6714 
## 
## Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
## pcL1      11.697166   0.032137  363.98   <2e-16 ***
## pcL10     11.893524   0.076471  155.53   <2e-16 ***
## pcL11     11.902319   0.043194  275.55   <2e-16 ***
## pcL12     12.065776   0.029692  406.37   <2e-16 ***
## pcL13     11.865523   0.034893  340.06   <2e-16 ***
## pcL14     11.920922   0.044480  268.01   <2e-16 ***
## pcL15     12.039916   0.028555  421.64   <2e-16 ***
## pcL16     12.295535   0.034950  351.81   <2e-16 ***
## pcL17     12.275339   0.027978  438.75   <2e-16 ***
## pcL18     12.396475   0.024534  505.27   <2e-16 ***
## pcL19     12.162630   0.029697  409.56   <2e-16 ***
## pcL2      12.061443   0.100805  119.65   <2e-16 ***
## pcL20     11.928142   0.226932   52.56   <2e-16 ***
## pcL24     11.868363   0.045785  259.22   <2e-16 ***
## pcL25     12.234462   0.028557  428.42   <2e-16 ***
## pcL27     12.035241   0.092832  129.65   <2e-16 ***
## pcL28     11.438267   0.206323   55.44   <2e-16 ***
## pcL3      11.954453   0.029561  404.40   <2e-16 ***
## pcL4      11.718609   0.039575  296.11   <2e-16 ***
## pcL5      12.037267   0.055952  215.14   <2e-16 ***
## pcL6      11.898506   0.042918  277.24   <2e-16 ***
## pcL7      11.855374   0.044681  265.33   <2e-16 ***
## pcL8      12.034093   0.039801  302.36   <2e-16 ***
## pcL9      11.759056   0.033610  349.87   <2e-16 ***
## newY       0.310829   0.020947   14.84   <2e-16 ***
## imd_score -0.012008   0.000517  -23.23   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.5007 on 6298 degrees of freedom
## Multiple R-squared:  0.9981, Adjusted R-squared:  0.9981 
## F-statistic: 1.305e+05 on 26 and 6298 DF,  p-value: < 2.2e-16
\end{verbatim}

Econometrically speaking, what the postcode FE we have introduced imply
is that, instead of comparing all house prices across Liverpool as
equal, we only derive variation from within each postcode\footnote{Additionally,
  estimating spatial FE in our particular example also gives you an
  indirect measure of area \emph{desirability}: since they are simple
  dummies in a regression explaining the price of a house, their
  estimate tells us about how much people are willing to pay to live in
  a given area. However, this interpretation does not necessarily apply
  in other contexts where introducing spatial FEs does make sense.
  \textbf{EXERCISE} \emph{What is the most desired area to live in
  Liverpool?}}. Remember that the interpretation of a \(\beta_k\)
coefficient is the effect of variable \(k\), \emph{given all the other
explanatory variables included remain constant}. By including a single
variable for each area, we are effectively forcing the model to compare
as equal only house prices that share the same value for each variable;
in other words, only houses located within the same area. Introducing FE
affords you a higher degree of isolation of the effects of the variables
you introduce in your model because you can control for unobserved
effects that align spatially with the distribution of the FE you
introduce (by postcode, in our case).

\textbf{Spatial regimes}

At the core of estimating spatial FEs is the idea that, instead of
assuming the dependent variable behaves uniformly over space, there are
systematic effects following a geographical pattern that affect its
behaviour. In other words, spatial FEs introduce econometrically the
notion of spatial heterogeneity. They do this in the simplest possible
form: by allowing the constant term to vary geographically. The other
elements of the regression are left untouched and hence apply uniformly
across space. The idea of spatial regimes (SRs) is to generalize the
spatial FE approach to allow not only the constant term to vary but also
any other explanatory variable. This implies that the equation we will
be estimating is:

\[
\log{P_i} = \alpha_r + \beta_{1r} NEW_i + \beta_{2r} IMD_i + \epsilon_i
\]

where we are not only allowing the constant term to vary by region
(\(\alpha_r\)), but also every other parameter (\(\beta_{kr}\)).

In R terms, this is more straightforward to estimate if \texttt{new} is
expressed as \texttt{0} and \texttt{1}, rather than as factors:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a new variable `newB` to store the binary form of `new`}
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{newB <-}\StringTok{ }\DecValTok{1}
\NormalTok{db[db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{new}\OperatorTok{==}\StringTok{'N'}\NormalTok{, }\StringTok{'newB'}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

Also, given we are going to allow \emph{every} coefficient to vary by
regime, we will need to explicitly set a constant term that we can allow
to vary:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{$}\NormalTok{one <-}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

Then, the estimation leverages the capabilities in model description of
R formulas:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# `:` notation implies interaction variables}
\NormalTok{m4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'log(price) ~ 0 +(one + newB + imd_score):(pc)'}\NormalTok{, db)}
\KeywordTok{summary}\NormalTok{(m4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "log(price) ~ 0 +(one + newB + imd_score):(pc)", 
##     data = db)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2051 -0.2506 -0.0182  0.2198  5.3507 
## 
## Coefficients: (8 not defined because of singularities)
##                  Estimate Std. Error t value Pr(>|t|)    
## one:pcL1        11.983460   0.098675 121.444  < 2e-16 ***
## one:pcL10       11.911281   0.504210  23.624  < 2e-16 ***
## one:pcL11       11.774865   0.160864  73.198  < 2e-16 ***
## one:pcL12       12.157627   0.068165 178.356  < 2e-16 ***
## one:pcL13       11.821576   0.103946 113.728  < 2e-16 ***
## one:pcL14       11.821895   0.115040 102.763  < 2e-16 ***
## one:pcL15       12.317524   0.052051 236.645  < 2e-16 ***
## one:pcL16       12.761711   0.098709 129.287  < 2e-16 ***
## one:pcL17       12.240652   0.065074 188.103  < 2e-16 ***
## one:pcL18       12.695103   0.056510 224.654  < 2e-16 ***
## one:pcL19       12.489604   0.054177 230.533  < 2e-16 ***
## one:pcL2        12.164848   0.321558  37.831  < 2e-16 ***
## one:pcL20       11.069199   0.211083  52.440  < 2e-16 ***
## one:pcL24       10.876938   0.135326  80.376  < 2e-16 ***
## one:pcL25       12.427878   0.051147 242.982  < 2e-16 ***
## one:pcL27       11.022835   0.435730  25.297  < 2e-16 ***
## one:pcL28       11.300143   1.126387  10.032  < 2e-16 ***
## one:pcL3        11.977708   0.054781 218.647  < 2e-16 ***
## one:pcL4        11.653464   0.109113 106.802  < 2e-16 ***
## one:pcL5        11.544085   0.277000  41.675  < 2e-16 ***
## one:pcL6        11.574617   0.158022  73.247  < 2e-16 ***
## one:pcL7        11.617544   0.128402  90.478  < 2e-16 ***
## one:pcL8        11.586224   0.085036 136.251  < 2e-16 ***
## one:pcL9        11.837544   0.080452 147.139  < 2e-16 ***
## newB:pcL1       -0.298430   0.051078  -5.843 5.40e-09 ***
## newB:pcL10             NA         NA      NA       NA    
## newB:pcL11       0.672479   0.089894   7.481 8.40e-14 ***
## newB:pcL12       0.977903   0.114281   8.557  < 2e-16 ***
## newB:pcL13      -0.659910   0.473773  -1.393 0.163705    
## newB:pcL14       0.610482   0.132911   4.593 4.45e-06 ***
## newB:pcL15       0.744844   0.139732   5.331 1.01e-07 ***
## newB:pcL16             NA         NA      NA       NA    
## newB:pcL17      -0.094580   0.095188  -0.994 0.320449    
## newB:pcL18      -0.197115   0.122109  -1.614 0.106521    
## newB:pcL19       0.393901   0.089607   4.396 1.12e-05 ***
## newB:pcL2              NA         NA      NA       NA    
## newB:pcL20             NA         NA      NA       NA    
## newB:pcL24       0.735316   0.072739  10.109  < 2e-16 ***
## newB:pcL25       0.196018   0.144527   1.356 0.175063    
## newB:pcL27       0.356678   0.189946   1.878 0.060457 .  
## newB:pcL28             NA         NA      NA       NA    
## newB:pcL3       -0.261461   0.055228  -4.734 2.25e-06 ***
## newB:pcL4        1.087380   0.079152  13.738  < 2e-16 ***
## newB:pcL5        0.519945   0.094850   5.482 4.38e-08 ***
## newB:pcL6        0.695856   0.062461  11.141  < 2e-16 ***
## newB:pcL7              NA         NA      NA       NA    
## newB:pcL8        0.334517   0.059368   5.635 1.83e-08 ***
## newB:pcL9              NA         NA      NA       NA    
## imd_score:pcL1  -0.010489   0.003383  -3.101 0.001938 ** 
## imd_score:pcL10 -0.012413   0.011381  -1.091 0.275485    
## imd_score:pcL11 -0.010637   0.003005  -3.540 0.000404 ***
## imd_score:pcL12 -0.017296   0.002625  -6.588 4.81e-11 ***
## imd_score:pcL13 -0.011004   0.002185  -5.036 4.89e-07 ***
## imd_score:pcL14 -0.010432   0.002469  -4.225 2.42e-05 ***
## imd_score:pcL15 -0.020737   0.001413 -14.681  < 2e-16 ***
## imd_score:pcL16 -0.050522   0.007703  -6.559 5.85e-11 ***
## imd_score:pcL17 -0.009763   0.002214  -4.410 1.05e-05 ***
## imd_score:pcL18 -0.032289   0.003799  -8.499  < 2e-16 ***
## imd_score:pcL19 -0.024629   0.001860 -13.242  < 2e-16 ***
## imd_score:pcL2  -0.016601   0.013654  -1.216 0.224084    
## imd_score:pcL20        NA         NA      NA       NA    
## imd_score:pcL24  0.004090   0.002391   1.711 0.087139 .  
## imd_score:pcL25 -0.020461   0.001981 -10.327  < 2e-16 ***
## imd_score:pcL27  0.006626   0.007561   0.876 0.380911    
## imd_score:pcL28 -0.009473   0.020367  -0.465 0.641850    
## imd_score:pcL3  -0.006935   0.001747  -3.970 7.28e-05 ***
## imd_score:pcL4  -0.012032   0.001731  -6.952 3.96e-12 ***
## imd_score:pcL5  -0.006051   0.004009  -1.509 0.131280    
## imd_score:pcL6  -0.008324   0.002367  -3.517 0.000440 ***
## imd_score:pcL7  -0.007517   0.002342  -3.209 0.001337 ** 
## imd_score:pcL8  -0.004064   0.001561  -2.603 0.009255 ** 
## imd_score:pcL9  -0.014124   0.002052  -6.882 6.47e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.472 on 6260 degrees of freedom
## Multiple R-squared:  0.9984, Adjusted R-squared:  0.9983 
## F-statistic: 5.966e+04 on 64 and 6260 DF,  p-value: < 2.2e-16
\end{verbatim}

As we can see, there are a few \texttt{NA} values (e.g. \texttt{pcL10}).
This has to do with the fact that there are not that many new houses, so
some of the buckets in which the regimes split the data to estimate each
parameter are empty. This can be readily seen by obtaining a quick cross
tabulation of \texttt{pc} and \texttt{new}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(db}\OperatorTok{$}\NormalTok{pc, db}\OperatorTok{$}\NormalTok{new)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
##         N   Y
##   L1  161 189
##   L10  47   0
##   L11 192  34
##   L12 326  18
##   L13 387   1
##   L14 144  19
##   L15 466  12
##   L16 212   0
##   L17 398  27
##   L18 439  16
##   L19 329  32
##   L2   25   0
##   L20   5   0
##   L24  97  84
##   L25 357  11
##   L27  22  10
##   L28   6   0
##   L3  272 101
##   L4  437  41
##   L5   97  46
##   L6  319  78
##   L7  201   0
##   L8  227 110
##   L9  329   0
\end{verbatim}

To illustrate a correct regime estimation, we can focus only on
\texttt{imd\_score}\footnote{Note this still returns a \texttt{NA} for
  the IMD estimate in \texttt{L20}. This is most likely due to the
  little amount of houses (five) sold in that area. The regression
  nevertheless serves the illustration}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# `:` notation implies interaction variables}
\NormalTok{m5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'log(price) ~ 0 + (one + imd_score):pc'}\NormalTok{, db)}
\KeywordTok{summary}\NormalTok{(m5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "log(price) ~ 0 + (one + imd_score):pc", data = db)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.4952 -0.2779 -0.0221  0.2477  5.4973 
## 
## Coefficients: (1 not defined because of singularities)
##                   Estimate Std. Error t value Pr(>|t|)    
## one:pcL1        11.8965065  0.1030279 115.469  < 2e-16 ***
## one:pcL10       11.9112807  0.5325447  22.367  < 2e-16 ***
## one:pcL11       11.6203268  0.1684973  68.964  < 2e-16 ***
## one:pcL12       12.2106579  0.0716973 170.309  < 2e-16 ***
## one:pcL13       11.8099973  0.1094358 107.917  < 2e-16 ***
## one:pcL14       12.1229804  0.0998496 121.412  < 2e-16 ***
## one:pcL15       12.3753226  0.0537697 230.154  < 2e-16 ***
## one:pcL16       12.7617106  0.1042555 122.408  < 2e-16 ***
## one:pcL17       12.2248054  0.0666348 183.460  < 2e-16 ***
## one:pcL18       12.7033951  0.0594382 213.724  < 2e-16 ***
## one:pcL19       12.4767606  0.0571384 218.360  < 2e-16 ***
## one:pcL2        12.1648479  0.3396283  35.818  < 2e-16 ***
## one:pcL20       11.0691989  0.2229447  49.650  < 2e-16 ***
## one:pcL24       11.5214275  0.1260747  91.386  < 2e-16 ***
## one:pcL25       12.4351502  0.0537240 231.464  < 2e-16 ***
## one:pcL27       11.3770205  0.4148632  27.424  < 2e-16 ***
## one:pcL28       11.3001428  1.1896847   9.498  < 2e-16 ***
## one:pcL3        11.8873513  0.0542344 219.185  < 2e-16 ***
## one:pcL4        11.4097020  0.1137109 100.340  < 2e-16 ***
## one:pcL5        10.9973874  0.2729459  40.291  < 2e-16 ***
## one:pcL6        11.1816689  0.1626918  68.729  < 2e-16 ***
## one:pcL7        11.6175445  0.1356173  85.664  < 2e-16 ***
## one:pcL8        11.5086161  0.0886286 129.852  < 2e-16 ***
## one:pcL9        11.8375435  0.0849726 139.310  < 2e-16 ***
## imd_score:pcL1  -0.0131286  0.0035407  -3.708 0.000211 ***
## imd_score:pcL10 -0.0124128  0.0120210  -1.033 0.301837    
## imd_score:pcL11 -0.0058373  0.0031009  -1.882 0.059824 .  
## imd_score:pcL12 -0.0173734  0.0027727  -6.266 3.95e-10 ***
## imd_score:pcL13 -0.0107902  0.0023022  -4.687 2.83e-06 ***
## imd_score:pcL14 -0.0160870  0.0022604  -7.117 1.23e-12 ***
## imd_score:pcL15 -0.0219205  0.0014734 -14.878  < 2e-16 ***
## imd_score:pcL16 -0.0505217  0.0081354  -6.210 5.63e-10 ***
## imd_score:pcL17 -0.0093980  0.0023061  -4.075 4.65e-05 ***
## imd_score:pcL18 -0.0333885  0.0039476  -8.458  < 2e-16 ***
## imd_score:pcL19 -0.0228257  0.0019160 -11.913  < 2e-16 ***
## imd_score:pcL2  -0.0166013  0.0144213  -1.151 0.249707    
## imd_score:pcL20         NA         NA      NA       NA    
## imd_score:pcL24 -0.0020546  0.0024420  -0.841 0.400163    
## imd_score:pcL25 -0.0205241  0.0020921  -9.810  < 2e-16 ***
## imd_score:pcL27  0.0020943  0.0075687   0.277 0.782014    
## imd_score:pcL28 -0.0094733  0.0215112  -0.440 0.659671    
## imd_score:pcL3  -0.0061811  0.0018374  -3.364 0.000773 ***
## imd_score:pcL4  -0.0066455  0.0017803  -3.733 0.000191 ***
## imd_score:pcL5   0.0039338  0.0037726   1.043 0.297115    
## imd_score:pcL6  -0.0004620  0.0023860  -0.194 0.846480    
## imd_score:pcL7  -0.0075165  0.0024737  -3.039 0.002387 ** 
## imd_score:pcL8  -0.0006921  0.0015228  -0.455 0.649469    
## imd_score:pcL9  -0.0141241  0.0021676  -6.516 7.79e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4985 on 6277 degrees of freedom
## Multiple R-squared:  0.9982, Adjusted R-squared:  0.9982 
## F-statistic: 7.282e+04 on 47 and 6277 DF,  p-value: < 2.2e-16
\end{verbatim}

This allows us to get a separate constant term and estimate of the
impact of IMD on the price of a house \emph{for every
postcode}\footnote{\textbf{PRO EXERCISE} \emph{How does the effect of
  IMD vary over space?} You can answer this by looking at the
  coefficients of \texttt{imd\_score} over postcodes, but it would be
  much clearer if you could create a choropleth of the house locations
  where each dot is colored based on the value of the
  \texttt{imd\_score} estimated for that postcode.}.

\section{Spatial dependence}\label{spatial-dependence}

As we have just discussed, SH is about effects of phenomena that are
\emph{explicitly linked} to geography and that hence cause spatial
variation and clustering of values. This encompasses many of the kinds
of spatial effects we may be interested in when we fit linear
regressions. However, in other cases, our interest is on the effect of
the \emph{spatial configuration} of the observations, and the extent to
which that has an effect on the outcome we are considering. For example,
we might think that the price of a house not only depends on the level
of deprivation where the house is located, but also whether is close to
other highly deprived areas. This kind of spatial effect is
fundamentally different from SH in that is it not related to inherent
characteristics of the geography but relates to the characteristics of
the observations in our dataset and, specially, to their spatial
arrangement. We call this phenomenon by which the values of observations
are related to each other through distance \emph{spatial dependence}
\citep{anselin1988spatial}.

\textbf{Spatial Weights}

There are several ways to introduce spatial dependence in an econometric
framework, with varying degrees of econometric sophistication
\citep[see][ for a good overview]{anselin2003spatial}. Common to all of
them however is the way space is formally encapsulated: through
\emph{spatial weights matrices (\(W\))}\footnote{If you need to refresh
  your knowledge on spatial weight matrices, check
  \href{http://darribas.org/gds15/notes/Class_05.html}{Lecture 5} of
  \citet{darribas_gds15}}. These are \(NxN\) matrices with zero
diagonals and every \(w_{ij}\) cell with a value that represents the
degree of spatial connectivity/interaction between observations \(i\)
and \(j\). If they are not connected at all, \(w_{ij}=0\), otherwise
\(w_{ij}>0\) and we call \(i\) and \(j\) neighbors. The exact value in
the latter case depends on the criterium we use to define neighborhood
relations. These matrices also tend to be row-standardized so the sum of
each row equals to one.

A related concept to spatial weight matrices is that of \emph{spatial
lag}. This is an operator that multiplies a given variable \(y\) by a
spatial weight matrix:

\[
y_{lag} = W y
\]

If \(W\) is row-standardized, \(y_{lag}\) is effectively the average
value of \(y\) in the neighborhood of each observation. The individual
notation may help clarify this:

\[
y_{lag-i} = \displaystyle \sum_j w_{ij} y_j
\]

where \(y_{lag-i}\) is the spatial lag of variable \(y\) at location
\(i\), and \(j\) sums over the entire dataset. If \(W\) is
row-standardized, \(y_{lag-i}\) becomes an average of \(y\) weighted by
the spatial criterium defined in \(W\).

Given that spatial weights matrices are not the focus of this tutorial,
we will stick to a very simple case. Since we are dealing with points,
we will use \(K\)-nn weights, which take the \(k\) nearest neighbors of
each observation as neighbors and assign a value of one, assigning
everyone else a zero. We will use \(k=150\) to get a good degree of
variation and sensible results. If your computer is struggles to compute
the following lines of code, you can replace 50 by a lowed number.
Technically speaking is the same thing, but the probability that you
will pick up only houses in the same LSOA (and hence with exactly the
same IMD score) will be higher.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Because some rows are different units on the same house, slightly}
\CommentTok{# jitter the locations to break ties}
\NormalTok{xy.jit <-}\StringTok{ }\KeywordTok{jitter}\NormalTok{(db}\OperatorTok{@}\NormalTok{coords)}
\CommentTok{# Create knn list of each house}
\NormalTok{hnn <-}\StringTok{ }\KeywordTok{knearneigh}\NormalTok{(xy.jit, }\DataTypeTok{k=}\DecValTok{50}\NormalTok{)}
\CommentTok{# Create nb object}
\NormalTok{hnb <-}\StringTok{ }\KeywordTok{knn2nb}\NormalTok{(hnn)}
\CommentTok{# Create spatial weights matrix (note it row-standardizes by default)}
\NormalTok{hknn <-}\StringTok{ }\KeywordTok{nb2listw}\NormalTok{(hnb)}
\end{Highlighting}
\end{Shaded}

We can inspect the weights created by simply typing the name of the
object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hknn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Characteristics of weights list object:
## Neighbour list object:
## Number of regions: 6324 
## Number of nonzero links: 316200 
## Percentage nonzero weights: 0.7906388 
## Average number of links: 50 
## Non-symmetric neighbours list
## 
## Weights style: W 
## Weights constants summary:
##      n       nn   S0      S1       S2
## W 6324 39992976 6324 230.464 25810.73
\end{verbatim}

\textbf{Exogenous spatial effects}

Let us come back to the house price example we have been working with.
So far, we have hypothesized that the price of a house sold in Liverpool
can be explained using information about whether it is newly built, the
level of deprivation of the area where it is located, and its postcode.
However, it is also reasonable to think that prospective house owners
care about the larger area around a house, not only about its immediate
surroundings, and would be willing to pay more for a house that was
close to nicer areas, everything else being equal. How could we test
this idea?

The most straightforward way to introduce spatial dependence in a
regression is by considering not only a given explanatory variable, but
also its spatial lag. In our example case, in addition to including the
level of deprivation in the area of the house, we will include its
spatial lag. In other words, we will be saying that it is not only the
level of deprivation of the area where a house is located but also that
of the surrounding locations that helps explain the final price at which
a house is sold. Mathematically, this implies estimating the following
model:

\[
\log{P_i} = \alpha + \beta_{1} NEW_i + \beta_{2} IMD_i + \beta_{3} IMD_{lag-i} + \epsilon_i
\]

Let us first compute the spatial lag of imd\_score:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{w_imd_score <-}\StringTok{ }\KeywordTok{lag.listw}\NormalTok{(hknn, db}\OperatorTok{@}\NormalTok{data}\OperatorTok{$}\NormalTok{imd_score)}
\end{Highlighting}
\end{Shaded}

And then we can include it in our previous specification. Note that we
apply the log to the lag, not the reverse:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# `:` notation implies interaction variables}
\NormalTok{m6 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\StringTok{'log(price) ~ new + imd_score + w_imd_score'}\NormalTok{, db)}
\KeywordTok{summary}\NormalTok{(m6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = "log(price) ~ new + imd_score + w_imd_score", data = db)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2906 -0.3016 -0.0153  0.2817  5.2631 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 12.2813156  0.0145273 845.393  < 2e-16 ***
## newY         0.2475670  0.0195179  12.684  < 2e-16 ***
## imd_score   -0.0042075  0.0008916  -4.719 2.42e-06 ***
## w_imd_score -0.0148166  0.0009684 -15.300  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.52 on 6320 degrees of freedom
## Multiple R-squared:  0.3348, Adjusted R-squared:  0.3345 
## F-statistic:  1060 on 3 and 6320 DF,  p-value: < 2.2e-16
\end{verbatim}

As we can see, the lag is not only significative and negative (as
expected), but its effect seems to be even larger that that of the house
itself. Taken literally, this would imply that prospective owners value
more the area of the surrounding houses than that of the actual house
they buy. However, it is important to remember how these variables have
been constructed and what they really represent. Because the IMD score
is not exactly calculated at the house level, but at the area level,
many of the surrounding houses will share that so, to some extent, the
IMD of neighboring houses is that of the house itself\footnote{\textbf{EXERCISE}
  \emph{How do results change if you modify the number of neighbors
  included to compute the \(K\)-nn spatial weight matrix?} Replace the
  originak \(k\) used and re-run the regressions. Try to interpret the
  results and the (potential) differences with the original ones.}. This
is likely to be affecting the final parameter, and it is a reminder and
an illustration that we cannot take model results as universal truth but
we need to use them as tools to inform analysis, couple with theory and
what we know about the particular question of analysis. Nevertheless,
the example does illustrate how to introduce spatial dependence in a
regression framework in a fairly straight forward way.

\textbf{A note on more advanced spatial regression}

Introducing a spatial lag of an explanatory variable, as we have just
seen, is the most straightforward way of incorporating the notion of
spatial dependence in a linear regression framework. It does not require
additional changes, it can be estimated with OLS, and the interpretation
is rather similar to interpreting non-spatial variables. The field of
spatial econometrics however is a much broader one and has produced over
the last decades many techniques to deal with spatial effects and
spatial dependence in different ways. Although this might be an over
simplification, one can say that most of such efforts for the case of a
single cross-section are focused on two main variations: the spatial lag
and the spatial error model. Both are similar to the case we have seen
in that they are based on the introduction of a spatial lag, but they
differ in the component of the model they modify and affect.

The spatial lag model introduces a spatial lag of the \emph{dependent}
variable. In the example we have covered, this would translate into:

\[
\log{P_i} = \alpha + \rho \log{P_{lag-i}} + \beta_{1} NEW_i + \beta_{2} IMD_i + \epsilon_i
\]

Although it might not seem very different from the previous equation,
this model violates the exogeneity assumption, crucial for OLS to work.

Equally, the spatial error model includes a spatial lag in the
\emph{error} term of the equation:

\[
\log{P_i} = \alpha + \beta_{1r} NEW_i + \beta_{2r} IMD_i + u_i
\]

\[
u_i = u_{lag-i} + \epsilon_i
\]

Again, although similar, one can show this specification violates the
assumptions about the error term in a classical OLS model.

Both the spatial lag and error model violate some of the assumptions on
which OLS relies and thus render the technique unusable. Much of the
efforts have thus focused on coming up with alternative methodologies
that allow unbiased, robust, and efficient estimation of such models. A
survey of those is beyond the scope of this note, but the interested
reader is referred to \citet{anselin1988spatial},
\citet{anselin2003spatial}, and \citet{anselin2014modern} for further
reference.

\section{Predicting house prices}\label{predicting-house-prices}

So far, we have seen how exploit the output of a regression model to
evaluate the role different variables play in explaining another one of
interest. However, once fit, a model can also be used to obtain
predictions of the dependent variable given a new set of values for the
explanatory variables. We will finish this session by dipping our toes
in predicting with linear models.

The core idea is that once you have estimates for the way in which the
explanatory variables can be combined to explain the dependent one, you
can plug new values on the explanatory side of the model and combine
them following the model estimates to obtain predictions. In the example
we have worked with, you can imagine this application would be useful to
obtain valuations of a house, given we know the IMD of the area where
the house is located and whether it is a newly built house or not.

Conceptually, predicting in linear regression models involves using the
estimates of the parameters to obtain a value for the dependent
variable:

\[
\bar{\log{P_i}} = \bar{\alpha} + \bar{\beta_{1r}} NEW_i^* + \bar{\beta_{2r}} IMD_i^*
\]

where \(\bar{\log{P_i}}\) is our predicted value, and we include the
\(\bar{}\) sign to note that it is our estimate obtained from fitting
the model. We use the \(^*\) sign to note that those can be new values
for the explanatory variables, not necessarily those used to fit the
model.

Technically speaking, prediction in linear models is fairly streamlined
in R. Suppose we are given data for a new house which is to be put in
the market. We know it is been newly built on an area with an IMD score
of 75, but surrounded by areas that, on average, have a score of 50. Let
us record the data first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new.house <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{new=}\StringTok{'Y'}\NormalTok{, }\DataTypeTok{imd_score=}\DecValTok{75}\NormalTok{, }\DataTypeTok{w_imd_score=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To obtain the prediction for its price, we can use the \texttt{predict}
method:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new.price <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(m6, new.house)}
\NormalTok{new.price}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 11.47249
\end{verbatim}

Now remember we were using the log of the price as dependent variable.
If we want to recover the actual price of the house, we need to take its
exponent:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(new.price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 96037.09
\end{verbatim}

According to our model, the house would be worth GBP96,060.29\footnote{\textbf{EXERCISE}
  \emph{How would the price change if the surrounding houses did not
  have an average of 50 but of 80?} Obtain a new prediction and compare
  it with the original one.}.

\section{References}\label{references-1}

\chapter{Multilevel Modelling - Part
1}\label{multilevel-modelling---part-1}

This chapter\footnote{This note is part of \href{index.html}{Spatial
  Analysis Notes} {Multilevel Modelling -- Random Intercept Multilevel
  Model} by Francisco Rowe is licensed under a Creative Commons
  Attribution-NonCommercial-ShareAlike 4.0 International License.}
provides an introduction to multi-level data structures and multi-level
modelling.

The content of this chapter is based on:

\begin{itemize}
\item
  \citet{Gelman_Hill_2006_book} provides an excellent and intuitive
  explanation of multilevel modelling and data analysis in general. Read
  Part 2A for a really good explanation of multilevel models.
\item
  \citet{bristol2020} is an useful online resource on multilevel
  modelling and is free!
\end{itemize}

This Chapter is part of \href{index.html}{Spatial Analysis Notes}, a
compilation hosted as a GitHub repository that you can access it in a
few ways:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of
  a \texttt{.zip} file that contains all the materials.
\item
  As an
  \href{https://gdsl-ul.github.io/san/multilevel-modelling-part-1.html}{html
  website}.
\item
  As a
  \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\section{Dependencies}\label{dependencies-4}

This chapter uses the following libraries: Ensure they are installed on
your machine\footnote{You can install package \texttt{mypackage} by
  running the command \texttt{install.packages("mypackage")} on the R
  prompt or through the
  \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in
  RStudio.} before loading them executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Data manipulation, transformation and visualisation}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{# Nice tables}
\KeywordTok{library}\NormalTok{(kableExtra)}
\CommentTok{# Simple features (a standardised way to encode vector data ie. points, lines, polygons)}
\KeywordTok{library}\NormalTok{(sf) }
\CommentTok{# Spatial objects conversion}
\KeywordTok{library}\NormalTok{(sp) }
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap) }
\CommentTok{# Colour palettes}
\KeywordTok{library}\NormalTok{(RColorBrewer) }
\CommentTok{# More colour palettes}
\KeywordTok{library}\NormalTok{(viridis) }\CommentTok{# nice colour schemes}
\CommentTok{# Fitting multilevel models}
\KeywordTok{library}\NormalTok{(lme4)}
\CommentTok{# Tools for extracting information generated by lme4}
\KeywordTok{library}\NormalTok{(merTools)}
\CommentTok{# Exportable regression tables}
\KeywordTok{library}\NormalTok{(jtools)}
\KeywordTok{library}\NormalTok{(stargazer)}
\KeywordTok{library}\NormalTok{(sjPlot)}
\end{Highlighting}
\end{Shaded}

\section{Data}\label{data-3}

For this chapter, we will data for Liverpool from England's 2011 Census.
The original source is the
\href{https://www.nomisweb.co.uk/home/census2001.asp}{Office of National
Statistics} and the dataset comprises a number of selected variables
capturing demographic, health and socio-economic attributes of the local
resident population at four geographic levels: Output Area (OA), Lower
Super Output Area (LSOA), Middle Super Output Area (MSOA) and Local
Authority District (LAD). The variables include population counts and
percentages. For a description of the variables, see the readme file in
the mlm data folder.\footnote{Read the file in R by executing
  \texttt{read\_tsv("data/mlm/readme.txt")}}

Let us read the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# clean workspace}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{# read data}
\NormalTok{oa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/mlm/oa.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now attach and visualise the structure of the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# attach data frame}
\KeywordTok{attach}\NormalTok{(oa_shp)}

\CommentTok{# sort data by oa}
\NormalTok{oa_shp <-}\StringTok{ }\NormalTok{oa_shp[}\KeywordTok{order}\NormalTok{(oa_cd),]}
\KeywordTok{head}\NormalTok{(oa_shp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 6 features and 19 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 335056 ymin: 389163 xmax: 336155 ymax: 389642
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
##       oa_cd   lsoa_cd   msoa_cd    lad_cd      ward_nm  dstrt_nm    cnty_nm
## 1 E00032987 E01006515 E02001383 E08000012    Riverside Liverpool Merseyside
## 2 E00032988 E01006514 E02001383 E08000012 Princes Park Liverpool Merseyside
## 3 E00032989 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside
## 4 E00032990 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside
## 5 E00032991 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside
## 6 E00032992 E01033768 E02001383 E08000012 Princes Park Liverpool Merseyside
##   cntry_nm pop     age_60     unemp      lat      long    males   lt_ill
## 1  England 198 0.11616162 0.1130435 53.39821 -2.976786 46.46465 19.19192
## 2  England 348 0.16954023 0.1458333 53.39813 -2.969072 58.33333 33.62069
## 3  England 333 0.09009009 0.1049724 53.39778 -2.965290 64.26426 23.72372
## 4  England 330 0.15151515 0.1329787 53.39802 -2.963597 59.69697 23.03030
## 5  England 320 0.04687500 0.1813725 53.39706 -2.968030 60.62500 25.00000
## 6  England 240 0.05833333 0.2519685 53.39679 -2.966494 57.91667 28.33333
##     Bhealth VBhealth  no_qual   manprof                       geometry
## 1  6.565657 1.515152 24.69136  7.643312 MULTIPOLYGON (((335187 3894...
## 2 10.344828 1.436782 14.84848 13.375796 MULTIPOLYGON (((335834 3895...
## 3  6.606607 2.102102 15.38462 10.204082 MULTIPOLYGON (((335975.2 38...
## 4  5.151515 2.424242 17.91531 15.224913 MULTIPOLYGON (((336030.8 38...
## 5  8.750000 2.187500 12.58278 11.333333 MULTIPOLYGON (((335804.9 38...
## 6  6.666667 2.916667 27.47748  5.479452 MULTIPOLYGON (((335804.9 38...
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figs/ch5/datastr.png}
\caption{Fig. 1. Data Structure.}
\end{figure}

The data are hierarchically structured: OAs nested within LSOAs; LSOAs
nested within MSOAs; and, MSOAs nested within LADs. Observations nested
within higher geographical units may be correlated.

This is one type of hierarchical structure. There is a range of data
structures:

\begin{itemize}
\item
  Strict nested data structures eg. an individual unit is nested within
  only one higher unit
\item
  Repeated measures structures eg. various measurements for an
  individual unit
\item
  Crossed classified structures eg. individuals may work and live in
  different neighbourhoods
\item
  Multiple membership structure eg. individuals may have two different
  work places
\end{itemize}

\emph{Why should we care about the structure of the data?}

\begin{itemize}
\item
  \emph{Draw correct statistical inference}: Failing to recognise
  hierarchical structures will lead to underestimated standard errors of
  regression coefficients and an overstatement of statistical
  significance. Standard errors for the coefficients of higher-level
  predictor variables will be the most affected by ignoring grouping.
\item
  \emph{Link context to individual units}: We can link and understand
  the extent of group effects on individual outcomes eg. how belonging
  to a certain socio-economic group influences on future career
  opportunities.
\item
  \emph{Spatial dependency}: Recognising the hierarchical structure of
  data may help mitigate the effects of severe spatial autocorrelation.
\end{itemize}

Quickly, let us get a better idea about the data and look at the number
of OAs nested within LSOAs and MSOAs

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# mean of nested OAs within LSOAs and MSOAs}
\NormalTok{lsoa_cd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mean}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{round}\NormalTok{(, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msoa_cd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mean}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{round}\NormalTok{(, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# number of OAs nested within LSOAs and MSOAs}
\NormalTok{lsoa_cd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sort}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msoa_cd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sort}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-4-2.pdf}

\section{Modelling}\label{modelling}

We should now be persuaded that ignoring the hierarchical structure of
data may be a major issue. Let us now use a simple example to understand
the intuition of multilevel model using the census data. We will seek to
understand the spatial distribution of the proportion of population in
unemployment in Liverpool, particularly why and where concentrations in
this proportion occur. To illustrate the advantages of taking a
multilevel modelling approach, we will start by estimating a linear
regression model and progressively building complexity. We will first
estimate a model and then explain the intuition underpinning the
process. We will seek to gain a general understanding of multilevel
modelling. If you are interested in the statistical and mathemathical
formalisation of the underpinning concepts, please refer to
\citet{Gelman_Hill_2006_book}.

We first need to want to understand our dependent variable: its density
ditribution;

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ oa_shp) }\OperatorTok{+}
\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.8}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"lightblue"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ unemp)) }\OperatorTok{+}
\StringTok{   }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(unemp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.00000 0.05797 0.10256 0.11581 0.16129 0.50000
\end{verbatim}

and, its spatial distribution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{oa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(oa_shp)}

\CommentTok{# create a map}
\NormalTok{legend_title =}\StringTok{ }\KeywordTok{expression}\NormalTok{(}\StringTok{"% unemployment"}\NormalTok{)}
\NormalTok{map_oa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(oa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"unemp"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{, }\DataTypeTok{begin =} \FloatTok{0.25}\NormalTok{, }\DataTypeTok{end =} \DecValTok{1}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }
\NormalTok{map_oa}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-7-1.pdf}

Let us look at those areas:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# high %s}
\NormalTok{oa_shp }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(unemp }\OperatorTok{>}\StringTok{ }\FloatTok{0.2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(oa_cd, pop, unemp) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 203 features and 3 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 333993.8 ymin: 379748.5 xmax: 345600.2 ymax: 397681.5
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
## First 10 features:
##        oa_cd pop     unemp                       geometry
## 1  E00032992 240 0.2519685 MULTIPOLYGON (((335804.9 38...
## 2  E00033008 345 0.2636364 MULTIPOLYGON (((335080 3885...
## 3  E00033074 299 0.2075472 MULTIPOLYGON (((336947.3 38...
## 4  E00033075 254 0.2288136 MULTIPOLYGON (((336753.6 38...
## 5  E00033080 197 0.2647059 MULTIPOLYGON (((338196 3870...
## 6  E00033103 298 0.2148148 MULTIPOLYGON (((340484 3854...
## 7  E00033116 190 0.2156863 MULTIPOLYGON (((341960.7 38...
## 8  E00033134 190 0.2674419 MULTIPOLYGON (((337137 3930...
## 9  E00033137 289 0.2661290 MULTIPOLYGON (((337363.8 39...
## 10 E00033138 171 0.3561644 MULTIPOLYGON (((337481.5 39...
\end{verbatim}

\subsection{Baseline Linear Regression
Model}\label{baseline-linear-regression-model}

Now let us estimate a simple linear regression model with the intercept
only:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# specify a model equation}
\NormalTok{eq1 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\DecValTok{1}
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ eq1, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = eq1, data = oa_shp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.11581 -0.05784 -0.01325  0.04548  0.38419 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 0.115812   0.001836   63.09   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.07306 on 1583 degrees of freedom
\end{verbatim}

To understand the differences between the linear regression model and
multilevel models, let us consider the model we have estimated:

\[y_{i} = \beta_{0} + e_{i}\] where \(y_{i}\) represents the proportion
of the unemployed resident population in the OA \(i\); \(\beta_{0}\) is
the regression intercept and measures the average proportion of the
unemployed resident population across OAs; and, \(e_{i}\) is the error
term. But how do we deal with the hierarchical structure of the data?

\subsubsection{Limitations}\label{limitations}

Before looking at the answer, let's first understand some of the key
limitations of the linear regression model to handle the hierarchical
structure of data. A key limitation of the linear regression model is
that it only captures average relationships in the data. It does not
capture variations in the relationship between variables across areas or
groups. Another key limitation is that the linear regression model can
capture associations at either macro or micro levels, but it does not
simultaneously measure their interdependencies.

To illustrate this, let us consider the regression intercept. It
indicates that the average percentage of unemployed population at the OA
level is 0.12 but this model ignores any spatial clustering ie. the
percentage of unemployed population tends to be similar across OAs
nested within a same LSOA or MSOA. A side effect of ignoring this is
that our standard errors are biased, and thus claims about statistical
significance based on them would be misleading. Additionally, this
situation also means we cannot explore variations in the percentage of
unemployed population across LSOAs or MSOAs ie. how the percentage of
unemployed population may be dependent on various contextual factors at
these geographical scales.

\subsubsection{Fixed Effect Approach}\label{fixed-effect-approach}

An alternative approach is to adopt a fixed effects approach, or
no-pooling model; that is, adding dummy variables indicating the group
classification into the regression model eg. the way OAs is nested
within LSOAs (or MSOAs). This approach has limitations. First, there is
high risk of overfitting. The number of groups may be too large,
relative to the number of observations. Second, the estimation of
multiple parameters may be required so that measuring differences
between groups may be challenging. Third, a fixed effects approach does
not allow including group-level explanatory variables. You can try
fitting a linear regression model extending our estimated model to
include dummy variables for individual LSOAs (and/or MSOAs) so you can
compare this to the multilevel model below.

An alternative is fitting separate linear regression models for each
group. This approach is not always possible if there are groups with
small sizes.

\section{Multilevel Modelling: Random Intercept
Model}\label{multilevel-modelling-random-intercept-model}

We use multilevel modelling to account for the hierarchical nature of
the data by explicitly recognising that OAs are nested within LSOAs and
MSOAs. Multilevel models can easily be estimated using in R using the
package \texttt{lme4}. We implement an two-level model to allow for
variation across LSOAs. We estimate an only intercept model allowing for
variation across LSOAs. In essence, we are estimating a model with
varying intercept coefficient by LSOA. As you can see in the code chunk
below, the equation has an additional component. This is the group
component or LSOA effect. The \texttt{(1\ \textbar{}\ lsoa\_cd)} means
that we are allowing the intercept, represented by 1, to vary by LSOA.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# specify a model equation}
\NormalTok{eq2 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{lsoa_cd)}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq2, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ 1 + (1 | lsoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4382.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.8741 -0.5531 -0.1215  0.4055  5.8207 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  lsoa_cd  (Intercept) 0.002701 0.05197 
##  Residual             0.002575 0.05074 
## Number of obs: 1584, groups:  lsoa_cd, 298
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.114316   0.003277   34.89
\end{verbatim}

We can estimate a three-level model by adding
\texttt{(1\ \textbar{}\ msoa\_cd)} to allow the intercept to also vary
by MSOAs and account for the nesting structure of LSOAs within MSOAs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# specify a model equation}
\NormalTok{eq3 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{lsoa_cd) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{msoa_cd)}
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq3, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ 1 + (1 | lsoa_cd) + (1 | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4529.3
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.5624 -0.5728 -0.1029  0.4228  6.1363 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  lsoa_cd  (Intercept) 0.0007603 0.02757 
##  msoa_cd  (Intercept) 0.0020735 0.04554 
##  Residual             0.0025723 0.05072 
## Number of obs: 1584, groups:  lsoa_cd, 298; msoa_cd, 61
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.115288   0.006187   18.64
\end{verbatim}

We see two sets of coefficients: \emph{fixed effects} and \emph{random
effects}. \emph{Fixed effects} correspond to the standard linear
regression coefficients. Their interpretation is as usual. \emph{Random
effects} are the novelty. It is a term in multilevel modelling and
refers to varying coefficients i.e.~the randomness in the probability of
the model for the group-level coefficients. Specifically they relate to
estimates of the average variance and standard deviation within groups
(i.e.~LSOAs or MSOAs). Intiutively, variance and standard deviation
indicate the extent to which the intercept, on average, varies by LSOAs
and MSOAs.

\begin{figure}
\centering
\includegraphics{figs/ch5/nm_dist_obs.png}
\caption{Fig. 2. Variation of observations around their level 1 group
mean.}
\end{figure}

\begin{figure}
\centering
\includegraphics{figs/ch5/nm_dist_msoa.png}
\caption{Fig. 3. Variation of level 1 group mean around their level 2
group mean.}
\end{figure}

\begin{figure}
\centering
\includegraphics{figs/ch5/nm_dist_grand.png}
\caption{Fig. 4. Grand mean.}
\end{figure}

More formally, we first estimated the simplest regression model which is
an intercept-only model and equivalent to the sample mean (i.e.~the
\emph{fixed} part of the model):

\[y_{ijk} = \mu + e_{ijk}\] and then we made the \emph{random} part of
the model (\(e_{ijk}\)) more complex to account for the hierarchical
structure of the data by estimating the following three-level regression
model:

\[y_{ijk} = \mu + u_{i..} + u_{ij.} + e_{ijk}\]

where \(y_{ijk}\) represents the proportion of unemployed population in
OA \(i\) nested within LSOA \(j\) and MSOA \(k\); \(\mu\) represents the
sample mean and the \emph{fixed} part of the model; \(e_{ijk}\) is the
deviation of an observation from its LSOA mean; \(u_{ij.}\) is the
deviation of the LSOA mean from its MSOA mean; \(u_{i..}\) is the
deviation of the MSOA mean from the fixed part of the model \(\mu\).
Conceptually, this model is decomposing the variance of the model in
terms of the hierarchical structure of the data. It is partitioning the
observation's residual into three parts or \emph{variance components}.
These components measure the relative extent of variation of each
hierarchical level ie. LSOA, MSOA and grand means. To estimate the set
of residuals, they are assumed to follow a normal distribution and are
obtained after fitting the model and are based on the estimates of the
model parameters (i.e.~intercept and variances of the random
parameters).

Let's now return to our three-level model (reported again below), we see
that the intercept or fixed part of the model is the same as for the
linear regression. The multilevel model reports greater standard errors.
Multilevel models capture the hierarchical structure of the data and
thus more precisely estimate the standard errors for our parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# report model 3}
\KeywordTok{summary}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ 1 + (1 | lsoa_cd) + (1 | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4529.3
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.5624 -0.5728 -0.1029  0.4228  6.1363 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  lsoa_cd  (Intercept) 0.0007603 0.02757 
##  msoa_cd  (Intercept) 0.0020735 0.04554 
##  Residual             0.0025723 0.05072 
## Number of obs: 1584, groups:  lsoa_cd, 298; msoa_cd, 61
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.115288   0.006187   18.64
\end{verbatim}

\subsection{Interpretation}\label{interpretation}

\begin{quote}
Fixed effects
\end{quote}

We start by examining the fixed effects or estimated model averaging
over LSOAs and MSOAs, \(y_{ijk} = 0.115288\) which can also be called by
executing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fixef}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept) 
##   0.1152881
\end{verbatim}

Th estimated intercept indicates that the overall mean taken across
LSOAs and MSOAs is estimated as \texttt{0.115288} and is statistically
significant at \texttt{5\%} significance.

\begin{quote}
Random effects
\end{quote}

The set of random effects contains three estimates of variance and
standard deviation and refer to the variance components discussed above.
The \texttt{lsoa\_cd}, \texttt{msoa\_cd} and \texttt{Residual} estimates
indicate that the extent of estimated LSOA-, MSOA- and individual-level
variance is \texttt{0.0007603}, \texttt{0.0020735} and
\texttt{0.0025723}, respectively.

\subsection{Variance Partition Coefficient
(VPC)}\label{variance-partition-coefficient-vpc}

The purpose of multilevel models is to partition variance in the outcome
between the different groupings in the data. We thus often want to know
the percentage of variation in the dependent variable accounted by
differences across groups i.e.~what proportion of the total variance is
attributable to variation within-groups, or how much is found
between-groups. The statistic to obtain this is termed the variance
partition coefficient (VPC), or intraclass correlation.\footnote{The VPC
  is equal to the intra-class correlation coefficient which is the
  correlation between the observations of the dependent variable
  selected randomly from the same group. For instance, if the VPC is
  0.1, we would say that 10\% of the variation is between groups and
  90\% within. The correlation between randomly chosen pairs of
  observations belonging to the same group is 0.1.} For our case, the
VPC at the LSOA level indicates that 14\% of the variation in percentage
of unemployed resident population across OAs can be explained by
differences across LSOAs. What is the VPC at the MSOA level?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vpc_lsoa <-}\StringTok{ }\FloatTok{0.0007603} \OperatorTok{/}\StringTok{ }\NormalTok{(}\FloatTok{0.0007603} \OperatorTok{+}\StringTok{ }\FloatTok{0.0020735} \OperatorTok{+}\StringTok{ }\FloatTok{0.0025723}\NormalTok{)}
\NormalTok{vpc_lsoa }\OperatorTok{*}\StringTok{ }\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14.06374
\end{verbatim}

You can also obtain the VPC by executing:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#summ(model3)}
\end{Highlighting}
\end{Shaded}

\subsection{Uncertainty of Estimates}\label{uncertainty-of-estimates}

You may have noticed that \texttt{lme4} does not provide p-values,
because of
\href{https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html}{various
reasons} as explained by Doug Bates, one of the author of \texttt{lme4}.
These explanations mainly refer to the complexity of dealing with
varying sample sizes at a given hierarchical level. The number of
observations at each hierarchical level varies across individual
groupings (i.e.~LSOA or MSOA). It may even be one single observation.
This has implications for the distributional assumptions, denominator
degrees of freedom and how to approximate a ``best'' solution. Various
approaches exist to compute the statistical significance of estimates.
We use the \texttt{confint} function available within \texttt{lme4} to
obtain confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model3, }\DataTypeTok{level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
##                  2.5 %     97.5 %
## .sig01      0.02360251 0.03189046
## .sig02      0.03707707 0.05562307
## .sigma      0.04882281 0.05273830
## (Intercept) 0.10307341 0.12751103
\end{verbatim}

\texttt{.sig01} refers to the LSOA level; \texttt{.sig02} refers to the
MSOA level; and, \texttt{.sigma} refers to the OA level.

\subsection{Assessing Group-level
Variation}\label{assessing-group-level-variation}

\emph{Estimated regression coefficients}

In multilevel modelling, our primary interest is in knowing differences
across groups. To visualise the estimated model within each group (ie.
LSOA and MSOA), we type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef_m3 <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(model3)}
\KeywordTok{head}\NormalTok{(coef_m3}\OperatorTok{$}\NormalTok{lsoa_cd,}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           (Intercept)
## E01006512  0.09915456
## E01006513  0.09889615
## E01006514  0.09297051
## E01006515  0.09803754
## E01006518  0.09642939
\end{verbatim}

The results indicate that the estimated regression line is
\(y = 0.09915456\) for LSOA \texttt{E01006512}; \(y = 0.09889615\) for
LSOA \texttt{E01006513} and so forth. Try getting the estimated model
within each MSOA.

\emph{Random effects}

We can look at the estimated group-level (or LSOA-level and MSOA-level)
errors; that is, \emph{random effects}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ranef_m3 <-}\StringTok{ }\KeywordTok{ranef}\NormalTok{(model3)}
\KeywordTok{head}\NormalTok{(ranef_m3}\OperatorTok{$}\NormalTok{lsoa_cd, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           (Intercept)
## E01006512 -0.01613353
## E01006513 -0.01639194
## E01006514 -0.02231758
## E01006515 -0.01725055
## E01006518 -0.01885870
\end{verbatim}

Group-level errors indicate how much the intercept is shifted up or down
in particular groups (ie. LSOAs or MSOAs). Thus, for example, in LSOA
\texttt{E01006512}, the estimated intercept is \texttt{-0.01613353}
lower than average, so that the regression line is
\texttt{(0.1152881\ -\ 0.01613353)} \texttt{=\ 0.09915457} which is what
we observed from the call to \texttt{coef()}.

We can also obtain group-level errors (\emph{random effects}) by using a
simulation approach, labelled ``Empirical Bayes'' and discussed
\href{https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q4/002984.html}{here}.
To this end, we run:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# obtain estimates}
\KeywordTok{REsim}\NormalTok{(model3) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    groupFctr   groupID        term         mean       median          sd
## 1    lsoa_cd E01006512 (Intercept) -0.017318442 -0.016110349 0.019686077
## 2    lsoa_cd E01006513 (Intercept) -0.014395021 -0.014910830 0.019900639
## 3    lsoa_cd E01006514 (Intercept) -0.022270247 -0.022332328 0.018868540
## 4    lsoa_cd E01006515 (Intercept) -0.017522889 -0.016520148 0.018395855
## 5    lsoa_cd E01006518 (Intercept) -0.017649891 -0.016861768 0.020620397
## 6    lsoa_cd E01006519 (Intercept) -0.015857953 -0.015276387 0.009668197
## 7    lsoa_cd E01006520 (Intercept) -0.023907575 -0.023590443 0.019805141
## 8    lsoa_cd E01006521 (Intercept)  0.007391788  0.007076058 0.019114378
## 9    lsoa_cd E01006522 (Intercept)  0.016853450  0.015098119 0.019858287
## 10   lsoa_cd E01006523 (Intercept)  0.004200268  0.004154191 0.019658887
\end{verbatim}

The results contain the estimated mean, median and standard deviation
for the intercept within each group (e.g.~LSOA). The mean estimates are
similar to those obtained from \texttt{ranef} with some small
differences due to rounding.

To gain an undertanding of the general pattern of the \emph{random
effects}, we can use caterpillar plots via \texttt{plotREsim} - reported
below. The plot on the right shows the estimated random effects for each
MSOA and their respective interval estimate. Note that random effects
are on average zero, represented by the red horizontal line. Intervals
that do not include zero are in bold. Also note that the width of the
confidence interval depends on the standard error of the respective
residual estimate, which is inversely related to the size of the sample.
The residuals represent an observation departures from the grand mean,
so an observation whose confidence interval does not overlap the line at
zero (representing the mean proportion of unemployed population across
all areas) is said to differ significantly from the average at the 5\%
level.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot}
\KeywordTok{plotREsim}\NormalTok{(}\KeywordTok{REsim}\NormalTok{(model3)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-20-1.pdf}

Focusing on the plot on the right, we see MSOAs whose mean proportion of
unemployed population, assuming no explanatory variables, is lower than
average. On the right-hand side of the plot, you will see MSOAs whose
mean proportion is higher than average. The MSOAs with the smallest
residuals include the districts of Allerton and Hunt Cross, Church,
Childwall, Wavertree and Woolton. What districts do we have at the other
extreme?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{re <-}\StringTok{ }\KeywordTok{REsim}\NormalTok{(model3)}
\NormalTok{oa_shp }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(msoa_cd, ward_nm, unemp) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(msoa_cd) }\OperatorTok{==}\StringTok{ "E02001387"} \OperatorTok{|}\StringTok{ }\KeywordTok{as.character}\NormalTok{(msoa_cd) }\OperatorTok{==}\StringTok{ "E02001393"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 49 features and 3 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 339178.6 ymin: 386244.2 xmax: 341959.9 ymax: 389646.7
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
## First 10 features:
##      msoa_cd                  ward_nm      unemp                       geometry
## 1  E02001393 Allerton and Hunts Cross 0.03246753 MULTIPOLYGON (((341333.6 38...
## 2  E02001393 Allerton and Hunts Cross 0.03684211 MULTIPOLYGON (((340658.2 38...
## 3  E02001393                   Church 0.04098361 MULTIPOLYGON (((339908.1 38...
## 4  E02001393 Allerton and Hunts Cross 0.05982906 MULTIPOLYGON (((340306 3865...
## 5  E02001393                   Church 0.01212121 MULTIPOLYGON (((339974.2 38...
## 6  E02001393                   Church 0.09219858 MULTIPOLYGON (((340181.4 38...
## 7  E02001393                   Church 0.01986755 MULTIPOLYGON (((340301.2 38...
## 8  E02001393                   Church 0.04615385 MULTIPOLYGON (((340375.9 38...
## 9  E02001393 Allerton and Hunts Cross 0.04117647 MULTIPOLYGON (((340435.3 38...
## 10 E02001393 Allerton and Hunts Cross 0.02272727 MULTIPOLYGON (((340681.7 38...
\end{verbatim}

We can also map the MSOA-level \emph{random effects}. To this end, we
first need to read a shapefile containing data at the MSOA level and
merge it with the \emph{random effects} estimates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/mlm/MSOA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `MSOA' from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/mlm/MSOA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 61 features and 17 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1
## epsg (SRID):    27700
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a dataframe for MSOA-level random effects}
\NormalTok{re_msoa <-}\StringTok{ }\NormalTok{re }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(groupFctr }\OperatorTok{==}\StringTok{ "msoa_cd"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(re_msoa)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    61 obs. of  6 variables:
##  $ groupFctr: chr  "msoa_cd" "msoa_cd" "msoa_cd" "msoa_cd" ...
##  $ groupID  : chr  "E02001347" "E02001348" "E02001349" "E02001350" ...
##  $ term     : chr  "(Intercept)" "(Intercept)" "(Intercept)" "(Intercept)" ...
##  $ mean     : num  -0.01355 -0.02823 -0.03004 0.00513 0.02317 ...
##  $ median   : num  -0.0141 -0.0265 -0.0305 0.0054 0.0232 ...
##  $ sd       : num  0.033 0.0328 0.0316 0.0311 0.0163 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# merge data}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ msoa_shp, }\DataTypeTok{y =}\NormalTok{ re_msoa, }\DataTypeTok{by.x =} \StringTok{"MSOA_CD"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{"groupID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can create our map:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{msoa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(msoa_shp)}

\CommentTok{# create a map}
\NormalTok{legend_title =}\StringTok{ }\KeywordTok{expression}\NormalTok{(}\StringTok{"MSOA-level residuals"}\NormalTok{)}
\NormalTok{map_msoa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(msoa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"mean"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{, }\DataTypeTok{begin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{end =} \DecValTok{1}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }
\NormalTok{map_msoa}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-multilevel_01_files/figure-latex/unnamed-chunk-23-1.pdf}

\hypertarget{adding-individual-level-predictors}{\subsection{Adding
Individual-level Predictors}\label{adding-individual-level-predictors}}

In this example, \(\mu\) represents the sample mean but it could include
a collection of independent variables or predictors. To explain the
logic, we will assume that unemployment is strongly associated to
long-term illness. We could expect that long-term illness
(\texttt{lt\_ill}) will reduce the chances of working and therefore
being unemployed. Note that our focus is on the relationship, not on
establishing causation. Specifically we want to estimate the
relationship between unemployment and long-term illness and we are
interested in variations in OA-level unemployment by MSOAs so we will
estimate the following two-level model:

OA-level:

\[y_{ij} = \beta_{0j} + \beta_{1}x_{ij} + e_{ij}\] MSOA-level:

\[\beta_{0j} = \beta_{0} + u_{0j}\] Replacing the first equation into
the second, we have:

\[y_{ij} = (\beta_{0} + u_{0j}) + \beta_{1}x_{ij} + e_{ij}\] where \(y\)
the proportion of unemployed population in OA \(i\) within MSOA \(j\);
\(\beta_{0}\) is the fixed intercept (averaging over all MSOAs);
\(u_{0j}\) represents the MSOA-level residuals or \emph{random effects};
\(\beta_{0}\) and \(u_{0j}\) together represent the varying-intercept;
\(\beta_{1}\) is the slope coefficient; \(x_{ij}\) represents the
percentage of long-term illness population; and, \(e_{ij}\) is the
individual-level residuals.

We estimate the model executing:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change to proportion}
\NormalTok{oa_shp}\OperatorTok{$}\NormalTok{lt_ill <-}\StringTok{ }\NormalTok{lt_ill}\OperatorTok{/}\DecValTok{100}

\CommentTok{# specify a model equation}
\NormalTok{eq4 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\NormalTok{lt_ill }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{msoa_cd)}
\NormalTok{model4 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq4, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ lt_ill + (1 | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4711.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.1941 -0.5718 -0.0906  0.4507  5.9393 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  msoa_cd  (Intercept) 0.001421 0.03769 
##  Residual             0.002674 0.05171 
## Number of obs: 1584, groups:  msoa_cd, 61
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  0.04682    0.00625   7.492
## lt_ill       0.29588    0.01615  18.317
## 
## Correlation of Fixed Effects:
##        (Intr)
## lt_ill -0.600
\end{verbatim}

\emph{Fixed effects}: model averaging over MSOAs

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fixef}\NormalTok{(model4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)      lt_ill 
##  0.04681959  0.29588110
\end{verbatim}

yields an estimated regression line in an average McSOA:
\(y = 0.04681959 + 0.29588110x\)

\emph{Random effects}: MSOA-level errors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ranef_m4 <-}\StringTok{ }\KeywordTok{ranef}\NormalTok{(model4)}
\KeywordTok{head}\NormalTok{(ranef_m4}\OperatorTok{$}\NormalTok{msoa_cd, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            (Intercept)
## E02001347 -0.017474815
## E02001348 -0.021203807
## E02001349 -0.022469313
## E02001350 -0.003539869
## E02001351  0.008502813
\end{verbatim}

yields an estimated intercept for MSOA \texttt{E02001347} which is
\texttt{0.017474815} lower than the average with a regression line:
\texttt{(0.04681959\ -\ 0.017474815)\ +\ 0.29588110x} \texttt{=}
\texttt{0.02934478\ +\ 0.29588110x}. You can confirm this by looking at
the estimated model within each MSOA by executing (remove the
\texttt{\#} sign):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#coef(model4)}
\end{Highlighting}
\end{Shaded}

\emph{Fixed effect correlations}

In the bottom of the output, we have the correlations between the
fixed-effects estimates. In our example, it refers to the correlation
between \(\beta_{0}\) and \(\beta_{1}\). It is negative indicating that
in MSOAs where the relationship between unemployment and long-term
illness is greater, as measured by \(\beta_{1}\), the average proportion
of unemployed people tends to be smaller, as captured by \(\beta_{0}\).

\hypertarget{adding-group-level-predictors}{\subsection{Adding
Group-level Predictors}\label{adding-group-level-predictors}}

We can also add group-level predictors. We use the formulation:

OA-level:

\[y_{ij} = \beta_{0j} + \beta_{1}x_{ij} + e_{ij}\]

MSOA-level:

\[\beta_{0j} = \beta_{0} + \gamma_{1}m_{j} + u_{0j}\]

where \(x_{ij}\) is the OA-level proportion of population suffering
long-term illness and \(m_{j}\) is the MSOA-level proportion of male
population. We first need to create this group-level predictor:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# detach OA shp and attach MSOA shp}
\KeywordTok{detach}\NormalTok{(oa_shp)}
\KeywordTok{attach}\NormalTok{(msoa_shp)}

\CommentTok{# group-level predictor}
\NormalTok{msoa_shp}\OperatorTok{$}\NormalTok{pr_male <-}\StringTok{ }\NormalTok{males}\OperatorTok{/}\NormalTok{pop}

\CommentTok{# remove geometries}
\NormalTok{msoa_df <-}\StringTok{ `}\DataTypeTok{st_geometry<-}\StringTok{`}\NormalTok{(msoa_shp, }\OtherTok{NULL}\NormalTok{)}

\CommentTok{# select variables}
\NormalTok{msoa_df <-}\StringTok{ }\NormalTok{msoa_df }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(MSOA_CD, pop, pr_male)}

\CommentTok{# merge data sets}
\NormalTok{oa_shp <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(}\DataTypeTok{x=}\NormalTok{oa_shp, }\DataTypeTok{y=}\NormalTok{msoa_df, }\DataTypeTok{by.x =} \StringTok{"msoa_cd"}\NormalTok{, }\DataTypeTok{by.y=}\StringTok{"MSOA_CD"}\NormalTok{)}

\CommentTok{# inspect data}
\KeywordTok{head}\NormalTok{(oa_shp[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"msoa_cd"}\NormalTok{, }\StringTok{"oa_cd"}\NormalTok{, }\StringTok{"unemp"}\NormalTok{, }\StringTok{"pr_male"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 6 features and 4 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 337693.5 ymin: 396068.2 xmax: 339430.9 ymax: 397790
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
##     msoa_cd     oa_cd      unemp   pr_male                       geometry
## 1 E02001347 E00033730 0.10322581 0.4775905 MULTIPOLYGON (((338376 3970...
## 2 E02001347 E00033722 0.06306306 0.4775905 MULTIPOLYGON (((337929.4 39...
## 3 E02001347 E00033712 0.09090909 0.4775905 MULTIPOLYGON (((338830 3960...
## 4 E02001347 E00033739 0.09401709 0.4775905 MULTIPOLYGON (((339140.3 39...
## 5 E02001347 E00033719 0.05855856 0.4775905 MULTIPOLYGON (((338128.8 39...
## 6 E02001347 E00033711 0.12195122 0.4775905 MULTIPOLYGON (((339163.2 39...
\end{verbatim}

We can now estimate our model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{detach}\NormalTok{(msoa_shp)}
\KeywordTok{attach}\NormalTok{(oa_shp)}

\CommentTok{# specify a model equation}
\NormalTok{eq5 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\NormalTok{lt_ill }\OperatorTok{+}\StringTok{ }\NormalTok{pr_male }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{msoa_cd)}
\NormalTok{model5 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq5, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ lt_ill + pr_male + (1 | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4712.3
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.2162 -0.5696 -0.0929  0.4549  5.9370 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  msoa_cd  (Intercept) 0.001391 0.03729 
##  Residual             0.002674 0.05171 
## Number of obs: 1584, groups:  msoa_cd, 61
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) -0.07746    0.08768  -0.883
## lt_ill       0.29781    0.01620  18.389
## pr_male      0.25059    0.17642   1.420
## 
## Correlation of Fixed Effects:
##         (Intr) lt_ill
## lt_ill  -0.118       
## pr_male -0.997  0.075
\end{verbatim}

This model includes the proportion of males and intercepts that vary by
MSOA. The \texttt{lmer()} function only accepts predictors at the
individual level, so we have included data on the proportion of male
population at this level. Explore and interpret the model running the
functions below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fixed effects}
\KeywordTok{fixef}\NormalTok{(model5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)      lt_ill     pr_male 
## -0.07746069  0.29780843  0.25059133
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random effects}
\NormalTok{ranef_m5 <-}\StringTok{ }\KeywordTok{ranef}\NormalTok{(model5)}
\KeywordTok{head}\NormalTok{(ranef_m5}\OperatorTok{$}\NormalTok{msoa_cd, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            (Intercept)
## E02001347 -0.013625262
## E02001348 -0.019757849
## E02001349 -0.023709996
## E02001350  0.003003862
## E02001351  0.003508479
\end{verbatim}

Adding group-level predictors tends to improve inferences for group
coefficients. Examine the confidence intervals, in order to evalute how
the precision of our estimates of the MSOA intercepts have changed.
\emph{Have confidence intervals for the intercepts of Model 4 and 5
increased or reduced?} Hint: look at how to get the confidence intervals
above.

\section{Useful Functions}\label{useful-functions-1}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.14\columnwidth}\raggedright\strut
Function\strut
\end{minipage} & \begin{minipage}[b]{0.60\columnwidth}\raggedright\strut
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
lmer()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
fit linear mixed-effects models\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
fixef()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
obtain estimated fixed effects or model averaging over groups\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
ranef()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
obtain estimated random effects or group-level residuals\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
REsim()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
obtain estimated random effects or group-level residuals based on
simulation\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
plotREsim()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
create a caterpillar plot of estimated random effects\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
coef()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
obtain coefficients within each group\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
anova()\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright\strut
provide regression model diagnostics\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\chapter{Multilevel Modelling - Part
2}\label{multilevel-modelling---part-2}

This chapter\footnote{This note is part of \href{index.html}{Spatial
  Analysis Notes} {Multilevel Modelling -- Random Slope Model} by
  Francisco Rowe is licensed under a Creative Commons
  Attribution-NonCommercial-ShareAlike 4.0 International License.}
provides an introduction to multi-level data structures and multi-level
modelling.

The content of this chapter is based on:

\begin{itemize}
\item
  \citet{Gelman_Hill_2006_book} provides an excellent and intuitive
  explanation of multilevel modelling and data analysis in general. Read
  Part 2A for a really good explanation of multilevel models.
\item
  \citet{bristol2020} is an useful online resource on multilevel
  modelling and is free!
\end{itemize}

This Chapter is part of \href{index.html}{Spatial Analysis Notes}, a
compilation hosted as a GitHub repository that you can access it in a
few ways:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/GDSL-UL/san/archive/master.zip}{download} of
  a \texttt{.zip} file that contains all the materials.
\item
  As an
  \href{https://gdsl-ul.github.io/san/multilevel-modelling-part-2.html}{html
  website}.
\item
  As a
  \href{https://gdsl-ul.github.io/san/spatial_analysis_notes.pdf}{pdf
  document}
\item
  As a \href{https://github.com/GDSL-UL/san}{GitHub repository}.
\end{itemize}

\section{Dependencies}\label{dependencies-5}

This chapter uses the following libraries: Ensure they are installed on
your machine\footnote{You can install package \texttt{mypackage} by
  running the command \texttt{install.packages("mypackage")} on the R
  prompt or through the
  \texttt{Tools\ -\/-\textgreater{}\ Install\ Packages...} menu in
  RStudio.} before loading them executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Data manipulation, transformation and visualisation}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{# Nice tables}
\KeywordTok{library}\NormalTok{(kableExtra)}
\CommentTok{# Simple features (a standardised way to encode vector data ie. points, lines, polygons)}
\KeywordTok{library}\NormalTok{(sf) }
\CommentTok{# Spatial objects conversion}
\KeywordTok{library}\NormalTok{(sp) }
\CommentTok{# Thematic maps}
\KeywordTok{library}\NormalTok{(tmap) }
\CommentTok{# Colour palettes}
\KeywordTok{library}\NormalTok{(RColorBrewer) }
\CommentTok{# More colour palettes}
\KeywordTok{library}\NormalTok{(viridis) }\CommentTok{# nice colour schemes}
\CommentTok{# Fitting multilevel models}
\KeywordTok{library}\NormalTok{(lme4)}
\CommentTok{# Tools for extracting information generated by lme4}
\KeywordTok{library}\NormalTok{(merTools)}
\CommentTok{# Exportable regression tables}
\KeywordTok{library}\NormalTok{(jtools)}
\KeywordTok{library}\NormalTok{(stargazer)}
\KeywordTok{library}\NormalTok{(sjPlot)}
\end{Highlighting}
\end{Shaded}

\section{Data}\label{data-4}

For this chapter, we will data for Liverpool from England's 2011 Census.
The original source is the
\href{https://www.nomisweb.co.uk/home/census2001.asp}{Office of National
Statistics} and the dataset comprises a number of selected variables
capturing demographic, health and socio-economic of the local resident
population at four geographic levels: Output Area (OA), Lower Super
Output Area (LSOA), Middle Super Output Area (MSOA) and Local Authority
District (LAD). The variables include population counts and percentages.
For a description of the variables, see the readme file in the mlm data
folder.\footnote{Read the file in R by executing
  \texttt{read\_tsv("data/mlm/readme.txt")}}

Let us read the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# clean workspace}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{# read data}
\NormalTok{oa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/mlm/oa.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Conceptual Overview}\label{conceptual-overview}

So far, we have estimated varying-intercept models; that is, when the
intercept (\(\beta_{0}\)) is allowed to vary by group (eg. geographical
area) - as shown in Fig. 1(a). The strength of the relationship between
\(y\) (i.e.~unemployment rate) and \(x\) (long-term illness) has been
assumed to be the same across groups (i.e.~MSOAs), as captured by the
regression slope (\(\beta_{1}\)). Yet it can also vary by group as shown
in Fig. 1(b), or we can observe group variability for both intercepts
and slopes as represented in Fig. 1(c).

\begin{figure}
\centering
\includegraphics{figs/ch6/fig11.1_Gelman_Hill.png}
\caption{Fig. 1. Linear regression model with (a) varying intercepts,
(b) varying slopes, and (c) both. Source: \citet{Gelman_Hill_2006_book}
p.238.}
\end{figure}

\subsection{Exploratory Analysis: Varying
Slopes}\label{exploratory-analysis-varying-slopes}

Let's then explore if there is variation in the relationship between
unemployment rate and the share of population in long-term illness. We
do this by selecting the 8 MSOAs containing OAs with the highest
unemployment rates in Liverpool.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Sort data }
\NormalTok{oa_shp <-}\StringTok{ }\NormalTok{oa_shp }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{unemp)}
\NormalTok{oa_shp[}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"msoa_cd"}\NormalTok{, }\StringTok{"unemp"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 9 features and 2 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 335032 ymin: 387777 xmax: 338576.1 ymax: 395022.4
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
##     msoa_cd     unemp                       geometry
## 1 E02001354 0.5000000 MULTIPOLYGON (((337491.2 39...
## 2 E02001369 0.4960630 MULTIPOLYGON (((335272.3 39...
## 3 E02001366 0.4461538 MULTIPOLYGON (((338198.1 39...
## 4 E02001365 0.4352941 MULTIPOLYGON (((336572.2 39...
## 5 E02001370 0.4024390 MULTIPOLYGON (((336328.3 39...
## 6 E02001390 0.3801653 MULTIPOLYGON (((335833.6 38...
## 7 E02001354 0.3750000 MULTIPOLYGON (((337403 3949...
## 8 E02001385 0.3707865 MULTIPOLYGON (((336251.6 38...
## 9 E02001368 0.3648649 MULTIPOLYGON (((335209.3 39...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Select MSOAs}
\NormalTok{s_t8 <-}\StringTok{ }\NormalTok{oa_shp }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{filter}\NormalTok{(}
    \KeywordTok{as.character}\NormalTok{(msoa_cd) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}
      \StringTok{"E02001354"}\NormalTok{, }
      \StringTok{"E02001369"}\NormalTok{, }
      \StringTok{"E02001366"}\NormalTok{, }
      \StringTok{"E02001365"}\NormalTok{, }
      \StringTok{"E02001370"}\NormalTok{, }
      \StringTok{"E02001390"}\NormalTok{, }
      \StringTok{"E02001368"}\NormalTok{, }
      \StringTok{"E02001385"}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

And then we generate a set of scatter plots and draw regression lines
for each MSOA.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(s_t8, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ lt_ill, }\DataTypeTok{y =}\NormalTok{ unemp)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{msoa_cd, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Unemployment rate"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Long-term Illness (%)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-multilevel_02_files/figure-latex/unnamed-chunk-4-1.pdf}

We can observe great variability in the relationship between
unemployment rates and the percentage of population in long-term
illness. A strong and positive relationship exists in MSOA
\texttt{E02001366} (Tuebrook and Stoneycroft), while it is negative in
MSOA \texttt{E02001370} (Everton) and neutral in MSOA \texttt{E02001390}
(Princes Park \& Riverside). This visual inspection suggests that
accounting for differences in the way unmployment rates relate to
long-term illness is important. Contextual factors may differ across
MSOAs in systematic ways.

\section{Estimating Varying Intercept and Slopes
Models}\label{estimating-varying-intercept-and-slopes-models}

A way to capture for these group differences in the relationship between
unemployment rates and long-term illness is to allow the relevant slope
to vary by group (i.e.~MSOA). We can do this estimating the following
model:

OA-level:

\[y_{ij} = \beta_{0j} + \beta_{1j}x_{ij} + e_{ij}\]

MSOA-level:

\[\beta_{0j} = \beta_{0} + u_{0j}\] \[\beta_{1j} = \beta_{1} + u_{1j} \]
Replacing the first equation into the second generates:

\[y_{ij} = (\beta_{0} + u_{0j}) + (\beta_{1} + u_{1j})x_{ij} + e_{ij}\]
where, as in the previous Chapter, \(y\) the proportion of unemployed
population in OA \(i\) within MSOA \(j\); \(\beta_{0}\) is the fixed
intercept (averaging over all MSOAs); \(u_{0j}\) represents the
MSOA-level residuals, or \emph{random effects}, of the intercept;
\(e_{ij}\) is the individual-level residuals; and, \(x_{ij}\) represents
the percentage of long-term illness population. \emph{But} now we have a
varying slope represented by \(\beta_{1}\) and \(u_{1j}\): \(\beta_{1}\)
is estimated average slope - fixed part of the model; and, \(u_{1j}\) is
the estimated group-level errors of the slope.

To estimate such model, we add \texttt{lt\_ill} in the bracket with a
\texttt{+} sign between \texttt{1} and \texttt{\textbar{}} i.e.
\texttt{(1\ +\ lt\_ill\ \textbar{}\ msoa\_cd)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# attach df}
\KeywordTok{attach}\NormalTok{(oa_shp)}

\CommentTok{# change to proportion}
\NormalTok{oa_shp}\OperatorTok{$}\NormalTok{lt_ill <-}\StringTok{ }\NormalTok{lt_ill}\OperatorTok{/}\DecValTok{100}

\CommentTok{# specify a model equation}
\NormalTok{eq6 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\NormalTok{lt_ill }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{lt_ill }\OperatorTok{|}\StringTok{ }\NormalTok{msoa_cd)}
\NormalTok{model6 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq6, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ lt_ill + (1 + lt_ill | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4762.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.6639 -0.5744 -0.0873  0.4565  5.4876 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  msoa_cd  (Intercept) 0.003428 0.05855       
##           lt_ill      0.029427 0.17154  -0.73
##  Residual             0.002474 0.04974       
## Number of obs: 1584, groups:  msoa_cd, 61
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.047650   0.008634   5.519
## lt_ill      0.301259   0.028163  10.697
## 
## Correlation of Fixed Effects:
##        (Intr)
## lt_ill -0.786
\end{verbatim}

In this model, the estimated standard deviation of the unexplained
within-MSOA variation is 0.04974, and the estimated standard deviation
of the MSOA intercepts is 0.05855. But, additionally, we also have
estimates of standard deviation of the MSOA slopes (0.17154) and
correlation between MSOA-level residuals for the intercept and slope
(-0.73). While the former measures the extent of average deviation in
the slopes across MSOAs, the latter indicates that the intercept and
slope MSOA-level residuals are negatively associated; that is, MSOAs
with large slopes have relatively smaller intercepts and \emph{vice
versa}. We will come back to this in Section
\protect\hyperlink{interpreting-correlations-between-group-level-intercepts-and-slopes}{Interpreting
Correlations Between Group-level Intercepts and Slopes}.

Similarly, the correlation of fixed effects indicates a negative
relationship between the intercept and slope of the average regression
model; that is, as the average model intercept tends to increase, the
average strength of the relationship between unemployment rate and
long-term illness decreases and \emph{vice versa}.

We then explore the estimated average coefficients (\emph{fixed
effects}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fixef}\NormalTok{(model6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)      lt_ill 
##  0.04764998  0.30125916
\end{verbatim}

yields an estimated regression line in an average LSOA:
\(y = 0.04764998 + 0.30125916x\). The fixed intercept indicates that the
average unemployment rate is 0.05 if the percentage of population with
long-term illness is zero.The fixed slope indicates that the average
relationship between unemployment rate and long-term illness is positive
across MSOAs i.e.~as the percentage of population with long-term illness
increases by 1 percentage point, the unemployment rate increases by 0.3.

We look the estimated MSOA-level errors (\emph{random effects}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ranef_m6 <-}\StringTok{ }\KeywordTok{ranef}\NormalTok{(model6)}
\KeywordTok{head}\NormalTok{(ranef_m6}\OperatorTok{$}\NormalTok{msoa_cd, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            (Intercept)      lt_ill
## E02001347 -0.026561023  0.02717992
## E02001348  0.001690577 -0.11534207
## E02001349 -0.036084591  0.05547012
## E02001350  0.032241334 -0.14298965
## E02001351  0.086214974 -0.28130545
\end{verbatim}

Recall these estimates indicate the extent of deviation of the
MSOA-specific intercept and slope from the estimated model average
captured by the fixed model component.

We can also regain the estimated intercept and slope for each county by
adding the estimated MSOA-level errors to the estimated average
coefficients; or by executing:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#coef(model6)}
\end{Highlighting}
\end{Shaded}

We are normally more interested in identifying the extent of deviation
and its significance. To this end, we create a caterpillar plot:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot}
\KeywordTok{plotREsim}\NormalTok{(}\KeywordTok{REsim}\NormalTok{(model6))}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-multilevel_02_files/figure-latex/unnamed-chunk-9-1.pdf}

These plots reveal some interesting patterns. First, only one MSOA,
containing wards such as Tuebrook and Stoneycroft, Anfield \& Everton,
seems to have a statistically significantly different intercept, or
average unemployment rate. Confidence intervals overlap zero for all
other 60 MSOAs. Despite this, note that when a slope is allowed to vary
by group, it generally makes sense for the intercept to also vary.
Second, significant variability exists in the association between
unemployment rate and long-term illness across MSOAs. Ten MSOAs display
a significant positive association, while 12 exhibit a significantly
negative relationship. Third, these results reveal that geographical
differences in the relationship between unemployment rate and long-term
illness can explain the significant differences in average unemployment
rates in the varying intercept only model.

Let's try to get a better understanding of the varying relationship
between unemployment rate and long-term illness by mapping the relevant
MSOA-level errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\StringTok{"data/mlm/MSOA.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `MSOA' from data source `/Users/Franciscorowe/Dropbox/Francisco/uol/teaching/envs453/201920/lectures/san/data/mlm/MSOA.shp' using driver `ESRI Shapefile'
## Simple feature collection with 61 features and 17 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 333086.1 ymin: 381426.3 xmax: 345636 ymax: 397980.1
## epsg (SRID):    27700
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a dataframe for MSOA-level random effects}
\NormalTok{re_msoa_m6 <-}\StringTok{ }\KeywordTok{REsim}\NormalTok{(model6) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(groupFctr }\OperatorTok{==}\StringTok{ "msoa_cd"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{==}\StringTok{ "lt_ill"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(re_msoa_m6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    61 obs. of  6 variables:
##  $ groupFctr: chr  "msoa_cd" "msoa_cd" "msoa_cd" "msoa_cd" ...
##  $ groupID  : chr  "E02001347" "E02001348" "E02001349" "E02001350" ...
##  $ term     : chr  "lt_ill" "lt_ill" "lt_ill" "lt_ill" ...
##  $ mean     : num  0.0264 -0.1257 0.0535 -0.1452 -0.2813 ...
##  $ median   : num  0.0237 -0.1189 0.0534 -0.1454 -0.2795 ...
##  $ sd       : num  0.0446 0.0696 0.0907 0.0351 0.0392 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# merge data}
\NormalTok{msoa_shp <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ msoa_shp, }\DataTypeTok{y =}\NormalTok{ re_msoa_m6, }\DataTypeTok{by.x =} \StringTok{"MSOA_CD"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{"groupID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure geometry is valid}
\NormalTok{msoa_shp =}\StringTok{ }\NormalTok{lwgeom}\OperatorTok{::}\KeywordTok{st_make_valid}\NormalTok{(msoa_shp)}

\CommentTok{# create a map}
\NormalTok{legend_title =}\StringTok{ }\KeywordTok{expression}\NormalTok{(}\StringTok{"MSOA-level residuals"}\NormalTok{)}
\NormalTok{map_msoa =}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(msoa_shp) }\OperatorTok{+}
\StringTok{  }\KeywordTok{tm_fill}\NormalTok{(}\DataTypeTok{col =} \StringTok{"median"}\NormalTok{, }\DataTypeTok{title =}\NormalTok{ legend_title, }\DataTypeTok{palette =} \KeywordTok{magma}\NormalTok{(}\DecValTok{256}\NormalTok{, }\DataTypeTok{begin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{end =} \DecValTok{1}\NormalTok{), }\DataTypeTok{style =} \StringTok{"cont"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{col =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_compass}\NormalTok{(}\DataTypeTok{type =} \StringTok{"arrow"}\NormalTok{, }\DataTypeTok{position =} \KeywordTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\DataTypeTok{size =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{tm_scale_bar}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{text.size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =}  \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }
\NormalTok{map_msoa}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-multilevel_02_files/figure-latex/unnamed-chunk-11-1.pdf}

The map indicates that the relationship between unemployment rate and
long-term illness is tends to stronger and positive in northern MSOAs;
that is, the percentage of population with long-term illness explains a
greater share of the variation in unemployment rates in these locations.
As expected, a greater share of population in long-term illness is
associated with higher local unemployment. In contrast, the relationship
between unemployment rate and long-term illness tends to operate in the
reverse direction in north-east and middle-southern MSOAs. In these
MSOAs, OAs tend to have a higher unemployment rate relative the share of
population in long-term illness. You can confirm this examining the data
for specific MSOA executing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oa_shp }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(msoa_cd, ward_nm, unemp, lt_ill) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(msoa_cd) }\OperatorTok{==}\StringTok{ "E02001370"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 23 features and 4 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 335885 ymin: 391134.2 xmax: 337596.3 ymax: 392467
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs
## First 10 features:
##      msoa_cd                  ward_nm     unemp    lt_ill
## 1  E02001370                  Everton 0.4024390 0.2792793
## 2  E02001370 Tuebrook and Stoneycroft 0.3561644 0.3391813
## 3  E02001370                  Everton 0.3285714 0.3106383
## 4  E02001370                  Everton 0.3209877 0.3283019
## 5  E02001370                  Anfield 0.3082707 0.1785714
## 6  E02001370                  Everton 0.3000000 0.4369501
## 7  E02001370                  Everton 0.2886598 0.3657143
## 8  E02001370                  Everton 0.2727273 0.3375000
## 9  E02001370                  Everton 0.2705882 0.2534247
## 10 E02001370 Tuebrook and Stoneycroft 0.2661290 0.2941176
##                          geometry
## 1  MULTIPOLYGON (((336328.3 39...
## 2  MULTIPOLYGON (((337481.5 39...
## 3  MULTIPOLYGON (((336018.5 39...
## 4  MULTIPOLYGON (((336475.7 39...
## 5  MULTIPOLYGON (((337110.6 39...
## 6  MULTIPOLYGON (((336516.3 39...
## 7  MULTIPOLYGON (((336668.6 39...
## 8  MULTIPOLYGON (((336173.8 39...
## 9  MULTIPOLYGON (((336870 3917...
## 10 MULTIPOLYGON (((337363.8 39...
\end{verbatim}

Now try adding a group-level predictor and an individual-level predictor
to the model. Unsure, look at the Sections
\protect\hyperlink{adding-group-level-predictors}{Adding Group-level
Predictors} and
\protect\hyperlink{adding-individual-level-predictors}{Adding
Individual-level Predictors} in the previous Chapter.

\hypertarget{interpreting-correlations-between-group-level-intercepts-and-slopes}{\section{Interpreting
Correlations Between Group-level Intercepts and
Slopes}\label{interpreting-correlations-between-group-level-intercepts-and-slopes}}

Correlations of random effects are confusing to interpret. Key for their
appropriate interpretation is to recall they refer to group-level
residuals i.e.~deviation of intercepts and slopes from the average model
intercept and slope. A strong \emph{negative} correlation indicates that
groups with high intercepts have relatively low slopes, and \emph{vice
versa}. A strong \emph{positive} correlation indicates that groups with
high intercepts have relatively high slopes, and \emph{vice versa}. A
correlation close to \emph{zero} indicate little or no systematic
between intercepts and slopes. Note that a high correlation between
intercepts and slopes is not a problem, but it makes the interpretation
of the estimated intercepts more challenging. For this reason, a
suggestion is to center predictors (\(x's\)); that is, substract their
average value (\(z = x - \bar{x}\)). For a more detailed discussion, see
\citet{bristol2020}.

To illustrate this, let's reestimate our model adding an
individual-level predictor: the share of population with no educational
qualification.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# centering to the mean}
\NormalTok{oa_shp}\OperatorTok{$}\NormalTok{z_no_qual <-}\StringTok{ }\NormalTok{no_qual}\OperatorTok{/}\DecValTok{100} \OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(no_qual}\OperatorTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{oa_shp}\OperatorTok{$}\NormalTok{z_lt_ill <-}\StringTok{ }\NormalTok{lt_ill }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(lt_ill)}

\CommentTok{# specify a model equation}
\NormalTok{eq7 <-}\StringTok{ }\NormalTok{unemp }\OperatorTok{~}\StringTok{ }\NormalTok{z_lt_ill }\OperatorTok{+}\StringTok{ }\NormalTok{z_no_qual }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{z_lt_ill }\OperatorTok{|}\StringTok{ }\NormalTok{msoa_cd)}
\NormalTok{model7 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(eq7, }\DataTypeTok{data =}\NormalTok{ oa_shp)}

\CommentTok{# estimates}
\KeywordTok{summary}\NormalTok{(model7)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML ['lmerMod']
## Formula: unemp ~ z_lt_ill + z_no_qual + (1 + z_lt_ill | msoa_cd)
##    Data: oa_shp
## 
## REML criterion at convergence: -4940.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.6830 -0.5949 -0.0868  0.4631  6.3556 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr 
##  msoa_cd  (Intercept) 8.200e-04 0.02864       
##           z_lt_ill    2.161e-06 0.00147  -0.04
##  Residual             2.246e-03 0.04739       
## Number of obs: 1584, groups:  msoa_cd, 61
## 
## Fixed effects:
##               Estimate Std. Error t value
## (Intercept)  0.1163682  0.0039201   29.68
## z_lt_ill    -0.0003130  0.0003404   -0.92
## z_no_qual    0.3245811  0.0221347   14.66
## 
## Correlation of Fixed Effects:
##           (Intr) z_lt_l
## z_lt_ill  -0.007       
## z_no_qual -0.015 -0.679
\end{verbatim}

How do you interpret the random effect correlation?

\section{Model building}\label{model-building}

Now we know how to estimate multilevel regression models in \emph{R}.
The question that remains is: \emph{When does multilevel modeling make a
difference?} The short answer is: when there is little group-level
variation. When there is very little group-level variation, the
multilevel modelling reduces to classical linear regression estimates
\emph{with no group indicators}. Inversely, when group-level
coefficients vary greatly (compared to their standard errors of
estimation), multilevel modelling reduces to classical regression
\emph{with group indicators} \citet{Gelman_Hill_2006_book}.

\emph{How do you go about building a model?}

We generally start simple by fitting simple linear regressions and then
work our way up to a full multilevel model - see
\citet{Gelman_Hill_2006_book} p.~270.

\emph{How many groups are needed?}

As an absolute minimum, more than two groups are required. With only one
or two groups, a multilevel model reduces to a linear regression model.

\emph{How many observations per group?}

Two observations per group is sufficient to fit a multilevel model.

\subsection{Model Comparison}\label{model-comparison}

\emph{How we assess different candidate models?} We can use the function
\texttt{anova()} and assess various statistics: The Akaike Information
Criterion (AIC), the Bayesian Information Criterion (BIC), Loglik and
Deviance. Generally, we look for lower scores for all these indicators.
We can also refer to the \emph{Chisq} statistic below. It tests the
hypothesis of whether additional predictors improve model fit.
Particularly it tests the \emph{Null Hypothesis} whether the
coefficients of the additional predictors equal 0. It does so comparing
the deviance statistic and determining if changes in the deviance are
statistically significant. Note that a major limitation of the deviance
test is that it is for nested models i.e.~a model being compared must be
nested in the other. Below we compare our two models. The results
indicate that adding an individual-level predictor (i.e.~the share of
population with no qualification) provides a model with better.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(model6, model7)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## refitting model(s) with ML (instead of REML)
\end{verbatim}

\begin{verbatim}
## Data: oa_shp
## Models:
## model6: unemp ~ lt_ill + (1 + lt_ill | msoa_cd)
## model7: unemp ~ z_lt_ill + z_no_qual + (1 + z_lt_ill | msoa_cd)
##        Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(>Chisq)    
## model6  6 -4764.7 -4732.5 2388.3  -4776.7                             
## model7  7 -4956.5 -4918.9 2485.2  -4970.5 193.76      1  < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\chapter{GWR}\label{gwr}

FR

\chapter{Space-Time Analysis}\label{space-time-analysis}

FR

\bibliography{book.bib,packages.bib}

\end{document}
